# 作业

## 同步转异步容错机制

22讲 2:03:00

设计一个新的容错机制，同步转异步的容错机制。当满足以下两个条件中的任何一个时，将请求转储到数据库，后续再另外启动一个 goroutine 异步发送出去。
• 触发了限流。
• 判定服务商已经崩溃。
要求：
• 如何判定服务商已经崩溃，不允许使用课程上的判断机制，你需要设计一个新的判断机制，并且解释这种判定机制
的决策理由。
• 控制异步重试次数，转储到数据库之后，可以重试 N 次，重试间隔你可以自由决策。
• 不允许写死任何参数，即用户必须可以控制控制参数。
• 保持面向接口和依赖注入风格。
• 写明这种容错机制适合什么场景，并且有什么优缺点。
• 针对提出的缺点，写出后续的改进方案。
• 提供单元测试。
• 设计方案的时候，不允许问老师、助教。即方案肯定得你自己设计，包括优缺点分析、改进方向必须独立完成。
作业时限：两周。

## 优化 Web 中打印日志的部分

在 Web 的 Handler 部分，有很多 if-else 分支，基本上都是在判定 err !=nil。如右图，每一个 if 里面都要打印日志。现在要求你优化这些打印日志的逻辑，避免每一处err !=nil 的时候，都得手动打一个日志。

<img src="assets/image-20240914213626194.png" alt="image-20240914213626194" style="zoom: 33%;" />



## 找出点赞数量前N个数据

这一次的面试要点和作业是同一个东西。有一个高频面试题：如果要你找出按照点赞数量前 N 个数据，怎么做？
这一次的作业就是要求你解决这个问题，设计一个高性能方案，要求：
• 综合考虑可以怎么利用缓存，包括 Redis 和本地缓存。要想清楚，你这个缓存方案拿出去面试究竟有没有竞
争力，有没有让面试官眼前一亮的点。
• 允许业务折中，但是你要说清楚你准备怎么折中。
• 自主设计解决方案，设计过程中不允许询问助教或者老师，需要自己搜索解决方案，或者在群里讨论。也禁
止你把你的方案直接告诉别人，只能采用启发性话语来和同学讨论。
• 实现过程可以询问助教或者老师。
提交代码和一个文档，其中文档：
• 核心接口、类的代码位置（HTTP地址）。
• 绘制 UML 序列图，至少包含两张图：缓存命中与未命中的场景。
• 性能测试结果，推荐使用 wire。要在结果中告知你的机器参数、性能测试在不同输入参数下的结果。
时间：你有两周时间。



问题：全表扫描，性能很差

## 为消息队列添加监控

在使用 Kafka 的时候，我为 Sarama 的 ConsumerClaim 添加了一个二次封装。
现在要求你在我的代码基础上，补充监控的内容，这一个是补充 Prometheus 的监控。
你要提交一个文档：
• 具体修改的文件。
• 解释你用了什么指标来监控，以及为什么用这个指标。
• 假设你在实践中，基于这个监控，你认为要设置告警，你会怎么设置。

## 作业一：选择最合适的节点

在使用 Redis 的分布式锁方案中，我们在筛选节点的时候，并没有考虑节点的真实负载，而是谁先抢到分布式锁，
谁就负责接下来的热榜计算。
但是在非常罕见的业务场景下，你会考虑选择负载更加低的节点来执行热榜计算。又或者在拿到了分布式锁之后，
突然发现自己的负载变高了，这个时候你就要考虑让出这个分布式锁。
现在你要做的就是设计一个方案，考虑节点负载情况，在选择节点来计算热榜的时候，总是能够选中负载比较低的
节点。
模拟节点负载：随机生成一个 0-100 的数字，代表该节点的负载，并且在运行期间可以间隔一定时间就重新生成随
机数，代表节点负载变化。
要求：
• 对实时性的要求并不高。也就是说，并不是节点的负载一变化，你就要考虑换节点，可以有一定的延迟；
• 并不需要确保每次都是选择了最佳节点，只需要大部分情况下能够选中负载比较低的节点就可以；
• 就使用 Redis，不允许引入额外的中间件。
提交：
• 一个文档，解释你的思路，配上对应的UML序列图，并且列出关键实现的 URL（方便助教老师批改）
• 分析极端情况。即有没有可能选中最差的节点？如果选中的节点宕机了，会发生什么？

**作业一：选择最合适的节点**

在使用 Redis 的分布式锁方案中，我们在筛选节点的时候，并没有考虑节点的真实负载，而是谁先抢到分布式锁，
谁就负责接下来的热榜计算。
但是在非常罕见的业务场景下，你会考虑选择负载更加低的节点来执行热榜计算。又或者在拿到了分布式锁之后，
突然发现自己的负载变高了，这个时候你就要考虑让出这个分布式锁。
现在你要做的是设计一个方案，考虑节点负载情况，在选择节点来计算热榜的时候，总是能够选中负载比较低的节
点。
模拟节点负载：随机生成一个 0-100 的数字，代表该节点的负载，并且在运行期间可以间隔一定时间就重新生成随
机数，代表节点负载变化。
要求：
• 对实时性的要求并不高。也就是说，并不是节点的负载一变化，你就要考虑换节点，可以有一定的延迟。
• 并不需要确保每次都是选择了最佳节点，只需要大部分情况下能够选中负载比较低的节点就可以。
• 就使用 Redis，不允许引入额外的中间件。
提交：
• 一个文档，解释你的思路，配上对应的 UML 序列图，并且列出关键实现的 URL（方便助教老师批改）。
• 分析极端情况。即有没有可能选中最差的节点？如果选中的节点宕机了，会发生什么？

## 作业二：抢占接口的查询条件

我们在讨论到抢占式的任务调度的时候，MySQL 的
实现里面，使用了续约机制。
但是我们对应的 SQL 没有加入续约失败的查询条件。
也就是说，实际上一个任务被调度有两个条件：
• 没人调度，这个查询条件我们已经有了。
• 曾经有人调度，但是后面续约失败了， 也就是这
个调度的节点，可能已经崩溃了。
现在需要你修改右边的查询条件，支持第二种情况。

## 作业一：其它聚合形态——在 Repository 里面聚合

在前面的例子里面，我直接在 Handler 层面上聚合了 gRPC
服务。
理论上来说，这不符合 DDD 的设计，但是好用。
按照 DDD 的设计来说的话，这边应该是要把 Interactive 的
gRPC做成一个 Repository，而后在 ArticleRepository 里面
完成 Interactive 相关的组装。
换言之，将 Interactive 看做是 Article 的一个部分。
因此你的作业就是，用这种形态来集成 gRPC 的 Interactive。
并且感受一下课堂风格和这种风格之间的差异。
提交：一个文档，里面贴上你的关键修改的 URL。
**可选作业**
挑选除了 Interactive 之外的任何一个模块，将它挪出去作为一个微服务。
如果你有足够的时间，那么尝试将所有的模块都挪出去之后，思考这个问题：
Web 这个东西，应该怎么办？是每一个需要提供 HTTP 服务的都有一个 Web 包，还是说全局只有一个 Web
包？
提前了解 BFF（backend for frontend）的概念，并且思考 BFF 的职责包含哪些，以及 BFF 在 DDD 中的地
位究竟该怎么样？

## 补完打赏流程

整个打赏流程，还有一些问题需要你处理。
• 支付消息必然发送成功。也就是在 Payment 收到了微信支付的回调之后，会更新本地状态，而后发一个消息
到 Kafka 上。现在要求你设计一个兜底机制，确保这个消息一定会发送成功，参考思路本地消息表。
• 记账的幂等性。你可以注意到，在 Reward 收到支付成功的消息之后，就会要求记账服务记账，现在要求你设
计提供一个方案，保证同一个 biz+bizId，不会重复记账。并且要考虑有什么优化方案，可以进一步提高性
能。参考唯一索引、Redis 去重、布隆过滤器等。
• 设计服务治理措施。考虑在整个流程的不同部分引入一些服务治理手段，你可以自由发挥。
• 三者对账机制。整个机制涉及到了打赏-支付-记账三个服务，现在要求你设计一个机制，能够对比三者的数
据，并且找出不一致的数据。
你有两周的时间，从四个里面挑选出来两个，提供代码实现。剩下的两个只需要提供思路，确保这个思路能够在
面试中顶住面试官的追问。



![image-20240827151705527](assets/image-20240827151705527.png)

# 学习记录

## 登录功能

### 正则表达式校验参数

我们不要用golang自带的正则表达式而是导包

```go
"github.com/dlclark/regexp2"
```

正则表达式是一种用于匹配和操作文本的强大工具，它是由一系列字符和特殊字符组成的模式，用于描述要匹配的文本模式。正则表达式可以在文本中查找、替换、提取和验证特定的模式。我的建议是不用学，要用的时候直接搜别人写好的。

**案例**

```go
const (
		emailRegexPattern = "^\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*$"
		// 和上面比起来，用 ` 看起来就比较清爽
		passwordRegexPattern = `^(?=.*[A-Za-z])(?=.*\d)(?=.*[$@$!%*#?&])[A-Za-z\d$@$!%*#?&]{8,}$`
	)
var emailExp    *regexp2.Regexp //邮箱校验器
var passwordExp *regexp2.Regexp //密码校验器
emailExp = regexp2.MustCompile(emailRegexPattern, regexp2.None)
passwordExp = regexp2.MustCompile(passwordRegexPattern, regexp2.None),
//校验	
ok, _ := emailExp.MatchString(req.Email)
if !ok {
    ctx.String(http.StatusOK, "你的邮箱格式不对")
}
ok, _ = passwordExp.MatchString(req.Password)
if !ok {
    ctx.String(http.StatusOK, "密码必须大于8位，包含数字、字母、特殊字符")
    return
}
```

### 跨域问题

跨域（Cross-Origin）指的是浏览器阻止前端网页从一个域名（Origin）向另一个域名的服务器发送请求。具体来说，一个页面的协议、域名、端口三者任意一个与请求的目标地址不同，就被视为跨域请求。

举例说明：

http://example.com 请求 http://api.example.com 会跨域，因为域名不同。
http://example.com 请求 https://example.com 会跨域，因为协议不同。
http://example.com:8080 请求 http://example.com:9090 会跨域，因为端口不同。

#### 使用中间件解决

使用 middleware 来解决 CORS 目前大多数的 Web 框架，都提供了 CORS 的解决方案。

Gin 里面也提供了一个 middleware 实现来解决跨域问题，在 `github.com/gin-contrib/cors`

```go
r.Use(cors.New(cors.Config{
		//允许跨域访问的客户端地址，*表示所有都允许
		//AllowOrigins: []string{"*"},
		//允许跨域访问的方法，默认都有
		//AllowMethods:     []string{"PUT", "PATCH"},
		//允许客户端使用的请求头
		AllowHeaders: []string{"Content-type", "Authorization"},
		//允许带cookie之类的用户认证信息
		AllowCredentials: true,
		//使用方法校验忽略掉AllowOrigins
		AllowOriginFunc: func(origin string) bool {
			//你的开发环境，包含localhost就允许访问
			if strings.Contains(origin, "localhost") {
				return true
			}
			return strings.Contains(origin, "yourcompany.com")
		},
		MaxAge: 12 * time.Hour,
	}))
```

• AllowCrendentials：是否允许带上用户认证
信息（比如 cookie）。
• AllowHeader：业务请求中可以带上的头。
• AllowOriginFunc：哪些来源是允许的。

### 安装nodeJs运行前端

```bash
npm config set prefix "D:\environment\Nodejs\node_global"

npm config set cache "D:\environment\Nodejs\node_cache"
```

安装nodejs使用功能npm run dev运行前端在localhost:3000

### docker容器编排

在windows下安装docker desktop，安装安装portanier（可视化的Docker操作页面），使用docker容器编排功能运行mysql、redis环境

### **引入Service - Repostiory - DAO三层结构**

- service: 代表的是领域服务（domain service），代表一个业务的完整的处理过程。
- repository：按照 DDD 的说法，是代表领域对象的存储，这里你直观理解为存储数据的抽象。不仅仅代表数据库
- dao: 代表的是数据库操作。
- domain: 代表领域对象

### **BCrypt加密**

bcrypt 是一个号称最安全的加密算法。
优点：
• 不需要你自己去生成盐值。
• 不需要额外存储盐值。
• 可以通过控制 cost 来控制加密性能。
• 同样的文本，加密后的结果不同。

要是用它，需要使用 golang.org/x/crypto。因为 bcrypt 限制密码长度不能超过 72 字节，所以在校验时要校验这个长度。只需要修改一下正则表达式就可以。

### **传导错误与检测**

在 repository 和 service 层，我们都使用别名机制，继续向上返回错误。在最顶层的 Handler 里面，我们进行检测。在确定是 ErrUserDuplicateEmail 的情况下，提示邮箱冲突了。使用别名的机制，层层传导，让我们在Handler 里面依旧保持只依赖 service，避免了`跨层依赖`的问题。

```go
var ErrUserDuplicateEmail = errors.New("邮箱冲突")
var ErrUserDuplicateEmail = dao.ErrUserDuplicateEmail
var ErrUserDuplicateEmail = repository.ErrUserDuplicateEmail
```

这样相比于定义一个err文件，并统一调用层次和扩展性更好

**使用errors.Is()方法与gorm内置错误类型比较判断err的具体类型**

```go
err := dao.db.WithContext(ctx).Where("email = ?", email).First(&u).Error
if errors.Is(err, gorm.ErrRecordNotFound) {
   return u, ErrUserNotFound
}
```

**注意**

不管是用户没找到，还是密码错误，我们都返回同一个 error。不能让前端知道具体哪错了

### cookie和session

#### 简介

HTTP 是无状态的。连续发两次请求，HTTP 并不知道这两个都是你发的。也就是，它没办法将上一次请求和这一次请求关联起来。所以我们需要有一种机制，记录一下这个状态。于是就有两个东西：Cookie 和 Session。

**Cookie**

浏览器存储一些数据到**本地**，这些数据就是Cookie。简单理解，就是存储在你电脑上的键值对。也正因为 Cookie 是放在浏览器本地的，所以很不安全。

**Session**

因为 Cookie 本身不安全的特性，所以大部分时候，我们都只在 Cookie 里面放一些不太关键的数据。关键数据我们希望放在后端，这个存储的东西就叫做 Session。因此在登录里面，我们就可以通过 Session 来记录登录状态。

第一次访问后，服务器要给浏览器一个 sess_id，也就是 Session 的ID。后续每一次请求都带上这个 Session ID，服务端就知道你是谁了。

因为 sess_id 是标识你身份的东西，所以你需要在每一次访问系统的时候都带上。

- 最佳方式就是用 Cookie，也就是 sess_id 放到 Cookie里面。sess_id 自身没有任何敏感信息，所以放 Cookie也可以。
- 也可以考虑放 Header，比如说在 Header 里面带一个sess_id。这就需要前端的研发记得在 Header里面带上。
- 还可以考虑放查询参数，也就是 ?sess_id = xxx。

#### 基于cookie的实现

使用中间件帮助创建session，校验session

```go
//使用session校验用户是否登录
//创建session
store := cookie.NewStore([]byte("secret"))
r.Use(sessions.Sessions("mysession", store))
//校验
r.Use(middleware.NewLoginMiddlewareBuilder().Build())
```

在登录成功后设置一个session

```go
//登录成功
//设置session的键值对为userId:user.Id
sess := sessions.Default(ctx)
sess.Set("userId", user.Id)
sess.Save()
ctx.String(http.StatusOK, "登录成功")
```

#### 基于redis的实现

redis好处在于分布式部署下的多个实例共享同一个session。但同时也会损耗性能。

通过 docker compose 来启动一个测试的 Redis。

```go
store, err := redis.NewStore(16, "tcp", "localhost:6379", "", []byte("secret"))
if err != nil {
   panic(err)
}
r.Use(sessions.Sessions("mysession", store))
//校验
r.Use(middleware.NewLoginMiddlewareBuilder().Build())
```

使用redis作为store存储session数据

`在 Gin 的 Session 设计里面，怎么存储你的 Session 数据，被抽象成了一个接口。`session的存储不但有redis,cookie还有memstore等等。

#### session参数

Gin 的 Session 有很多参数，你可以**通过 Options 方法来传入 Option**。可以理解为Gin 的 Session 用很多选项来初始化 Cookie。**比如设置session数据的过期时间**

```go
sess.Options(sessions.Options{
    //只允许https访问
    //secure: true,
   //60秒过期
   MaxAge: 60,
})
```

```go
// Logout 登出
func (u *UserHandler) Logout(ctx *gin.Context) {
   sess := sessions.Default(ctx)
   sess.Options(sessions.Options{
      //删除用户的cookie
      MaxAge: -1,
   })
   sess.Save()
   ctx.String(http.StatusOK, "退出登录成功")
}
```

#### 刷新登录状态

需要在用户持续使用网站的时候，刷新session过期时间。

##### 如何刷新

用户每次访问，我都刷新。性能差，对 Redis 之类的影响很大。

快要过期了我再刷新，比如说 10 分钟过期。当用户第 9 分钟访问过来的时候，我就刷新。万一我在第 9 分钟以后都没再访问过呢？

固定间隔时间刷新，比如说每分钟内第一次访问我都刷新。

使用长短 token。这个我们在后面接入微信登录的时候再深入讨论。

##### 在中间件中刷新

刷新可以在登录校验的中间件中完成，我们采用固定时间间隔刷新。在session中再设置一个update_time的数据，用户每次访问网站都会更新update_time为当前时间，并判断如果距离上次更新时间大于10s，就更新session过期时间进行刷新。

```go
updateTime := sess.Get("update_time")
//当前时间time.Time
now := time.Now()
//还没有刷新过刚登录
if updateTime == nil {
   sess.Set("update_time", now)
   sess.Save()
   return
}
updateTimeVal, _ := updateTime.(time.Time)
if now.Sub(updateTimeVal) > time.Second*10 {
   sess.Set("update_time", now)
   sess.Options(sessions.Options{
   	MaxAge: 60,
	})
   sess.Save()
   return
}
```

**使用redis的话每次访问都会去访问redis或取update_time，在高并发环境下是不行的。**

### JWT

#### 简介

前面我们实现的登录功能，是直接依赖于 Gin 的Session 插件达成的。我们还可以考虑使用 JWT 来实现登录功能。JWT（JSON Web Token）是很常用的一种机制，主要用于身份认证，也就是登录。它的基本原理就是通过加密生成一个 token，而后客户端每次访问的时候都带上这个 token。

它由三部分组成：
• Header：头部，JWT 的元数据，也就是描述这个token 本身的数据，一个 JSON 对象。
• Payload：负载，数据内容，一个 JSON 对象。
• Signature：签名，根据 header 和 token 生成。

#### 使用

直接使用 JWT 原始 API。

```go
go get github.com/golang-jwt/jwt/v5
```

##### 在midderware中定义生成token所需Claims，

```go
type UserClaims struct {
   jwt.RegisteredClaims
   //声明你自己的要放进去token里面的数据
   Uid int64
}
```

##### 登录成功后创建token，将tokenStr返回给前端

```go
//用jwt设置登录状态
//创建token
claims := middleware.UserClaims{
		RegisteredClaims: jwt.RegisteredClaims{
			//设置过期时间
			ExpiresAt: jwt.NewNumericDate(time.Now().Add(time.Minute)),
		},
		Uid: user.Id,
}
token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
tokenStr, err := token.SignedString([]byte("secret"))
if err != nil {
	ctx.String(http.StatusInternalServerError, "系统错误")
	return
}
//将token放到header中
ctx.Header("x-jwt-token", tokenStr)
//fmt.Println(tokenStr)
ctx.String(http.StatusOK, "登录成功")
```

##### 将登录校验从session改为jwt

```go
// Build 登录校验或者刷新登录状态
func (l *LoginJWTMiddlewareBuilder) Build() gin.HandlerFunc {
	//用go的方式编码解码
	gob.Register(time.Now())
	return func(ctx *gin.Context) {
		//登录和注册不需要校验
		for _, path := range l.paths {
			if ctx.Request.URL.Path == path {
				return
			}
		}
		//Bearer xxx.xxx.xxx
		tokenHeader := ctx.GetHeader("Authorization")
		if tokenHeader == "" {
			//没登录
			ctx.AbortWithStatus(http.StatusUnauthorized)
			return
		}
		segs := strings.Split(tokenHeader, " ")
		if len(segs) != 2 {
			//格式不对，没登录
			ctx.AbortWithStatus(http.StatusUnauthorized)
			return
		}
		tokenStr := segs[1]
		//解析时传指针
		claims := &UserClaims{}
		token, err := jwt.ParseWithClaims(tokenStr, claims, func(token *jwt.Token) (interface{}, error) {
			return []byte("secret"), nil
		})
		if err != nil {
			//没登录
			ctx.AbortWithStatus(http.StatusUnauthorized)
			return
		}
		//token无效
		if token == nil || !token.Valid || claims.Uid == 0 {
			//没登录
			ctx.AbortWithStatus(http.StatusUnauthorized)
			return
		}
        //每10s刷新一次，token过期时间
		now := time.Now()
		if claims.ExpiresAt.Sub(now) < time.Second*50 {
			claims.ExpiresAt = jwt.NewNumericDate(time.Now().Add(time.Minute))
			tokenStr, err = token.SignedString([]byte("secret"))
			if err != nil {
				log.Println("jwt 续约失败")
				return
			}
			ctx.Header("x-jwt-token", tokenStr)
		}
		ctx.Set("claims", claims)
	}
}
```

#### JWT 改造跨域设置

后端将token放在响应头中前端接收不到，是因为跨域设置中要设置ExposeHeaders ：[]strings{"x-jwt-token"}

请求报文要带上Authorization，因此跨域设置中要AllowHeaders加上Authorization

#### 用布隆过滤器实现退出登录

#### 接入 JWT 的步骤总结

• 要在 Login 接口中，登录成功后生成 JWT token。

• 在 JWT token 中写入数据。

• 把 JWT token 通过 HTTP Response Header x-jwt-token 返回。

• 改造跨域中间，允许前端访问 x-jwt-token 这个响应头。

• 要接入 JWT 登录校验的 Gin middleware。

• 读取 JWT token。

• 验证 JWT token 是否合法。

• 前端要携带 JWT token。

#### 使用 JWT 的优缺点

和 Session 比起来，优点：

• 不依赖于第三方存储。

• 适合在分布式环境下使用。

• 提高性能（因为没有 Redis 访问之类的）。

缺点：

• 对加密依赖非常大，比 Session 容易泄密。

• 最好不要在 JWT 里面放置敏感信息。

#### 保护系统

##### 限流（无需掌握）

限流是最常见的保护系统的办法。限流有很多算法，但是都大同小异，后面在微服务架构部分会进一步讲解。这里我们使用限流，限制每一个用户，每秒最多发送固定数量的请求。所以问题来了：

• 我怎么认定谁是谁？尤其是在登录和注册这个接口里，都还没登录成功，我都不知道他是谁。

• 我怎么确定我限流的这个阈值应该是多少？每秒 100 还是每秒 200？

**限流对象**

**限流阈值**

**被限流的请求直接返回错误**

**使用gin的限流插件**



##### jwt增强登录安全

利用 User-Agent 增强安全性,为此需要改造两个地方：

• Login 接口，在 JWT token 里面带上 User-Agent信息。

```go
claims := middleware.UserClaims{
   RegisteredClaims: jwt.RegisteredClaims{
      //设置过期时间
      ExpiresAt: jwt.NewNumericDate(time.Now().Add(time.Minute)),
   },
   UserAgent: ctx.Request.UserAgent(),
   Uid:       user.Id,
}
```

• JWT 登录校验中间件，在里面比较 User-Agent。

```go
//增强登录安全，校验UserAgent
if claims.UserAgent != ctx.Request.UserAgent() {
   ctx.AbortWithStatus(http.StatusUnauthorized)
   return
}
```

### 性能测试与优化

#### wrk

[wrk](https://github.com/wg/wrk)是一款开源的HTTP性能测试工具，它和上面提到的`ab`同属于HTTP性能测试工具，它比`ab`功能更加强大，可以通过编写lua脚本来支持更加复杂的测试场景。

Mac下安装：

```bash
brew install wrk
```

常用命令参数：

```bash
-c --conections：保持的连接数
-d --duration：压测持续时间(s)
-t --threads：使用的线程总数
-s --script：加载lua脚本
-H --header：在请求头部添加一些参数
--latency 打印详细的延迟统计信息
--timeout 请求的最大超时时间(s)
```

使用示例：

```bash
wrk -t8 -c100 -d30s --latency http://127.0.0.1:8080/api/v1/posts?size=10
```

输出结果：

```bash
Running 30s test @ http://127.0.0.1:8080/api/v1/posts?size=10
  8 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    14.55ms    2.02ms  31.59ms   76.70%
    Req/Sec   828.16     85.69     0.97k    60.46%
  Latency Distribution
     50%   14.44ms
     75%   15.76ms
     90%   16.63ms
     99%   21.07ms
  198091 requests in 30.05s, 29.66MB read
Requests/sec:   6592.29
Transfer/sec:      0.99MB
```

#### go-wrk

[go-wrk](https://github.com/adjust/go-wrk)是Go语言版本的`wrk`，Windows同学可以使用它来测试，使用如下命令来安装`go-wrk`：

```bash
go install github.com/adeven/go-wrk
```

使用方法同`wrk`类似，基本格式如下：

```bash
go-wrk [flags] url
```

常用的参数：

```bash
-H="User-Agent: go-wrk 0.1 bechmark\nContent-Type: text/html;": 由'\n'分隔的请求头
-c=100: 使用的最大连接数
-k=true: 是否禁用keep-alives
-i=false: if TLS security checks are disabled
-m="GET": HTTP请求方法
-n=1000: 请求总数
-t=1: 使用的线程数
-b="" HTTP请求体
-s="" 如果指定，它将计 算响应中包含搜索到的字符串s的频率
```

执行测试：

```bash
go-wrk -t=8 -c=100 -n=10000 "http://127.0.0.1:8080/api/v1/posts?size=10"
```

输出结果：

```bash
==========================BENCHMARK==========================
URL:                            http://127.0.0.1:8080/api/v1/posts?size=10

Used Connections:               100
Used Threads:                   8
Total number of calls:          10000

===========================TIMINGS===========================
Total time passed:              2.74s
Avg time per request:           27.11ms
Requests per second:            3644.53
Median time per request:        26.88ms
99th percentile time:           39.16ms
Slowest time for request:       45.00ms

=============================DATA=============================
Total response body sizes:              340000
Avg response body per request:          34.00 Byte
Transfer rate per second:               123914.11 Byte/s (0.12 MByte/s)
==========================RESPONSES==========================
20X Responses:          10000   (100.00%)
30X Responses:          0       (0.00%)
40X Responses:          0       (0.00%)
50X Responses:          0       (0.00%)
Errors:                 0       (0.00%)
```

#### 利用 wrk 来压测接口

我们可以考虑使用 wrk 先来压测接口。我们压测三个接口：

• 注册：写为主的接口
• 登录：读为主的接口
• Profile：读为主的接口

wrk 这个工具前期不要求你掌握，只需要把我这里运行的命令贴到命令行跑起来就可以。

##### **安装**

```bash
3.2、在Ubuntu子系统安装wrk

# 进入Ubuntu子系统
wsl -d Ubuntu

# 安装wrk
sudo apt-get install -y wrk
复制
3.3、安装验证

wrk -v
```

##### 编写lua脚本（后面学）

##### 在lua脚本目录下执行命令

```bash
wrk -t1 -d1s -c2 -s ./scripts/wrk/signup.lua http://localhost:8080/users/signup
//使用虚拟机用这个
wrk -t1 -d10m -c10 -s ./scripts/wrk/signup.lua http://192.168.83.1:8080/users/signup
```

注意，你可以不断调整这些参数：

**• -t：后面跟着的是线程数量。**

**• -d：后面跟着的是持续时间，比如说 1s 是一秒，**

**也可以是 1m，是一分钟。**

**• -c：后面跟着的是并发数。**

**• -s：后面跟着的是测试的脚本。**

最终能跑多少，和你机器有关。

![image-20240907165659707](assets/image-20240907165659707.png)

#### 使用redis缓存优化性能

以上完成的功能中基本上性能瓶颈是出在数据库的查询上，所以我们可以考虑引入 Redis 来优化性能。用户会先从 Redis 里面查询，而后在缓存未命中的情况下，就会直接从数据库中查询。

##### 代码解释

查询有两种写法：

- 只要缓存返回了 error，不管有没有查找到或有没有出错，直接去数据库查询，**有隐患的。那就是万一 Redis 本身崩溃了，那么查询都会落到数据库上。**
- 只有缓存没查找到数据才去数据库查询。**代价就是 Redis 崩溃之后，业务也不可用，但是数据库保住了。**

##### 登录要不要缓存

**可以，但是收益不大。**

因为登录是一个非常低频的事情，正常的互联网网站都是好几天才会让你登录一次，你缓存了也没用。

## k8s部署

### Kubernetes入门

• Pod：实例。运行的容器

• Service：逻辑上的服务，可以认为这个是你业务上 XXX 服务的直接映射。

• Deployment：管理 Pod 的东西。

。。。。。

#### 开启docker desktop的enable kubernetes

docker desktop使用kubenetes国内安装不了，https://github.com/AliyunContainerService/k8s-for-docker-desktop/tree/v1.30.2查看教程下载

下载上面的zip文件解压后，在powershell中执行.\load_images.ps1，等待安装完成后，退出Docker Desktop，删除c盘用户目录下/.kube/config文件，然后再运行Docker Desktop

下载10个镜像文件

```bash
hubproxy.docker.internal:5555/docker/desktop-kubernetes                                      kubernetes-v1.27.2-cni-v1.2.0-critools-v1.27.0-cri-dockerd-v0.3.2-1-debian   c763812a4530   2 months ago   418MB
registry.k8s.io/kube-apiserver                                                               v1.27.2                                                                      c5b13e4f7806   2 months ago   121MB
registry.k8s.io/kube-scheduler                                                               v1.27.2                                                                      89e70da428d2   2 months ago   58.4MB
registry.k8s.io/kube-controller-manager                                                      v1.27.2                                                                      ac2b7465ebba   2 months ago   112MB
registry.k8s.io/kube-proxy                                                                   v1.27.2                                                                      b8aa50768fd6   2 months ago   71.1MB
docker/desktop-vpnkit-controller                                                             dc331cb22850be0cdd97c84a9cfecaf44a1afb6e                                     556098075b3d   2 months ago   36.2MB
registry.k8s.io/coredns/coredns                                                              v1.10.1                                                                      ead0a4a53df8   6 months ago   53.6MB
registry.k8s.io/etcd                                                                         3.5.7-0                                                                      86b6af7dd652   6 months ago   296MB
registry.k8s.io/pause                                                                        3.9                                                                          e6f181688397   9 months ago   744kB
docker/desktop-storage-provisioner
```

#### kubectl

后就要安装kubectl工具了

##### 安装（科学上网）

```
cul 。。。
```

##### 指令

kubectl get pods/deployment/service 查看实例/管理/服务

kubectl logs webook-2343h234-qwe24 查看具体实例信息

### 部署web服务

#### 准备Kubernetes容器镜像

因为k8s调度的是容器，所有先要把 webook 打包成镜像。

• 首先在本地完成编译，生成一个可在 Linux 平台上执行的 webook 可执行文件。基本命令是：GOOS=linux GOARCH=arm go build -o webook .

**windows下先set GOOS=linux和set GOARCH=arm然后go build -o webook**

• 其次是编写Dockerfile文件运行 Docker 命令，**docker build -t flycash/webook:v0.0.1 .**打包成一个镜像。

出于方便的目的，**使用make工具编写Makefile文件，打包成了一个 make docker 命令**。

```makefile
.PHONY: docker
docker:
   @del webook || true
   @set GOOS=linux
   @set GOARCH=amd64
   @go build -o webook
   @set GOOS=windows
   @docker rmi -f flycash/webook:v0.0.1
   @docker build -t flycash/webook:v0.0.1 .
```

#### 编写Deployment

k8s-webook-deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webook
spec:
  #副本数量
  replicas: 3
  selector:
    matchLabels:
      app: webook
  #template描述的是你的POD是什么样的
  template:
    metadata:
      labels:
        app: webook
    #POD的具体信息
    spec:
      containers:
        - name: webook
          image: flycash/webook:v0.0.1
          ports:
            - containerPort: 8080
```

使用`kubectl apply -f k8s-webook-deployment.yaml`应用配置

```bash
D:\Go\project\src\geektime\webook>kubectl get deployment
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
webook   3/3     3            3           3h51m
```



##### apiVersion

K8s 简单理解就是一个配置驱动的，或者元数据驱动，或者声明式的框架。怎么理解这句话？

我们用户只负责提供各种配置，然后 K8s 根据配置来执行一些动作。所以问题就来了，K8s 怎么知道该怎么解读这个配置呢？答案就是利用 apiVersion 来确定怎么解读。

##### spec

spec(specification)，也就是 Deployment 的规格说明，规格说明你就直接理解为说明书：

• **replicas**：值为 3，也就是我这个 Deployment 有三个副本，实际上就是三个 Pod。

• **selector**：筛选器，就是在 K8s 的一大堆 Pod 里面，我怎么知道哪些是我管理的那三个。

• **template**：我该怎么创建每个 Pod，或者说每个Pod 长什么样。有了 template（模板），我就可以照猫画虎直接创建出来了。

##### selector

筛选器，它在 K8s 里面被用来筛选所需资源。我们配置 selector 使用的是matchLabels。也就是说，Deployment 按照标签（label）来筛选它需要的资源——Pod。它需要的是含有 app=webook 这个标签的 Pod。

常用的除了 matchLabels 还有 matchExpressions，即根据表达式来筛选。

##### template

emplate 在 kind 不同的时候对应的内容也不同。在 Deployment 里面，template 是**创建 Pod 的模**
**板。**

##### iamge

image 就是镜像，显然这里我们用的是 Docker 构建的镜像。

#### 编写Service

k8s-webook-service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: webook
#规格说明，这是一个怎样的服务
spec:
  type: LoadBalancer
  selector:
    app: webook
  ports:
    - name: http
      port: 81
      protocol: TCP
      targetPort: 8080
```

只有 Deployment 你是没办法从外面访问的，你需要**将 Pod 封装为一个逻辑上的服务，即Service。**

spec 里面有两个字段要注意：

• type：这里选择的是负载均衡，意思是这个 Service 在逻辑上还要负责负载均衡的问题。给谁负载均衡？给 selector 里面筛选出来的 Pod 做负载均衡。

• ports：端口，你可以配置很多个，比如还可以额外配置一个 HTTPS 的。

• name：就是你这个端口的名字，一般用来指示用途，比如http 意思是这个端口用来接收 HTTP 请求。你可以随便写。

• port：外部访问的端口。

• protocol：这个端口监听什么协议。

• targetPort：我这边转发请求的时候，应该转发到 Pod 上的哪个端口。

#### 启动

`使用kubectl apply -f k8s-webook-service.yaml命令应用配置启动服务，就可以访问了`

```bash
D:\Go\project\src\geektime\webook>kubectl get service                         
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP        3h28m
webook       LoadBalancer   10.102.65.255   localhost     81:30481/TCP   118m
```

### 部署MySQL

![mysql_deploy](assets/mysql_deploy.png)

部署方式与部署web服务类似，不过MySQL 和前面的 Go 应用不一样，它需要存储数据，也就是我们要给它一个存储空间。`在 K8s 里面，存储空间被抽象为 PersistentVolume（持久化卷）`

#### deployment

在 template 里面，关键是**spec.containers.volumeMounts 和 volumes。**

• 在 containers 里面的 volumeMounts，意思是挂载到我容器的哪个地方。

• volumns 是指，我这里挂载的东西究竟是什么。

k8s-mysql-deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webook-mysql
spec:
  replicas: 1
  #筛选
  selector:
    matchLabels:
      app: webook-mysql
  #模板
  template:
    metadata:
      name: webook-mysql
      labels:
        app: webook-mysql
    spec:
      containers:
        - name: webook-mysql
          image: mysql:8
          imagePullPolicy: IfNotPresent
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: root
          volumeMounts:
#            这边要对应到mysql的数据存储的位置
            - mountPath: /var/lib/mysql
              #指定使用的volume
              name: mysql-storage
          ports:
            - containerPort: 3306
      restartPolicy: Always
      #整个pod拥有的volume
      volumes:
        #持久化卷可以有多个
        - name: mysql-storage
          persistentVolumeClaim:
            claimName: webook-mysql-claim
#        - name: hadoop-storage
#          persistentVolumeClaim:
#            claimName: webook-hadoop-claim
```



#### service

k8s-mysql-service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: webook-mysql
spec:
  #筛选 app:webook-mysql的pod
  selector:
    app: webook-mysql
  ports:
    #避免冲突，使用11309端口，这里使用了nodePort所以是30002端口
    - port: 11309
      name: mysql
      protocol: TCP
      targetPort: 3306
      nodePort: 30002
  type: NodePort
```

#### PersistentVolumeClaim

一个容器需要什么存储资源，是通过PersistentVolumeClaim 来声明的。

k8s-mysql-pvc.yaml

```yaml
apiVersion: v1
#声明需要什么样的volume
kind: PersistentVolumeClaim
metadata:
  name: webook-mysql-claim
  labels:
    app: webook-mysql-claim
spec:
  storageClassName: suibian
  #需要的读写方式
  accessModes:
    #一个POD读写
    - ReadWriteOnce
#    #多个读，一个写
#    - ReadOnlyMany
#    #多个读写
#    - ReadWriteMany
  resources:
    requests:
      #1GB = 1Gi
      storage: 1Gi
```

#### PersistentVolume

持久化卷，表达我是一个什么样的存储结构。所以，PersistentVolume 是存储本身说我有什么特
性，而 PersistentVolumeClaim 是用的人告诉 K8s说他需要什么特性。

k8s-mysql-pv.yaml

```yaml
apiVersion: v1
#声明 k8s 有哪些 volume
kind: PersistentVolume
metadata:
  name: my-local-pv
  labels:
    app: my-local-pv
spec:
  storageClassName: suibian
  capacity:
    storage: 1Gi
  #支持的读写方式
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

#### 访问模式accessMode

如果你设计一个存储的东西，你是不是要考虑，我这个东西是只读还是只写？是允许一个人访问，还是允许很多人访问？这个东西，就是由 accessMode 来控制的。

**在 PersistentVolume 里面，accessMode 是说明我这个 PV 支持什么访问模式。**

**在 PersistentVolumeClaim 里面，accessMode 是说明我这个 PVC 需要怎么访问。**

• **ReadWriteOnce**：只能被挂在到一个 Pod，被它读写。

• **ReadOnlyMany**：可以被多个 Pod 挂载，但是只能读。

• **ReadWriteMany**：可以被多个 Pod 挂载，它们都能读写。

### 部署Redis

#### 允许外部访问

设置`sepc.type: NodePort和sepc.ports.nodePort: 外部端口`，直接通过外部访问

#### port、nodePort 和 targetPort 的含义

- **port**：kubernetes中的服务之间访问的端口，是提供给集群内部客户访问service的入口。尽管mysql容器暴露了3306端口，但是集群内其他容器需要通过33306服务端口访问该服务
- **targetPort**：是指 Pod 上暴露的端口，容器的端口（最根本的端口入口）。port和nodePort上到来的访问都要经过targetPort到达容器
- **nodePort**：是指我在 K8s 集群之外访问的端口，比如说我执行 redis-cli -p 30379。

**port和nodePort都是service的端口，前者暴露给集群内客户访问服务，后者暴露给集群外客户访问服务。**从这两个端口到来的数据都需要经过反向代理kube-proxy流入后端pod的targetPod，从而到达pod上的容器内。

![ports](assets/ports.png)

### 部署nginx（有问题待处理）

**安装helm**

**安装ingress-nginx**

**编写ingress**

### 集成Redis和MySQL

#### 编译标签

因为暂时还没有引入配置模块，所以我们现在用一种特别的方法来控制不同的参数。首先我们定义一个结构体，用来接收配置。而后我们在两个文件里面初始化这个配置，其中一个就是正常的文件，叫做 dev.go。另外一个就是k8s.go。两者唯一的区别就是在 k8s.go 里面需要指定一个叫做 k8s 的编译标签。

```go
type config struct {
   DB    DBConfig
   Redis RedisConfig
}

type DBConfig struct {
   DSN string
}

type RedisConfig struct {
   Addr string
}
```

k8s.go

```go
//go:build k8s
package config

var Config = config{
	DB: DBConfig{
		DSN: "root:root@tcp(webook-mysql:11309)/webook",
	},
	Redis: RedisConfig{
		Addr: "webook-redis:11479",
	},
}
```

dev.go

```go
//go:build !k8s
package config

var Config = config{
	DB: DBConfig{
		DSN: "root:root@tcp(localhost:13316)/webook",
	},
	Redis: RedisConfig{
		Addr: "localhost:6379",
	},
}
```

**编译的时候只要带上对应的标签，对应的文件就回参与编译**，比如只编译k8s文件

```bash
go build -tags=k8s -o webook .
```

!k8s只要没有带上k8s标签，那么就会编译这个文件

## 短信登录

### 需求分析

### 短信服务API

#### 基本步骤

首先是**初始化短信客户端，里面要求传入各种鉴权参数。其次是发送一条短信的请求构造，然后发送请求并接收响应**，里面传入的主要是短信本身相关的参数。关键的是：
• 目标手机号码
• 模板：要求你提前在服务商里面配置好，得到模板 ID。
• 参数：发送短信的时候的具体参数。
• appId：你在腾讯短信上创建的应用的 ID。
• 签名：要求发送的短信必须标记是谁发的。

其中 appId 和签名在我们小微书里面都是固定的，同样可以在初始化的时候指定，但是发送的时候就不用传递参数了。

#### 使用API发送短信

这里我使用官方的快速使用案例，为其中每一步做出解释

首先我们需要导入dysmsapi包

```go
go get github.com/aliyun/alibaba-cloud-sdk-go/services/dysmsapi
```

使用NewClientWithAccessKey()创建一个client客户端,这个客户端可以调用阿里云的API

```go
package main

import (
	"github.com/aliyun/alibaba-cloud-sdk-go/services/dysmsapi"
)

func main() {
    //参数一：连接的节点地址（有很多节点选择，这里我选择杭州）
    //参数二：AccessKey ID
    //参数三：AccessKey Secret
	client, err := dysmsapi.NewClientWithAccessKey("cn-hangzhou", "LTAI5t**********AWVNP", "qgzyEW4cA*******PBl7iyw")
｝
```

然后创建请求，并填写请求信息

```go
package main

import (
    "encoding/json"
	"github.com/aliyun/alibaba-cloud-sdk-go/services/dysmsapi"
)

func main() {
    //参数一：连接的节点地址（有很多节点选择，这里我选择杭州）
    //参数二：AccessKey ID
    //参数三：AccessKey Secret
	client, err := dysmsapi.NewClientWithAccessKey("cn-hangzhou", 	  "LTAI5t**********AWVNP", "qgzyEW4cA*******PBl7iyw")
	request := dysmsapi.CreateSendSmsRequest()       //创建请求
	request.Scheme = "https"                         //请求协议
	request.PhoneNumbers = "155******85"             //接收短信的手机号码
	request.SignName = "教程"                         //短信签名名称
	request.TemplateCode = "SMS_******236"           //短信模板ID
	par, err := json.Marshal(map[string]interface{}{ //定义短信模板参数（具体需要几个参数根据自己短信模板格式）
		"code": "123456",
	})
	request.TemplateParam = string(par)      //将短信模板参数传入短信模板
}
调用SendSms（）方法发送请求，并处理错误
```
调用SendSms（）方法发送请求，并处理错误

```go
package main

import (
    "encoding/json"
	"github.com/aliyun/alibaba-cloud-sdk-go/services/dysmsapi"
)

func main() {
    //参数一：连接的节点地址（有很多节点选择，这里我选择杭州）
    //参数二：AccessKey ID
    //参数三：AccessKey Secret
	client, err := dysmsapi.NewClientWithAccessKey("cn-hangzhou", 	"LTAI5t**********AWVNP", "qgzyEW4cA*******PBl7iyw")
	request := dysmsapi.CreateSendSmsRequest()       //创建请求
	request.Scheme = "https"                         //请求协议
	request.PhoneNumbers = "155******85"             //接收短信的手机号码
	request.SignName = "教程"                         //短信签名名称
	request.TemplateCode = "SMS_******236"           //短信模板ID
	par, err := json.Marshal(map[string]interface{}{ //定义短信模板参数（具体需要几个参数根据自己短信模板格式）
		"code": "123456",
	})
	request.TemplateParam = string(par)      //将短信模板参数传入短信模板
 
	response, err := client.SendSms(request) //调用阿里云API发送信息
	if err != nil {                          //处理错误
		fmt.Print(err.Error())
	}
	fmt.Printf("response is %#v\n", response) //控制台输出响应
}
```
#### 测试

```go
client, err := dysmsapi.NewClientWithAccessKey("cn-hangzhou",
   "LTAI5t5Zg34ubpUzEmUoT5at",
   "lzKeLpayDR7Brh35jSdSGXxSCHd9LC")

request := dysmsapi.CreateSendSmsRequest()       //创建请求
request.Scheme = "https"                         //请求协议
request.PhoneNumbers = "18873288626"             //接收短信的手机号码
request.SignName = "小微书"                         //短信签名名称
request.TemplateCode = "SMS_472665076"           //短信模板ID
par, err := json.Marshal(map[string]interface{}{ //定义短信模板参数（具体需要几个参数根据自己短信模板格式）
   "code": "123456",
})
request.TemplateParam = string(par)      //将短信模板参数传入短信模板
response, err := client.SendSms(request) //调用阿里云API发送信息
if err != nil {                          //处理错误
   fmt.Print(err.Error())
}
fmt.Printf("response is %#v\n", response) //控制台输出响应
```

### 验证码服务

#### 安全问题

**控制住验证码发送频率，不至于一下子发送几百万条。**

• 同一个手机号码，一分钟以内只能发送一次。
• 验证码有效期十分钟。
• 本身整个系统也有限流，也可以保护住系统。

**验证码不能被攻击者暴力破解，因为验证码只有 6 位，也就是只有十万种可能，所以不能让用户频繁输入验证码来暴力破解。**

• 一个验证码，如果已经验证通过了，那么就不能再用。
• 一个验证码，如果已经三次验证失败，那么这个验证码就不再可用。在这种情况下，只会告
诉用户输入的验证码不对，但是不会提示验证码过于频繁失败问题。

#### 功能分析

- 前端输入手机号码点击获取验证码后向后端发送请求
- 后端生成一个6位随机数作为验证码，先将其存入redis中
- **存入redis**时，key为`biz(用途):phone`，val为验证码。首先获取biz:phone的过期时间
  - 如果key存在，但没有过期时间，表明系统错误
  - 如果key存在且过期时间表明已经过了1分钟 或者 key不存在，那么可以存入并**设置过期时间为10分钟，set(cntKey, 3)表示可验证的次数为3**
  - 如果key存在且时间没过1分钟，发送太频繁，不能存入
- 如果后端验证码存入成功后，就调用手机短信服务将验证码发送给对应手机用户
- 用户在填写验证码后点击确认，**后端进行校验**
  - 如果get(cntKey) <=0 即校验次数用完了，验证码校验次数用完
  - 或者验证码与redis中的不符即用户输错
  - 验证成功，set(cntKey, -1), 该验证码不可用

#### 并发问题

假如多个线程同时查询验证码，这些线程都没查到验证码或者已经过了1分钟，就都会更小验证码，那么久只有一个客户端能够成功登录，其他验证码输对了但是登录失败。

##### 解决方法

分布式锁（不推荐）

**参考**：redis结合lua脚本解决多线程并发安全问题：https://blog.csdn.net/incredible1024/article/details/130093799

我们要考虑的就是在 Redis 层面上实现。利用 lua 脚本将我们的检查并且做某事的逻辑封装成一个整体操作。为什么 Redis 是安全的？因为 Redis 是单线程的。

**不同线程的 lua 脚本是依次执行的**。也就是说，**只有一个线程原子性的多个操作执行完，下一个线程才可以执行**。实际上也是保证了在 redis 内部不同线程操作的串行执行，从而能够解决并发安全问题。

<img src="assets/image-20240908172153926.png" alt="image-20240908172153926" style="zoom: 25%;" />

### 短信验证码登录

#### 验证码登录接口

整体来说，我们需要两个 HTTP 接口：

**• 触发发送验证码的接口。**
**• 校验验证码的接口。**

我都定义到了 UserHandler 里面。也就是 UserHandler 通过聚合 CodeService来实现验证码登录。

**如图**

<img src="assets/image-20240909083034969.png" alt="image-20240909083034969" style="zoom: 25%;" />

#### 发送验证码核心代码

```go
err := u.codeSvc.Send(ctx, "login", req.Phone)
if err != nil {
   if err == service.ErrCodeSendTooMany {
      ctx.JSON(http.StatusOK, Result{
         Code: 5,
         Msg:  "发送太频繁",
      })
   } else if err == service.ErrCodeVerifyTooMany {
      ctx.JSON(http.StatusOK, Result{
         Code: 5,
         Msg:  "验证太频繁",
      })
   } else {
      ctx.JSON(http.StatusOK, Result{
         Code: 5,
         Msg:  "系统错误",
      })
   }
   return
}
ctx.JSON(http.StatusOK, Result{
   Code: 5,
   Msg:  "发送成功",
})
```

#### 校验验证码核心

```go
//通过正则表达式判断是否是一个合法的电话号码
ok, _ := u.phoneExp.MatchString(req.Phone)
if !ok {
   ctx.JSON(http.StatusOK, Result{
      Code: 4,
      Msg:  "输入有误",
   })
}
//这里也可以先初步校验一下验证码
ok, err := u.codeSvc.Verify(ctx, "login", req.Phone, req.Code)
if err != nil {
   ctx.JSON(http.StatusOK, Result{
      Code: 5,
      Msg:  "系统错误",
   })
   return
}
if !ok {
   ctx.JSON(http.StatusOK, Result{
      Code: 4,
      Msg:  "验证有误",
   })
   return
}
ctx.JSON(http.StatusOK, Result{
   Code: 4,
   Msg:  "验证码校验成功",
})
```

#### 实现登录核心

##### FindOrCreate

从数据库中根据phone查找用户，如果user不存在就创建进行注册，存在就返回。

也可以不查找，直接insert用phone注册用户，这样如果发生了phone冲突，那么就表明用户存在直接返回

```go
func (svc *UserService) FindOrCreate(ctx *gin.Context, phone string) (domain.User, error) {
   //这个叫快路径，数据库中找到了就不用去创建了
   //快路径可以不执行，直接执行慢路径，直接去创建，冲突则创建失败
   u, err := svc.repo.FindByPhone(ctx, phone)
   //找到了 或者 报错
   if err != repository.ErrUserNotFound {
      return u, err
   }
   /* //在系统资源不足，触发降级之后，不执行慢路径了
      if ctx.Value("降级") == "true" {
         return domain.User{}, errors.New("系统降级了")
      }*/
   //这个叫慢路径
   //没有找到，就用手机号码注册一个新用户
   err = svc.repo.Create(ctx, domain.User{
      Phone: phone,
   })
   //手机号冲突就说明用户存在
   if err != nil && err != ErrUserDuplicate {
      return domain.User{}, err
   }
   //查找新创建的或已存在的用户，获取id
   return svc.repo.FindByPhone(ctx, phone)
}
```

登录或注册成功后，同样设置jwt-token

##### 唯一索引问题

phone和email都是唯一索引，而当分别使用email和phone注册账号后，对应账号的phone和email分别为空，因为唯一索引不能有多个空值，**所以手机号和邮箱设置为`sql.NullString`，表示可以为空**

```go
type NullString struct {
   String string
   Valid  bool // Valid is true if String is not NULL
}
```

```go
// User 直接对应数据库表结构，这个User是存储时候的User
type User struct {
   Id int64 `gorm:"primaryKey, autoIncrement"`
   //邮箱，用户唯一但可以为空
   Email    sql.NullString `gorm:"unique"`
   Password string
   //手机号，用户唯一但可以为空
   Phone sql.NullString `gorm:"unique"`
   //创建时间，毫秒
   Ctime int64
   //更新时间，毫秒
   Utime int64
}
```

##### 扩展

之所以我们可以用 sql.NullString，是因为它实现了两个接口：sql.Scanner 和 driver.Valuer。

![db123](assets/db123.png)

### 依赖注入

你仔细看我们的初始化代码，就会发现整体来说可以分成几个步骤：
• 初始化第三方依赖，也就是 DB、Redis 等。
• 用 DB、Redis 等来初始化 DAO、Cache。
• 用 DAO、Cache 初始化 Repository。
• 用 Repository 初始化 Service。
• 用 Service 初始化 Handler。
• 初始化 Gin，注册路由。
• 初始化结束。

整个过程就是层层组装，也是标准的依赖注入写法。所谓的依赖注入，是指 A 依赖于 B，也就是 A 要调用 B 上的方法，那么 A 在初始化的时候就要求传入一个构建好的 B。

**依赖注入写法**：不关心依赖是如何构造的。

**非依赖注入写法**：必须自己初始化依赖，比如说 Repository 需要知道如何初始化 DAO 和 Cache。由此带来的缺点是：

• 深度耦合依赖的初始化过程。
• 往往需要定义额外的 Config 类型来传递依赖所需的配置信息。
• 一旦依赖增加新的配置，或者更改了初始化过程，都要跟着修改。
• 缺乏扩展性。
• 测试不友好。
• 难以复用公共组件，例如 DB 或 Redis 之类的客户端。

#### Wire

依赖注入的缺点就是要在系统初始化的过程中（main函数），完成复杂冗长的构造链表。所以我们可以借助一些中间件来帮助我们完成这些复杂又没有多少技术含量的事情。在 Go 里面，我推荐使用 wire。wire 分成两部分，**一个是在项目中使用的依赖，一个是命令行工具。**

**安装 Wire**

```bash
go install github.com/google/wire/cmd/wire@latest
```

**项目依赖**

```go
"github.com/google/wire"
```

**简单示例**

wire 使用有两个基本步骤：

• 往 wire 里面注册各种初始化方法。
• 运行 `wire` 命令。

```go
//go:build wireinject

package wire

import (
   "geektime/wire/repository"
   "geektime/wire/repository/dao"
   "github.com/google/wire"
)

func InitUserRepository() *repository.UserRepository {
   wire.Build(repository.NewUserRepository, InitDB, dao.NewUserDAO)
   return new(repository.UserRepository)
}

func InitDB() *gorm.DB {
   db, err := gorm.Open(mysql.Open("xxx"))
   if err != nil {
      panic(err)
   }
   return db
}
```

```go
type UserDAO struct {
	db *gorm.DB
}

func NewUserDAO(db *gorm.DB) *UserDAO {
	return &UserDAO{
		db: db,
	}
}
```

```go
type UserRepository struct {
   dao *dao.UserDAO
}

func NewUserRepository(d *dao.UserDAO) *UserRepository {
   return &UserRepository{
      dao: d,
   }
}
```

在wire文件目录执行wire命令即可，自动生成如下文件wire_gen，调用这个方法即可

```go
// Code generated by Wire. DO NOT EDIT.

//go:generate go run -mod=mod github.com/google/wire/cmd/wire
//go:build !wireinject
// +build !wireinject

package wire

import (
	"geektime/wire/repository"
	"geektime/wire/repository/dao"
)
// Injectors from wire.go:
func InitUserRepository() *repository.UserRepository {
	db := InitDB()
	userDAO := dao.NewUserDAO(db)
	userRepository := repository.NewUserRepository(userDAO)
	return userRepository
}
```

#### 组装核心代码

```go
//go:build wireinject

package main

import (
   "geektime/webook/internal/repository"
   "geektime/webook/internal/repository/cache"
   "geektime/webook/internal/repository/dao"
   "geektime/webook/internal/service"
   "geektime/webook/internal/web"
   "geektime/webook/ioc"
   "github.com/gin-gonic/gin"
   "github.com/google/wire"
)

func InitWebServer() *gin.Engine {
   wire.Build(
      //第三方依赖
      ioc.InitDB, ioc.InitRedis,
      //dao
      dao.NewUserDao,
      //cache
      cache.NewUserCache, cache.NewCodeCache,
      //repository
      repository.NewUserRepository, repository.NewCodeRepository,
      //service
      ioc.InitSMSService,
      service.NewCodeService, service.NewUserService,
      //handler
      web.NewUserHandler,
      ioc.InitMiddlewares,
      ioc.InitWebServer,
   )
   return new(gin.Engine)
}
```

#### wire缺点

 wire 的缺点：

• 缺乏根据环境使用不同实现的能力。
• 缺乏根据接口查找实现的能力。
• 缺乏根据类型查找所有实例的能力。

这也就导致我们的依赖注入的代码，始终做不到非常清爽。一句话，wire 也就是矮个子里面挑高个。wire 最大的好处就是很清晰，可控性非常强。

#### IoC：控制反转

**依赖注入是控制反转的一种实现形式**。还有一种叫做**依赖发现。比如说 A 调用 B，然后 A 自己去**
**找到可用的 B，那就是依赖发现。**简单来说，如果 A 用 B 接口的时候，需要自己创建一个，
比如说 UserHandler 里面自己创建一个 UserService 的实现，那么 UserHandler 就和 UserService 的实现耦合在一起了。

**控制反转的意思就是，UserHandler 不会去初始化UserService。要么外面传入 UserService 给**
**UserHandler，这种叫做依赖注入；要么是 UserHandler自己去找一个 UserService，这种叫做依赖查找。**

### 面向接口编程

面向接口编程是指将应用程序定义为组件的集合，组件与组件之间的通信必须通过接口。简单来说，就是如果你要用到另外一个类型，那么你肯定用的是接口。比如说 A 调用 B 的时候，B 必须是一个接口。结合依赖注入，就是写代码的时候 A 调用 B，B 是一个接口。而后在初始化的时候，注入一个实现了 B 接口的实例。

#### 示例

CodeService 对 sms.Service 的用法就是面向接口编程。所以你可以注入不同的实现，比如说腾讯云的实现，或者基于内存的实现。**sms.Service是一个接口，sms可以是不同的实例，比如阿里云、腾讯、本地内存的实现。**

```go
type CodeService interface {
	// Send bix 区别业务场景，用于什么的验证码
	Send(ctx context.Context, biz, phone string) error
	Verify(ctx context.Context,
		biz, phone, inputCode string) (bool, error)
}

type codeService struct {
	repo repository.CodeRepository
	sms  sms.Service
}
```

#### 为什么要面向接口编程

![面向接口.drawio](assets/面向接口.drawio.png)

**为了扩展性**。但面向接口编程，能够有效让你的代码充满扩展性，正如 sms.Service，你可以随时提供一个基于内存的实现、基于腾讯的实现。

#### 现有代码改造

##### 改造dao

实现一个GORMUserDAO，后续可能使用sqlx，原生sql等等其他数据库的实现

```go
type UserDAO interface {
   Insert(ctx context.Context, u User) error
   FindByEmail(ctx context.Context, email string) (User, error)
   FindById(ctx context.Context, id int64) (User, error)
   FindByPhone(ctx *gin.Context, phone string) (User, error)
}

type GORMUserDAO struct {
   db *gorm.DB
}

func NewUserDao(db *gorm.DB) UserDAO {
   return &GORMUserDAO{
      db: db,
   }
}
```

##### 改造 UserCache

这里实现只有一个，叫做 RedisUserCache，意思就是它是基于 Redis 来实现的。这意味着将来你可以：

• 提供本地缓存实现。
• 提供 Memcache 实现。
• 提供本地缓存 + Memcache 双重缓存的实现。
• 甚至于还可以进一步提供各种缓存模式的实现。

##### 改造 UserRepository

这里实现只有一个，叫做CachedUserRepository，意思就是使用了缓存
的实现。将来你可以考虑：

• 不使用缓存，或者使用不同的缓存。
• 甚至不用数据库，使用 NoSQL 啥的。

### 单元测试

#### 简单使用与介绍

各种环境全部运行起来后，出了bug，修该代码后，又要重新运行所有环境，非常麻烦。因此需要测试。正常的业务开发之后的测试流程，都是先单元测试，后集成测试。

• 单元测试：针对每一个方法进行的测试，单独验证每一个方法的正确性。

• 集成测试：多个组件合并在一起的测试，验证各个方法、组件之间配合无误。

##### go中使用单元测试

在 Go 中编写单元测试，很简单：

**• 文件名以 _test.go 结尾。**

**• 测试方法以 Test 为开头。**

**• 测试方法只接收一个参数： t *testing.T。**

可以在 IDE 中直接运行。注意，你同样可以用这个来写集成测试、冒烟测试、回归测试，都可以。

**运行模式**：

• Run：运行模式，直接运行整个测试。

• Debug：Debug 模式，你可以打断点。

• Run xxx with Coverage：运行并且输出测试覆盖率。

• 其它 Profile 都是性能分析，很少用。

**命令行运行单元测试**

一般你在考虑集成 CI 的时候，就会使用命令行来执行单元测试。
使用 go test 命令：

• go test your_test.go: 这是跑单一一个文件的测试，但你需要把 your_test.go 里面用到的文件都选进来。

• go test . :运行这个包的测试，里面所有的测试都会执行。

• go test ./... : 运行这个包的测试，包括子目录的。

##### Table Driven 模式

Go 里面，惯常的组织测试的方式，都是用 TableDriven。Table Driven 的形式主要分成三个部分：

• 测试用例的定义：即每一个测试用例需要有什么。

• 具体的测试用例：你设计的每一个测试用例都在这里。

• 执行测试用例：这里面还包括了对测试结果进行断言。

注意，你要优先使用 Table Driven，但是不用强求。你把测试用例定义看做是列名，每一个测试用例就是一行数据，就能理解 Table Driven 这个含义了。

<img src="assets/image-20240909221626534.png" alt="image-20240909221626534" style="zoom:25%;" />

##### 测试用例定义

我总结的测试用例定义，最完整的情况下应该包含：

• **名字**：简明扼要说清楚你测试的场景，建议用中文。
• **预期输入**：也就是作为你方法的输入。如果测试的是定义在类型上的方法，那么也可以包含类型实例。
• **预期输出**：你的方法执行完毕之后，预期返回的数据。如果方法是定义在类型上的方法，那么也可以包含执行之后的实例的状态。
• **mock**：每一个测试需要使用到的 mock 状态。单元测试里面常见，集成测试一般没有。
• **数据准备**：每一个测试用例需要的数据。集成测试里常见。
• **数据清理**：每一个测试用例在执行完毕之后，需要执行一些数据清理动作。集成测试里常见。

最完整的意思是：如果你要测试的方法很简单，那么你用不上全部字段。



#### 测试Handler

##### 构造 HTTP 请求

因为这里还是单元测试，所以没有必要真的经过网络发起 HTTP 请求，而是可以考虑使用编程手段，构造HTTP 请求。

我们可以通过控制 HTTP 方法、URL 和传入的Body，构造出任何一种情况。

```go
//构造http请求
req, err := http.NewRequest(http.MethodPost, "/users/signup",
   bytes.NewBuffer([]byte(`
   {
     "email": "123@qq.com",
     "password": "hello@123",
     "confirm_password": "hello@123"
   }
   `)))
```

##### 验证HTTP响应

httptest 包里面已经提供了这么一个东西，可以帮助我们记录响应。也就是 httptest.Recorder。从右图中你也能看出来，通过这个 Recorder，我们能够知道，响应究竟写了些什么。

```go
//初始化一个响应体
resp := httptest.NewRecorder()
//gin会处理这个请求
//响应写回到resp里
server.ServeHTTP(resp, req)
```

##### mock工具

serHandler 在初始化的时候，是需要传入 UserService 和 CodeService 的，怎么办？如果单元测试也要构造这两个 Service，岂不是得一路构造到 DAO，然后就需要在数据库中准备数据？

所以可以用到mock工具生成测试用的模拟UserService 和 CodeService。，模拟一下调用获取返回值即可。它分成两个部分：

• mockgen：命令行工具。
• 测试中使用的控制 mock 对象的包。

##### **安装命令行工具：**

```go
go install go.uber.org/mock/mockgen@latest
```

##### 生成mock实现

mockgen 一般要指定三个参数：

• source：也就是你接口所在的文件。
• destination：也就是你生成代码的目标路径。
• package：也就是生成代码的文件的 package。

例如这个命令

```go
mockgen -source=./internal/service/user.go -package=svcmocks -destination=./internal/service/mocks/user.mock.go
```

后续就可以在测试中使用user.mock.go了

##### mock使用

**主要对测试的东西使用的依赖使用mock模拟**，使用起来很简单，都是一样的套路：

- **初始化控制器，这里是 ctrl。**
- **创建模拟的对象，这里是 usersvc。**
- **设计模拟调用**
  - **先调用 EXPECT，不要忘了。**
  - **调用同名方法，传入模拟的条件。**
  - **指定返回值。**

注意事项：你设计了几个模拟调用，在使用的时候就都要用上，而且顺序也要对上！！！！不能多！！！
不能少！！！不能乱！！！记住一条原则：**后续但凡出现和 mock 有关的错误，你就检查多了没，少了没，漏了没，条件对上了没。**

```go
func TestMock(t *testing.T) {
   //先创建一个mock的控制器
   ctrl := gomock.NewController(t)
   //每个测试结束都要调用Finish
   //然后mock就会验证你的验证流程是否符合预期
   defer ctrl.Finish()
   //调用user.mock.go文件中的方法生成模拟的service
   usersvc := svcmocks.NewMockUserService(ctrl)
   //开始设计一个Signup的模拟调用
   //模拟的条件是gomock.Any,gomock.Any
   //然后设置Signup的返回值
   usersvc.EXPECT().SignUp(gomock.Any(), domain.User{
      Email: "123@qq.com",
   }).Return(errors.New("mock error"))

   err := usersvc.SignUp(context.Background(), domain.User{
      Email: "123@qq.com",
   })
   t.Log(err)
}
```

##### handler测试核心

一个测试的执行过程就是：

• 准备一个 gin.Engine，并且注册路由。
• 准备请求。
• 准备接收响应的 Recorder。
• 直接发起调用，也就是假装收到了 HTTP 请求。
• 比较 Recorder 里面记录的响应。

注意，当你调用 ServeHTTP 的时候，效果等价于server 真的从网络里面收到了一个请求。

```go
t.Run(tc.name, func(t *testing.T) {
   //先创建一个mock控制器
   ctrl := gomock.NewController(t)
   //每个测试结束都要调用Finish
   //然后mock就会验证测试流程是否符合预期
   defer ctrl.Finish()

   //获取默认的gin对象
   server := gin.Default()
   //创建模拟对象，并设置模拟调用
   h := NewUserHandler(tc.mock(ctrl), nil)
   h.RegisterRoutes(server)
   //构造http请求
   req, err := http.NewRequest(http.MethodPost, "/users/signup",
      bytes.NewBuffer([]byte(tc.reqBody)))
   require.NoError(t, err)
   //数据是json格式
   req.Header.Set("Content-Type", "application/json")

   //初始化一个响应体
   resp := httptest.NewRecorder()
   //gin会处理这个请求
   //响应写回到resp里
   server.ServeHTTP(resp, req)

   t.Log(resp)
   assert.Equal(t, tc.wantCode, resp.Code)
   assert.Equal(t, tc.wantBody, resp.Body.String())
})
```

##### 定义测试用例

根据Table Driven 中的匿名结构体的字段设计用例即可。

```go
{
   name: "注册成功",
   mock: func(ctrl *gomock.Controller) service.UserService {
      usersvc := svcmocks.NewMockUserService(ctrl)
      usersvc.EXPECT().SignUp(gomock.Any(), domain.User{
         Email:    "123@qq.com",
         Password: "hello@123",
      }).Return(nil)
      return usersvc
   },
   reqBody: `{       
      "email": "123@qq.com",
      "password": "hello@123",
      "confirm_password": "hello@123"
      }`,
   wantCode: 200,
   wantBody: "注册成功",
},
```

mock字段是一个函数，在其中使用mock，初始化控制器，创建模拟对象，设计模拟调用。

reqBody是请求体

##### 步骤总结

- 使用mock工具生成测试所需的依赖，比如handler的service，service的dao
- 用例匿名结构体定义。
- 实现并执行测试用例的整体代码。
- 设计测试用例运行

#### 测试dao

和其它层次的测试不一样，DAO 的测试要使用一个新的工具 —— `sqlmock` 来测试。因为 GORM 本身并没有提供一个接口给我们操作，导致我们没办法生成 mock 代码。为什么不用基于文件/内存的数据库来执行单元测试呢？比如说用 sqlite?因为，只要真的使用数据库，你就要操心准备数据的事情了，效率太低。

```go
go get github.com/DATA-DOG/go-sqlmock
```

基本用法：

• 用 sqlmock 来创建一个 db。
• 设置模拟调用。
• 使用 db 来测试代码：在使用 GORM 的时候

就是让 GORM 使用这个 db。



### 集成测试

单元测试只能确保任何一个组件单独看是没有任何问题的。但是组合在一起是否有问题，就需要
集成测试了。

在日常开发中，这一步主要是由测试来进行的。但你也可以考虑先提前用集成测试验证一下自己
维护的所有模块。

创建一个包integration

![image-20240911191433251](assets/image-20240911191433251.png)

**使用wire初始化所有依赖，测试短信的发送功能** 

#### 定义测试用例

整个测试用例分成了几个部分：
• name： 测试名字。
• before：准备数据。
• after：验证并且删除数据。防止对业务
• phone：前端只需要传入一个手机号码，所以这里我就直接定义了。标准做法是这里定义http.Request。
• 预期响应
• wantCode：HTTP 状态码。
• wantResult：预期数据，因为直接比较JSON 很麻烦，所以直接比较 web.Result。

#### 运行代码

运行测试用例整个代码分成几部分：
• 执行 before：也就是准备数据。
• 构建 req。
• 准备 recorder。
• 调用 ServeHTTP。
• 比较结果：比较响应码，将响应数据反序列化为 JSON 之后再比较。

## 第三方服务治理（面试）

### 整体思路

要知道，一切不在你控制范围内的东西，都是不定时炸弹，冷不丁就炸了。所以，针对一切跟第三方打交道的地方，都要做好容错。推而广之，只要接口不是你写的，你调用的时候都要考虑做好治理。

核心思路就是两点：

• 尽量不要搞崩第三方。
• 万一第三方崩了，你的系统还要能够稳定运行。

具体到短信服务这里： 

• 短信服务商都有保护自己系统的机制，你要小心不要触发了。比如说短信服务商的限流机制。
• 短信服务商可能崩溃，你和短信服务商之间的网络通信可能崩溃，所以你要想好容错机制。

### 客户端限流

在腾讯的短信 API 里面，它系统本身的限流阈值是3000/s 。对于你的系统来说，如果你一秒内发送了 3000 个请求，都能处理。但是如果你超过了 3000 次，就会被拒绝。**因此，与其等着腾讯那边返回限流响应，还不如你直接本地就限制住。进行客户端限流**

#### 滑动窗口限流大致实现原理

- 使用redis的zset数据结构，设置一个以时间为单位的窗口参数可以是1s，zset的key为限流的服务
- 对于一个请求，首先在redis获取当前窗口内的请求数量 `zcount(key, now-window, +inf)` 判断有没有超过阈值
  - 超过了就执行限流，直接返回true表示要执行限流
  - 否则没被限流就在redis中添加一个元素，存储当前时间 `zadd(key, now, now)`

#### 在已有代码中加入限流器

这里我们对发送短信服务进行限流，在ailiyun服务send代码中加入限流功能

```go
type Service struct {
	signName string
	client   *dysmsapi.Client
	tplId    string
    limiter ratelimit.LimitSMSService
}

func NewService(signName string, client *dysmsapi.Client, tplId string, limiter ratelimit.LimitSMSService) *Service {
	return &Service{
		signName: signName,
		client:   client,
		tplId:    tplId,
        limiter: limiter,
	}
}

func (s Service) Send(ctx context.Context, args []string, numbers ...string) error {
    limited, err := s.limiter.Limit(ctx, "sms:tencent")
	//系统错误
	if err != nil {
		//可以限流: 保守策略；下游很坑的时候
		//可以不限: 下游很强，或业务可用性要求很高，尽量容错策略

		//这里采用保守策略
		//包装错误
		return fmt.Errorf("短信服务判断是否限流出现错误 %w", err)
	}
	//发生了限流
	if limited {
		return errLimited
	}
	err = s.svc.Send(ctx, args, numbers...)
    if err != nil {
        return err
    }
   //创建请求
   req := dysmsapi.CreateSendSmsRequest()
   。。。。。。。。
```

保持依赖注入风格，要求初始化腾讯短信服务实现的时候，传入一个限流器。在真的调用腾讯短信 API 之前，检查一下是否触发限流了。

仔细分析你会想到：
• 如果将来我有别的短信服务商，别的短信服务商要不要限流？显然要。
• 要是再有类似的功能，岂不是还得继续修改Send 方法？改来改去，这个 Send 方法不就成了屎山吗？

所以就用到了装饰器模式，在一个服务上添加功能

#### 利用装饰器模式改进

##### 实现方法

利用装饰器重新将短信服务封装，并加上限流功能，避免了直接在短信服务上做修改

**封装方式也很简单，创建一个新的限流短信服务实现短信服务接口，封装好短信服务，添加pkg包中定义好的限流器进行装饰**

```go
// LimitSMSService 装饰器模式使用发送短信服务
// 采用了redis滑动窗口限流
type LimitSMSService struct {
   svc     sms.Service //封装短信服务
   limiter ratelimit.Limiter
}

func NewLimitSMSService(svc sms.Service, limiter ratelimit.Limiter) *LimitSMSService {
   return &LimitSMSService{
      svc:     svc,
      limiter: limiter,
   }
}

// Send 装饰这个方法
func (s LimitSMSService) Send(ctx context.Context, args []string, numbers ...string) error {
   //加一些代码，新特性
   //使用redis限流功能
   limited, err := s.limiter.Limit(ctx, "sms:tencent")
   //系统错误
   if err != nil {
      //可以限流: 保守策略；下游很坑的时候
      //可以不限: 下游很强，或业务可用性要求很高，尽量容错策略

      //这里采用保守策略
      //包装错误
      return fmt.Errorf("短信服务判断是否限流出现错误 %w", err)
   }
   //发生了限流
   if limited {
      return errLimited
   }
   err = s.svc.Send(ctx, args, numbers...)
   return err
}
```

##### 另一种实现方式

也可以考虑使用组合来实现装饰器。limitSMSServiceV1 就是组合的做法。两种方法对比：
**使用组合：**直接继承

- 用户可以直接访问 Service，绕开你装饰器本身。
- 可以只实现 Service 的部分方法。

**不使用组合：**实现接口

- 可以有效阻止用户绕开装饰器。
- 必须实现 Service 的全部方法

```go
type limitSMSServiceV1 struct {
   sms.Service
   limiter ratelimit.Limiter
}
```

#### 降级策略（面试要点）

如果限流中redis崩了返回了err不为nil, 这里有一个问题，要不要采用限流？

- 可以限流: **保守策略**；下游很坑的时候
- 可以不限: 下游很强，或**业务可用性要求很高**，尽量容错策略

```go
if err != nil {
   //系统错误
   //可以限流: 保守策略；下游很坑的时候
   //可以不限: 下游很强，或业务可用性要求很高，尽量容错策略
   
   //这里采用保守策略
   //包装错误
   return fmt.Errorf("短信服务判断是否限流出现错误 %w", err)
}
```

### 自由切换短信服务商（面试）

#### 引入

前面的限流是为了不触发服务商的自我保护机制。但是，万一服务商真的出了问题呢？即便服务商的可用性做到了四个九，也还是有小概率崩溃，怎么办？可以考虑在服务商出了问题的时候，**切换到新的服务商。**

> 怎么知道服务商出了问题？

问题在于，你怎么知道服务商出现问题了？你无法准确知道，但是可以大概猜测一下。
• 频繁收到超时响应。
• 收到 EOF 响应或者 UnexpectedEOF 响应。
• 响应时间很长。
• ……

#### 第一种策略：failover

如果出现错误了，就直接换一个服务商，进行重试。一样可以使用装饰器模式来实现。

##### 第一种实现

使用for循环发送短信，如果发送成功就退出

**缺点：**

• 每次都从头开始轮询，**绝大多数请求会在svcs[0] 就成功，负载不均衡。**
• 如果 svcs 有几十个，轮询都很慢。

```go
// FailoverSMSService 自动切换服务商
// 第一种策略，直接轮询
type FailoverSMSService struct {
   //短信服务各种不同的实现，不同的服务商
   svcs []sms.Service
   //当前服务商下标
   idx uint64
}

func NewFailoverSMSService(svcs []sms.Service) *FailoverSMSService {
   return &FailoverSMSService{svcs: svcs}
}

func (f FailoverSMSService) Send(ctx context.Context, args []string, numbers ...string) error {
    //发送失败就选择下一个服务商的短信服务发送
   for _, svc := range f.svcs {
      err := svc.Send(ctx, args, numbers...)
      //发送成功
      if err == nil {
         return nil
      }
      //发送失败需要打印日志
      //这里需要监控
   }
   return errors.New("发送失败，所有服务商都尝试过了")
}
```

##### 改进写法

起始 svc 是动态计算的。区别了错误：context.DeadlineExceeded 和context.Canceled 都是跟用户体验密切相关的，所以我们直接返回。

这边你不需要考虑 idx 溢出的问题，uint64 让你发短信发到天荒地老都用不完。

**发送流程**

**每次请求都是从当前的服务商下一个服务商开始轮询发送短信**

<img src="assets/image-20240912160637256.png" alt="image-20240912160637256" style="zoom: 33%;" />

```go
func (f FailoverSMSService) SendV1(ctx context.Context, args []string, numbers ...string) error {
	//我们取下一个节点作为起始节点，让负载均衡
	//注意使用原子操作
	idx := atomic.AddUint64(&f.idx, 1)
	length := uint64(len(f.svcs))
	//遍历所有服务商
	for i := idx; i < idx+length; i++ {
		svc := f.svcs[int(i%length)]
		err := svc.Send(ctx, args, numbers...)
		switch err {
		case nil:
			return nil
		case context.DeadlineExceeded, context.Canceled:
			//调用者超时或调用者主动取消了
			return err
		default:
			//输出日志
		}
	}
	return errors.New("发送失败，所有服务商都尝试过了")
}
```

#### 第二种策略：动态判定服务商状态

##### 基于超时响应的判定

前面的轮询 failover 是非常简单的策略。另外一个策略就是真的计算服务商是否还运作正常。常用的判断标准有：

• 错误率：例如连续 N 个超时响应，错误率超过 10%。
• 响应时间增长率：例如响应时间从 100ms 突然变成 1s。

你可以根据自己调用的服务商的特点来确定使用哪个。比如cpu密集型，内存型

**这里用一个比较简单的算法：只要连续超过 N 个请求超时了**，就直接切换。如果有多个服务商，那么就是在这些服务商之间逐个切换过去。

##### 具体实现

在timeout_failover下利用装饰器模式新建service

在发送短信时进行判断，如果超时次数大于阈值，切换当前服务商（考虑并发问题），调用短信服务

- 超时，将当前的超时次数加1
- 正常发送，没有连续超时了，将连续超时次数置0

```go
// TimeoutFailoverSMSService
// 第二种策略
type TimeoutFailoverSMSService struct {
	//所有服务商
	svcs []sms.Service
	idx  int32
	//连续超时的个数
	cnt int32
	//阈值
	//连续超时超过这个数字，就切换服务商
	threshold int32
}

func NewTimeoutFailoverSMSService(svcs []sms.Service, cnt int32) *TimeoutFailoverSMSService {
	return &TimeoutFailoverSMSService{svcs: svcs, cnt: cnt}
}

func (t TimeoutFailoverSMSService) Send(ctx context.Context, args []string, numbers ...string) error {
   idx := atomic.LoadInt32(&t.idx)
   cnt := atomic.LoadInt32(&t.cnt)
   //超时次数大于阈值，切换当前服务商
   if cnt > t.threshold {
      newIdx := (idx + 1) % int32(len(t.svcs))
      //并发问题
      //不能直接用atomic.AddInt32(&t.idx, 1)，不然一个线程切换一次
      //而是用CAS, 如果有一个执行了切换，那么后面线程就不用切换了
      if atomic.CompareAndSwapInt32(&t.idx, idx, newIdx) {
         //将cnt置0
         atomic.StoreInt32(&t.idx, 0)
      }
      // else 就是并发问题，其他线程已经将t.idx+1
      // 只要使用这个t.idx即可
      //idx = newIdx
      idx = atomic.LoadInt32(&t.idx)
   }

   svc := t.svcs[idx]
   err := svc.Send(ctx, args, numbers...)
   switch err {
   case nil:
      //正常发送了消息，连续超时次数置0
      atomic.StoreInt32(&t.cnt, 0)
      return nil
   case context.DeadlineExceeded, context.Canceled:
      //调用者超时，将超时次数cnt+1
      atomic.AddInt32(&t.cnt, 1)
      return err
   default:
      //其他错误
      return err
   }
}
```

#### 并发安全问题

**记住一个原则**：**`任何变量的任何操作，在没有并发控制的情况下，都不是并发安全的。`**如下图，第二个goroutine应该将f.idx写成4，但确写为了3

![并发问题](assets/并发问题.png)

并发问题不要太过纠结，这需要多年经验

### 提高安全性

如果你在一个大公司，那么类似于短信这种资源，是需要考虑安全和成本控制的。比如说利用审批流程来申请一个 tpl（模板），里面要求业务方对自己的用量进行评估。而后要考虑：

• 只有特定的组才能使用这个 tpl。

• 这个 tpl 的容量不能超过一个数。比如说总共就是一个月十万条，多了就要额外申请，以达成成本控制的目标。

![image-20240912165013189](assets/image-20240912165013189.png)

```go
// SMSService 使用功能token提高安全性
type SMSService struct {
   svc sms.Service
   key []byte
}

func (s *SMSService) Send(ctx context.Context, tplToken string, args []string, numbers ...string) error {
   var claims SMSClaims
   //如果这里能解析成功 且 token有效，就说明是对应的业务方
   token, err := jwt.ParseWithClaims(tplToken, &claims, func(token *jwt.Token) (interface{}, error) {
      return s.key, nil
   })
   if err != nil {
      return err
   }
   if !token.Valid {
      return errors.New("token 不合法")
   }
   return s.svc.Send(ctx, claims.Tpl, args, numbers...)
}

type SMSClaims struct {
   jwt.RegisteredClaims
   Tpl string
   // 额外加字段
}
```

### 同步转异步

## 微信扫码登录

### 流程

两次跳转：跳转到微信，再跳回来

- 点击微信登录，跳转到后台**`/oauth2/wechat/authurl`**
- 后台返回微信扫码的地址，**并且在地址上附加上appId、回调地址redirectURL、和state参数**，前端重定向转到微信，前端显示二维码。
- 前端扫码确认登录后，**微信带上code参数访问回调地址redirectURL即`/oauth2/wechat/callback`**
- 后端获取到code后，**带上appId、appSecret、授权码code参数**，找微信换取真正的长时间有效的授权码**`AccessToken`**和id。
- 然后从数据库中根据openId查找或注册用户，设置token，登录成功

#### 第一步：获取授权码code

这第一步就是对应从极客时间跳到二维码那一步。需要构造一个 URL，里面带上这些参数：

• **appid**：你在微信开放平台上注册的 ID。

• **redirect_uri**：不出所料，这个就是微信跳回来的地址。

• **state**：一个随机数，用于防止跨域攻击

• **response_type**：固定为 code。

• **scope**：OAuth2中声明你要什么权限，这里是要登录权限，固定为 snsapi_login。

导包 "github.com/google/uuid" 用 uuid 来生成了 state。暂时你还不知道 state 有什么用，就先不用管。

```go
func (s *service) AuthURL(ctx context.Context) (string, error) {
   const authURLPattern = `https://open.weixin.qq.com/connect/qrconnect?appid=%s&redirect_uri=%s&response_type=code&scope=snsapi_login&state=%s#wechat_redirect`
   state := uuid.New()
   return fmt.Sprintf(authURLPattern, s.appId, redirectURL, state), nil
}
```

#### 第二步：处理跳回来的请求并获取授权码

微信跳回来的时候，**`会带上一个code和请求发送的state`** (这里校验state)，你要用这个 code 去微信里面换一个 access_token。基本上也就是发起一个调用，这里就需要传入appid + secret

获取到的响应中的数据都存在Result结构体对象中：

**授权码部分：**

• **access_token**: 你后面就可以拿着这个access_token 去访问微信，获取用户的数据。

• **expires_in**: 也就是 access_token 的有效期。

• **refresh_token**：当 access_token 过期之后，你就可以用这个 refresh_token 去找微信换一个新的access_token。

**ID 部分：**

• **open_id**：你在这个应用下的唯一 ID

• **union_id**：你在这个公司下的唯一 ID。

```go
type Result struct {
   AccessToken string `json:"access_token"`
   // access_token接口调用凭证超时时间，单位（秒）
   ExpiresIn int64 `json:"expires_in"`
   // 用户刷新access_token
   RefreshToken string `json:"refresh_token"`
   // 授权用户唯一标识
   OpenId string `json:"openid"`
   // 用户授权的作用域，使用逗号（,）分隔
   Scope string `json:"scope"`
   // 当且仅当该网站应用已获得该用户的userinfo授权时，才会出现该字段。
   UnionId string `json:"unionid"`

   // 错误返回
   ErrCode int    `json:"errcode"`
   ErrMsg  string `json:"errmsg"`
}
```

```go
// Callback 扫码后的回调方法
func (h *OAuth2WechatHandler) Callback(ctx *gin.Context) {
   code := ctx.Query("code")
   state := ctx.Query("state")
   //向微信请求 获取授权码和ID
   info, err := h.svc.VerifyCode(ctx, code, state)
    。。。
```

```go
// VerifyCode 获取授权码
func (s *service) VerifyCode(ctx context.Context,
   code string, state string) (domain.WechatInfo, error) {
   target := fmt.Sprintf(`https://api.weixin.qq.com/sns/oauth2/access_token?appid=%s&secret=%s&code=%s&grant_type=authorization_code`,
      s.appId, s.appSecret, code)
   req, err := http.NewRequestWithContext(ctx, http.MethodGet, target, nil)
   if err != nil {
      return domain.WechatInfo{}, err
   }
   //执行请求，获取响应
   resp, err := s.client.Do(req)
   if err != nil {
      return domain.WechatInfo{}, err
   }
   //读取响应中的json字符串保存到res结构体中
   decoder := json.NewDecoder(resp.Body)
   var res Result
   err = decoder.Decode(&res)
   if err != nil {
      return domain.WechatInfo{}, err
   }
   //这种写法不推荐，因为整个响应都读出来，unmarshal再读一遍，合计两遍
   //body, err := io.ReadAll(resp.Body)
   //json.Unmarshal(body, &res)
   if res.ErrCode != 0 {
      return domain.WechatInfo{},
         fmt.Errorf("微信返回错误信息 %s 错误码 %s", res.ErrMsg, res.ErrCode)
   }
   return domain.WechatInfo{
      OpenId:  res.OpenId,
      UnionId: res.UnionId,
   }, nil
}
```

#### 第三步：FindOrCreate

这也类似于短信登录：

• 如果用户是第一次登录，我们就注册一个新的账号。

• 如果用户不是第一次登录，就直接设置 JWTtoken。

```go
func (svc *userService) FindOrCreateByWechat(ctx context.Context, phone string) (domain.User, error) {
   u, err := svc.repo.FindByWechat(ctx, phone)
   //找到了 或者 报错
   if err != repository.ErrUserNotFound {
      return u, err
   }
   err = svc.repo.Create(ctx, domain.User{
      Phone: phone,
   })
   //手机号冲突就说明用户存在
   if err != nil && err != ErrUserDuplicate {
      return domain.User{}, err
   }
   //查找新创建的或已存在的用户，获取id
   return svc.repo.FindByWechat(ctx, phone)
}
```

### state用法

在前面微信的 API 里面，有一个很奇怪的参数 state：

**• 当我们跳转到微信的时候，生成了 state 并且传给了微信。**

**• 当从微信跳回来的时候，微信还是会带回来这个 state。**

#### CORS 场景

记住，理解 state 的核心是抓住攻击者让你用他的临时授权码来绑定账号。

• 攻击者首先弄出来一个绑定微信的临时授权码 A。

• 正常用户登录成功。

• 攻击者伪造一个页面，诱导用户点击，攻击者带着正常用户的 Cookie（或者 JWT token）去请求，攻击者的临时授权码 A 去绑定。

结果：在 webook 中，攻击者可以通过微信扫码登录成功，看到正常用户的数据。

#### 如何避免这个问题

在我们 webook 登录这个场景下，是不存在这个问题的。因为我们在登录的时候，是如果用户存在，就直接登录。如果不存在，就创建用户。不过我们还是可以考虑尝试解决掉这个问题，至少面试用得上。

整体思路就是：

**• 当生成 AuthURL 的时候，我们标识一下这一次的会话，将 state 和这一次的请求绑定在一起（使用cookie存state）。**

**• 等到回调回来要使用授权码之前，我们看看回调中的 state 是不是我们生成的时候用的state（通过cookie获取）。**

如果不是的话就说明被跨域攻击了

#### 记录state

在请求code时，将uuid.New()生成的state字符串存到token中

再将tokenStr存到cookie中保存到用户浏览器，标识这一次的会话

```go
func (h *OAuth2WechatHandler) setStateCookie(ctx *gin.Context, state string) error {
   //使用token存state
   token := jwt.NewWithClaims(jwt.SigningMethodHS256, StateClaims{
      State: state,
      RegisteredClaims: jwt.RegisteredClaims{
         //过期时间为预期扫码登录的时间
         ExpiresAt: jwt.NewNumericDate(time.Now().Add(time.Minute)),
      },
   })
   tokenStr, err := token.SignedString(h.cfg.StateKey)
   if err != nil {
      return err
   }
   //将stateToken存到cookie中
   ctx.SetCookie("jwt-state", tokenStr, 600, "/oauth2/wechat/callback/",
      "", h.cfg.Secure, true)
   return nil
}
```

#### 校验state

在扫码后，回调中可以通过ctx获取到存储着state的token将其与返回的state进行比较，判断是否被攻击了

```go
func (h *OAuth2WechatHandler) VerifyState(ctx *gin.Context, state string) error {
   //校验state
   //获取stateTokenStr
   ck, err := ctx.Cookie("jwt-state")
   if err != nil {
      //获取不到说明不是一个会话，被攻击了
      return fmt.Errorf("拿不到state的cookie %w", err)
   }
   var stateClaim StateClaims
   token, err := jwt.ParseWithClaims(ck, &stateClaim, func(token *jwt.Token) (interface{}, error) {
      return h.cfg.StateKey, nil
   })
   if err != nil || !token.Valid {
      return fmt.Errorf("token已经过期了 %w", err)
   }
   if stateClaim.State != state {
      return fmt.Errorf("state不相等")
   }
   return nil
}
```

### 使用长短token改进jwt

#### 长短token介绍与作用

长短 token：

• 短 token：用于访问资源，一般也叫做access_token。也就是你在微信里面接触到的access_token。

• 长 token：在短 token 过期之后，生成一个新的短token。一般也叫做 refresh_token，也就是你在微信里面接触到的 refresh_token。

**作用：**

**`如果只使用一个token的话，访问所有接口都携带这个token，容易泄露，所以说把这个token的有效期设置短点，而之所以加一个长token用来刷新短token，是因为刷新也要鉴权`**

#### 设计与实现

##### 基本步骤

• 在用户登录成功的时候，我们会颁发两个 token，一个就是已有的普通的 JWT token，充当
access_token。再额外返回一个 token，也就是refresh_token。

• 前端在发现 access_token 过期之后，要发一个刷新 token 的请求。而如果长token过期，则用户需要重新登录获取长短token。

• 前端就可以使用新的 access_token 来请求资源。

##### 登录成功返回两个token

```go
// SetJWTToken 设置短token
func (h *RedisJWTHandler) SetJWTToken(ctx *gin.Context, uid int64) error {
   claims := UserClaims{
      RegisteredClaims: jwt.RegisteredClaims{
         //设置过期时间
         ExpiresAt: jwt.NewNumericDate(time.Now().Add(time.Minute * 30)),
      },
      UserAgent: ctx.Request.UserAgent(),
      Uid:       uid,
   }
   token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
   tokenStr, err := token.SignedString([]byte("secret"))
   if err != nil {
      return err
   }
   //将token放到header中
   ctx.Header("x-jwt-token", tokenStr)
   return nil
}
```

```go
// SetRefreshToken 设置长token
func (h *RedisJWTHandler) SetRefreshToken(ctx *gin.Context, uid int64) error {
   rc := RefreshClaims{
      Uid: uid,
      RegisteredClaims: jwt.RegisteredClaims{
         ExpiresAt: jwt.NewNumericDate(time.Now().Add(h.rcExpiration)),
      },
   }
   token := jwt.NewWithClaims(h.signingMethod, rc)
   tokenStr, err := token.SignedString(RCJWTKey)
   if err != nil {
      return err
   }
   ctx.Header("x-refresh-token", tokenStr)
   return nil
}
```

##### 刷新短token

删除原本的自动刷新功能，将token的刷新设置成一个新的请求功能，并使用长token鉴权

```go
// RefreshToken 刷新短token
func (u *UserHandler) RefreshToken(ctx *gin.Context) {
   //只有这个接口拿出来的才是refresh_token，其他地方都是access_token
   tokenStr := u.ExtractToken(ctx)
   var rc jwt2.RefreshClaims
   token, err := jwt.ParseWithClaims(tokenStr, &rc, func(token *jwt.Token) (interface{}, error) {
      return jwt2.RCJWTKey, nil
   })
   if err != nil {
      ctx.AbortWithStatus(http.StatusUnauthorized)
      return
   }
   if token == nil || !token.Valid {
      ctx.AbortWithStatus(http.StatusUnauthorized)
      return
   }
   //换个新的access_token
   err = u.SetJWTToken(ctx, rc.Uid)
   if err != nil {
      ctx.AbortWithStatus(http.StatusUnauthorized)
      return
   }
   ctx.JSON(http.StatusOK, Result{
      Msg: "OK",
   })
}
```

#### jwt进一步提高安全性

但凡你的长 token 泄露了，就没办法了

• 第一种方案：**在长 token 里面编码必要的登录时的信息，比如说 User-Agent 这种。**

• 第二种方案：长 token 只能用一次。也就是每一次刷新的时候，你再给用户一个长 token。这种方案能
在一定程度上缓解泄露的问题，但是不能根治。

#### jwt退出登录实现

##### 基本思路

Session 下退出登录的思路很简单，先把 Cookie 删了，再把对应的 Session 本身删了。例如，当你使用 Redis 来存储 Session 数据时，那么就要连带着将 Redis 里面的数据也清空。

而 JWT 就比较麻烦，因为 JWT 本身是无状态的。如右图，在你退出登录的时候，你没办法把 xx 这个token 直接通过 JWT 置为无效。

**JWT 要退出登录，就只能考虑用一个额外的东西来记录这个 JWT 已经不可用了。我们用一个ssid来记录用户是否已经退出登录，在多实例应用下，使用redis存储**

最终方案如下：

• 用户登录的时候，生成一个标识，放到长短token 里面，这个我们叫做 ssid。

• 用户登录校验的时候，要进一步看看 ssid 是不是已经无效了。

• 用户在调用 refresh_token 的时候，也要进一步看看 ssid 是不是无效了。

• 用户在退出登录的时候，就要把 ssid 存到redis中标记为不可用。

##### 设计与实现

###### 登录时生成ssid

```go
func (h *RedisJWTHandler) SetLoginToken(ctx *gin.Context, uid int64) error {
   //使用ssid标识用户登录状态
   ssid := uuid.New().String()
   err := h.SetJWTToken(ctx, uid, ssid)
   if err != nil {
      return err
   }
   err = h.SetRefreshToken(ctx, uid, ssid)
   return err
}
```

###### 登录校验和refreshToken校验ssid

```go
// CheckSession 从redis中查询标识，判断是否已经退出登录
func (h *RedisJWTHandler) CheckSession(ctx *gin.Context, ssid string) error {
	val, err := h.cmd.Exists(ctx, fmt.Sprintf("users:ssid:%s", ssid)).Result()
	if err != nil && err != redis.Nil{
		return err
	}
	if val > 0 {
		return errors.New("token 无效")
	}
	// err == redis.Nil
	return nil
}
```

###### 退出登录清除token，标记ssid不可用

```go
// ClearToken 清理token，退出登录
func (h *RedisJWTHandler) ClearToken(ctx *gin.Context) error {
   ctx.Header("x-jwt-token", "")
   ctx.Header("x-refresh-token", "")
   claims := ctx.MustGet("claims").(*UserClaims)
   err := h.cmd.Set(ctx, fmt.Sprintf("users:ssid:%s", claims.Ssid), "", h.rcExpiration).Err()
   return err
}
```

##### 能不能用 userId?

也就是说，我们不用 UUID 生成的 ssid，而是用 user id。也就是说过退出登录之后，我们在 Redis 记录一下这个 user id，说明他已经退出登录了。答案是可以，但是要注意两个点：

• 相应地在登录的时候，要删除这个 user id。因为每次登录都重新生成了一个ssid，就不用删除

• 要处理用户一边登录，一边退出登录的并发问题。

##### 使用布隆过滤器

在这里，我们使用的是 Redis 来记录退出登录的会话（ssid）。另外一种可以考虑的面试方案是用布隆过滤器。之所以说是面试方案，因为它的改进非常有限。

• 因为布隆过滤器存在假阳性的问题，所以还是需要 Redis 兜底。

• 布隆过滤器本身也要靠 Redis 实现，所以你至少都要访问一次 Redis。

### 接入配置模块

之前我们一直有一个问题没有解决，就是如何读取配置。因为当下我们都是直接在代码里面写死了，而后通过编
译标签来控制不同环境。

#### viper使用

在main函数中使用并读取配置文件，监听配置文件的变化

```go
func initViper() {
   viper.SetConfigFile("config/config.yaml")
   err := viper.ReadInConfig() // 查找并读取配置文件
   if err != nil {             // 处理读取配置文件的错误
      panic(fmt.Errorf("Fatal error config file: %s \n", err))
   }
   viper.WatchConfig()
   viper.OnConfigChange(func(in fsnotify.Event) {
      err := viper.ReadInConfig() // 查找并读取配置文件
      if err != nil {             // 处理读取配置文件的错误
         panic(fmt.Errorf("Fatal error config file: %s \n", err))
      }
   })
}
```

然后就可以获取到参数了

```go
//使用unmarshal
type Config struct {
   DSN string `yaml:"dsn"`
}
var cfg Config
err := viper.UnmarshalKey("db", &cfg)
if err != nil {
   panic(err)
}
fmt.Println(cfg.DSN)

//直接get
dsn := viper.GetString("db.dsn")
fmt.Println(dsn)
```

#### 远程配置中心

配置文件的缺点就是不够灵活，难以接入复杂的加密解密、权限控制、实时更新等功能。所以就可以考虑使用远程配置中心

##### 安装 etcd

viper 本身提供了远程配置中心的适配。这里我们使用 etcd 来作为我们的远程配置中心。我们利用 Docker Compose 来启动一个 etcd服务。这里我们允许不使用密码，一般在开发环境都设置为不使用密码，方便调试。

```yaml
etcd:
  image: "docker.m.daocloud.io/bitnami/etcd:latest"
  restart: always
  entrypoint:
    - ALLOW_NON_AUTHENTICATION=yes
  ports:
    - 12379:2379
```

##### 安装 etcdctl

有一些人为 etcd 提供了管理界面，比如说 etcdkeeper。我个人用过这些界面，都不是很好用。所以，我们直接使用原生的 etcdctl 命令行工具。如果你本地直接安装了整个 etcd （非 Docker），那么自带
etcdctl，否则就需要另外安装。

##### 使用

```go
// 要先启动etcd容器
func initViperRemote() {
   viper.SetConfigType("yaml")
   err := viper.AddRemoteProvider("etcd3",
      "127.0.0.1:12379", "/webook")
   if err != nil {
      panic(err)
   }
   err = viper.ReadRemoteConfig()
   if err != nil {
      panic(err)
   }
}
```

### 接入日志模块

#### 引入

> 为什么要使用日志，没日志有什么缺点？

• 无法确认系统状态，出现问题了都不知道。

• 在出现问题的时候难以定位。

> 日志级别

比较常见的会有：

• DEBUG：记录了一些辅助排查问题的信息。一般在线上不会打印这个级别的日志。

• INFO：中性地（不好也不坏）描述了发生了什么。线上一般都在这个级别。

• WARN：系统发生了一些不好的事情，但是还处于可以容忍的范畴。至少会打印这个级别的日志。

• ERROR：系统发生了一些需要你保持关注的事情。

还有一些公司会有一些额外的级别：

• DATA：用来记录请求和响应。个人觉得是不好的实践，因为本身可以用 INFO 来 记录。

• FATAL：一般用来表达非常严重的错误，差不多就是你需要立刻手工介入。个人觉得也不是很好的实践，大部分人难以分清什么时候用 ERROR，什么时候用 FATAL。

> 什么时候打日志？打什么级别？

• 统一利用 AOP 机制，记录任何跟第三方交互的请求和响应，包括**数据库、缓存、RPC 调用**，使用`DEBUG` 级别。

• 统一利用 AOP 机制，**记录自己的系统接收的请求，以及自己返回的响应**，使用 `DEBUG` 级别。

• 在开发阶段，使用 `DEBUG` **记录流程的关键中间结果**。

• 怀疑系统**可能有问题，但是又不严重的地方**，记录 `WARN`。只有在频繁出现 WARN 的时候，我才会去看看发生了什么事情。

• 部分情况下，任何**不可能出问题，或者表达有人攻击系统的地方**，记录 `ERROR`

#### zap日志库使用

```bash
go get -u go.uber.org/zap
```

```go
func initLogger() {
   logger, err := zap.NewDevelopment()
   if err != nil {
      panic(err)
   }
   //替换全局包变量
   zap.ReplaceGlobals(logger)
}


//打印字符串和field信息
zap.L().Debug("msg", zap.String("name", "zs"))
```

#### 定义日志接口

随着时代发展，当下看起来很合适的开源仓库可能就变得不再合适了。比如说将来出现一个更加合适的日志仓库，那么你就可能考虑替换已有的 zap。

这里我们自定义一个loggerV1，让每个参数都有名字

```go
type LoggerV1 interface {
   Debug(msg string, args ...Field)
   Info(msg string, args ...Field)
   Warn(msg string, args ...Field)
   Error(msg string, args ...Field)
}
type Field struct {
	Key string
	Val any
}
```

```go
type ZapLogger struct {
   l *zap.Logger
}

func NewZapLogger(l *zap.Logger) *ZapLogger {
   return &ZapLogger{
      l: l,
   }
}

func (z *ZapLogger) Debug(msg string, args ...Field) {
   z.l.Debug(msg, z.toArgs(args)...)
}

func (z *ZapLogger) Info(msg string, args ...Field) {
   z.l.Info(msg, z.toArgs(args)...)
}

func (z *ZapLogger) Warn(msg string, args ...Field) {
   z.l.Warn(msg, z.toArgs(args)...)
}

func (z *ZapLogger) Error(msg string, args ...Field) {
   z.l.Error(msg, z.toArgs(args)...)
}

func (z *ZapLogger) toArgs(args []Field) []zap.Field {
   res := make([]zap.Field, 0, len(args))
   for _, arg := range args {
      res = append(res, zap.Any(arg.Key, arg.Val))
   }
   return res
}
```

#### 使用中间件打印web请求和响应

实现一个log的中间件，先定义一个结构体，表示要打印的信息,主要信息是请求体和响应体

```go
type AccessLog struct {
   Path     string        `json:"path"`
   Method   string        `json:"method"`
   ReqBody  string        `json:"req_body"`
   Status   int           `json:"status"`
   RespBody string        `json:"resp_body"`
   Duration time.Duration `json:"duration"`
}
```

通过AllowReqBody和AllowRespBody来决定是否打印请求体和响应体，并在处理时，控制他们的长度

获取请求体以及其他信息

```go
type LogMiddlewareBuilder struct {
   logFn         func(ctx context.Context, l *AccessLog)
   allowReqBody  bool
   allowRespBody bool
}

func NewLogMiddlewareBuilder(logFn func(ctx context.Context, l *AccessLog)) *LogMiddlewareBuilder {
   return &LogMiddlewareBuilder{
      logFn: logFn,
   }
}

func (l *LogMiddlewareBuilder) AllowReqBody(ok bool) *LogMiddlewareBuilder {
   l.allowReqBody = ok
   return l
}

func (l *LogMiddlewareBuilder) AllowRespBody(ok bool) *LogMiddlewareBuilder {
   l.allowRespBody = ok
   return l
}

func (l *LogMiddlewareBuilder) Build() gin.HandlerFunc {
   return func(ctx *gin.Context) {
      path := ctx.Request.URL.Path
      if len(path) > 1024 {
         path = path[:1024]
      }
      method := ctx.Request.Method
      al := &AccessLog{
         Path:   path,
         Method: method,
      }
      if l.allowReqBody {
         // Request.Body 是一个 Stream 对象，只能读一次
         body, _ := ctx.GetRawData()
         if len(body) > 2048 {
            al.ReqBody = string(body[:2048])
         } else {
            al.ReqBody = string(body)
         }
         // 放回去
         ctx.Request.Body = io.NopCloser(bytes.NewReader(body))
         //ctx.Request.Body = io.NopCloser(bytes.NewBuffer(body))
      }

      start := time.Now()

      if l.allowRespBody {
         ctx.Writer = &responseWriter{
            ResponseWriter: ctx.Writer,
            al:             al,
         }
      }

      defer func() {
         al.Duration = time.Since(start)
         //duration := time.Now().Sub(start)
         l.logFn(ctx, al)
      }()

      // 直接执行下一个 middleware...直到业务逻辑
      ctx.Next()
      // 在这里，你就拿到了响应
   }
}
```

通过装饰器模式，重写Write和WriteHeader方法，将响应体写到AccessLog变量中

```go
type responseWriter struct {
   gin.ResponseWriter
   al *AccessLog
}

func (w *responseWriter) Write(data []byte) (int, error) {
   w.al.RespBody = string(data)
   return w.ResponseWriter.Write(data)
}

func (w *responseWriter) WriteHeader(statusCode int) {
   w.al.Status = statusCode
   w.ResponseWriter.WriteHeader(statusCode)
}
```

#### GORM打印日志

GORM 直接提供了一个日志的相关配置，所以我们可以直接利用。但是用起来很不方便，因为 GORM 这部分是一个过度设计的产物 **gormLoggerFunc 是一个函数衍生类型，而后我们用它实现了一个 glogger.Writer 接口，在 Go 里面这是一个很常见的做法。**

```go
func InitDB(l logger.LoggerV1) *gorm.DB {
    。。。
db, err := gorm.Open(mysql.Open(cfg.DSN), &gorm.Config{
   //日志
    
    
   Logger: glogger.New(gormLoggerFunc(l.Debug), glogger.Config{
      //慢查询阈值，只有执行时间超过这个阈值才会使用
      SlowThreshold: time.Millisecond * 10,
      LogLevel:      glogger.Info,
   }),
})
   。。。 
}
```

```go
type gormLoggerFunc func(msg string, fields ...logger.Field)

func (g gormLoggerFunc) Printf(msg string, args ...interface{}) {
   g(msg, logger.Field{Key: "args", Val: args})
}
```

## 发帖功能

### 需求分析

#### 帖子状态

**创作者**：增删改查文章

**读者**：阅读文章

创作者在创作的时候，不管如何修改，只要没发表，读者就看不到。

**帖子状态：**

![](assets/帖子状态-17264791938131.png)

使用连个库，创作者需修改的是制作库，而读着读到的是线上库，创作者发表帖子后，帖子将被保存到线上库

#### 保存帖子

##### 创作者新建一个帖子

用户在浏览器输入-点击保存-保存到制作库

##### 创作者修改并且保存

查看我的文章-编辑某篇-保存-修改制作库文章

这个流程和上一个流程差不多，最为关键的一步都是保存。因此我们就要考虑：

• 新建一个文章之后保存。create

• 已有的文章修改之后保存。update

是否要共用一个接口？如果共用，怎么区别是新建还是更新？

#### 发表帖子

##### 创作者新建文章并发表

浏览器输入写作-点击发表-create到制作库-create到线上库

##### 创作者更新文章并发表

更新文章-点击发表-update制作库-create到线上库

##### 创作者修改已发表的文章再次发表

更新文章-点击发表-update制作库-update线上库

#### 查询帖子

查询接口包括：

**作者的查询接口**

- 列表接口：在自己的创作中心能够看到自己发表的所有的文章，这是一个分页接口。
- 详情接口：当作者在选中了某篇文章的时候，能看到文章的全部内容。

**读者的查询接口**

- 搜索接口：这个需要我们接入搜索模块之后再设计。
- 推荐接口：当读者打开首页的时候，系统可以针对用户的喜好来推荐一些文章，这个也需要我们将来再设计。
- 阅读文章接口：当选中某篇文章的时候，可以看到文章的全部内容。

### TDD测试驱动开发

在掌握了撰写单元测试和集成测试之后，我们现在从测试开始出发，也就是尝试 TDD。TDD：测试驱动开发。大明简洁版定义：**先写测试、再写实现。**

• 通过撰写测试，理清楚接口该如何定义，体会用户使用起来是否合适。

• 通过撰写测试用例，理清楚整个功能要考虑的主流程、异常流程。

TDD 专注于某个功能的实现。

#### 新建帖子

### 设计与实现

#### 修改或新建文章的保存

前端获取文章后，调用service的save方法：

- 如果传进来的参数中有id，那么就是修改文章
- 否则就是新建文章

```go
func (a *articleService) Save(ctx context.Context, art domain.Article) (int64, error) {
   if art.Id > 0 {
      err := a.author.Update(ctx, art)
      return art.Id, err
   }
   return a.author.Create(ctx, art)
}
```

注意dao层修改文章，只能修改自己的文章，要在更新条件里面带上author_id。判断该文章的作者是不是当前用户，如果更新成功了，说明的确是同一个人，如果更新失败，要么是 ID 不存在，要么不是同一个人。

```go
res := g.db.WithContext(ctx).Model(&art).
   Where("id=? and author_id=?", art.Id, art.AuthorId).
   Updates(map[string]any{
      "title":   art.Title,
      "content": art.Content,
      "utime":   art.Utime,
   })
```

#### 发表文章

##### 职责划分

发表文章不仅要操作制作库还要操作线上库

当你准备提供实现的时候，你就遇到了第一个问题：**究竟哪一个层次负责将数据从制作库同步到线上库？**理论上来说，根据你的项目是单体应用还是微服务应用，你有几个选择：

• web/聚合服务：你在保存了数据之后，要调用另外一个 service 来同步数据。你可以认为，创作者和读者各自有各自的服务。

• service：你会调用不同的 repository 来保存数据。这种设计意味着你认为同样是帖子，但是创作者的帖子和读者看的帖子，从存储的角度来说，很不一样。

• repository：你调用不同的，只是 repository 要负责操作两个单独的数据源

##### 第一种实现：service层分发

使用两个 service 的做法，等微服务架构拆分的时候直接拆出来。所以第一个演示的是使用两个 repoistory。

• authorRepo: 操作制作库

• readerRepo：操作线上库

> 确保制作库和线上库，要么全部成功，要么全部失败。为什么不直接开启事务呢？

- 站在 service 的角度，根本不知道repository 是用什么来存储数据的，更加谈不上开启数据库事务了。
- 你的数据既有可能真的存在两个数据库上，也有可能是在同一个数据库的两个数据表上。service 这一层既不适合开事务，也不一定就能开事务。

```go
// PublishV1 依靠连个不同repository来完成
func (a *articleService) PublishV1(ctx context.Context, art domain.Article) (int64, error) {
   var (
      id  = art.Id
      err error
   )
   if art.Id > 0 {
      err = a.author.Update(ctx, art)
   } else {
      id, err = a.author.Create(ctx, art)
   }
   if err != nil {
      return 0, err
   }
   //因为service层没有事务，所有为了保证两个操作同时成功，只能重试
   //制作库和线上库的ID是相等的
   //重试保存
   art.Id = id
   for i := 0; i < 3; i++ {
      id, err = a.reader.Save(ctx, art)
      if err == nil {
         break
      }
      a.l.Error("保存失败，保存到线上失败",
         logger.Int64("art_id", art.Id),
         logger.Error(err))
   }
   if err != nil {
      a.l.Error("部分失败，重试彻底失败",
         logger.Int64("art_id", art.Id),
         logger.Error(err))
      //接入告警系统，手工处理一下
      //走异步，直接保存到文件
      //走Canal
      //打MQ
   }
   return id, err
}
```

##### 第二种实现：repository 层同步数据

在前面演示的是在 service 层面上操作两个 repository。接着演示另外一种做法：**在 repository里面操作两个 DAO**。为此，先实现了 Publish 方法，它只是调用了简单的 Sync 方法。

这里没有使用事务，但是也可以在repository层使用gorm.db的事务，不过缺陷就是跨层依赖，导致后续如果切换 gorm.DB 到别的实现，repository 也会受到影响。

```go
func (c *articleRepository) SyncV1(ctx context.Context, art domain.Article) (int64, error) {
   var (
      id  = art.Id
      err error
   )
   //操作制作库
   if art.Id > 0 {
      err = c.authorDao.Update(ctx, c.ToEntity(art))
   } else {
      id, err = c.authorDao.Insert(ctx, c.ToEntity(art))
   }
   if err != nil {
      return id, err
   }
   //线上库可能有数据也可能没有数据
   //如果数据库有则更新，没有则插入
   err = c.readerDao.Upsert(ctx, c.ToEntity(art))
   return id, err
}
```

##### 第三种实现：dao层同步数据

如果你在 DAO 层面按成保存-同步线上库的事情，就要简单很多。并且可以使用事务操作，GORM 为了简化我们的事务操作，提供了一个执行事务的闭包。你只需要在闭包里面执行业务逻辑，GORM 本身帮你管理了事务的生命周期。

这里复用了update和Insert的方法操作制作库，通过Upsert方法操作线上库

这里的upsert操作时插入或修改操作，尝试将`art`对象插入到数据库中。如果`art`对象的`id`已经存在于数据库中，那么就不会插入新记录，而是更新数据

```go
func (g *GROMArticleDAO) Sync(ctx context.Context, art Article) (int64, error) {
   //在事务内部，采用了闭包形式
   var (
      id  int64
      err error
   )
   err = g.db.Transaction(func(tx *gorm.DB) error {
      txDAO := NewGROMArticleDAO(tx)
      //根据传入的贴子id是否为0判断是插入还是修改
      if id > 0 {
         err = txDAO.Update(ctx, art)
      } else {
         id, err = txDAO.Insert(ctx, art)
      }
      if err != nil {
         return err
      }
      //因为不知道帖子有没有发表过，所以不能用id判断插入还是修改
      //操作线上库
      err = txDAO.Upsert(ctx, PublishArticle{Article: art})
      return err
   })
   return id, err
}

// Upsert 插入或删除
func (g *GROMArticleDAO) Upsert(ctx context.Context, art PublishArticle) error {
   now := time.Now().UnixMilli()
   art.Ctime = now
   art.Utime = now
   //执行insert xxx on duplicate key update xxx
   return g.db.Clauses(clause.OnConflict{
      Columns: []clause.Column{{Name: "id"}},
      DoUpdates: clause.Assignments(map[string]interface{}{
         "title":   art.Title,
         "content": art.Content,
         "utime":   art.Utime,
      }),
   }).Create(&art).Error
   return nil
}
```

##### 为什么在制作库上不使用 UPSERT

因为要使用where语句

#### 帖子状态设置

##### 状态常量定义

在 domain 层上定义。

```go
type ArticleStatus uint8

const (
   // ArticleStatusUnknown 这是一个未知状态
   ArticleStatusUnknown = iota
   // ArticleStatusUnpublished 未发表
   ArticleStatusUnpublished
   // ArticleStatusPublished 已发表
   ArticleStatusPublished
   // ArticleStatusPrivate 仅自己可见
   ArticleStatusPrivate
)
```

##### 状态流转的点

**edit接口：**，在帖子未发表的情况下保存，帖子状态仍是未发表，**如果修改了已发表的文章，那就要将帖子状态设为未发表，所以在service层直接将帖子状态设为domain.ArticleStatusUnpublished**

**publish接口：**层同时将制作库和线上库的状态修改为发表状态。

**withdraw接口：**将帖子状态设置为仅自己可见

##### withdraw实现

```go
func (a *articleService) Withdraw(ctx context.Context, art domain.Article) error {
   return a.repo.SyncStatus(ctx, art.Id, art.Author.Id, domain.ArticleStatusPrivate)
}
```

```go
func (c *articleRepository) SyncStatus(ctx context.Context, artId int64, authorId int64, status int) error {
   return c.dao.SyncStatus(ctx, artId, authorId, status)
}
```

```go
// SyncStatus 更新帖子状态仅自己可见
func (g *GROMArticleDAO) SyncStatus(ctx context.Context, artId int64, authorId int64, status int) error {
   now := time.Now().UnixMilli()
   return g.db.WithContext(ctx).Transaction(func(tx *gorm.DB) error {
      //更新制作库
      res := tx.Model(&Article{}).
         Where("id=? and author_id=?", artId, authorId).
         Updates(map[string]any{
            "status": status,
            "utime":  now,
         })
      if res.Error != nil {
         //数据库问题
         return res.Error
      }
      if res.RowsAffected != 1 {
         //要么Id错了，要么作者不对
         //用prometheus打点，只要频繁出现，就警告，然后手工介入排查
         return fmt.Errorf("可能有人修改其他人文章，id:%d, authorId:%d", artId, authorId)
      }
      //更新线上库
      return tx.Model(&Article{}).
         Where("id=?", artId).
         Updates(map[string]any{
            "status": status,
            "utime":  now,
         }).Error
   })
}
```

#### MongoDB 入门

##### mongodb简介

MongoDB 基本特性：

• **面向集合存储**：一个集合你可以理解为类似于 MySQL 中的表，集合中存储了很多文档，一个文档类似于 MySQL 中的一行数据。

• **模式自由：**MongoDB 采用无模式结构存储数据，也就是说你不需要预先定义文档必须满足的格式。

• **支持分片：**MongoDB 支持分片，并且MongoDB 自动解决了分片的各种问题，包括自动化扩容。

所以一般我选用 MongoDB 就是为了两个优点：

• **灵活的文档模型**：不同于 MySQL，你不需要预先定义MongoDB 的文档模型，并且可以灵活修改。

• **易于横向扩展**：也就是你可以通过增加 MongoDB 实例来应对流量和数据增长

##### 初始化数据库、创建集合

可以注意到，分成了三个步骤：

• 初始化客户端，注意传入的 option 里面传入了具体的链接信息。

• 获取一个 Database，右图中叫做 webook。

• 在 Database 里面获取一个集合，右图中叫做articles。

```go
//控制初始化超时时间
ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
defer cancel()
monitor := &event.CommandMonitor{
   //每个命令执行之前
   Started: func(ctx context.Context, startedEvent *event.CommandStartedEvent) {
      fmt.Println(startedEvent.Command)
   },
   Succeeded: func(ctx context.Context, succeededEvent *event.CommandSucceededEvent) {

   },
   Failed: func(ctx context.Context, failedEvent *event.CommandFailedEvent) {

   },
}
opts := options.Client().ApplyURI("mongodb://root:example@localhost:27017").
   SetMonitor(monitor)
client, err := mongo.Connect(opts)
assert.NoError(t, err)
//初始化数据库、集合
mdb := client.Database("webook")
col := mdb.Collection("article")

//创建索引
idxRes, err := col.Indexes().CreateMany(ctx, []mongo.IndexModel{
   {
      Keys:    bson.M{"id": 1},
      Options: options.Index().SetUnique(true),
   },
   {
      Keys: bson.M{"author_id": 1},
   },
})
assert.NoError(t, err)
t.Log(idxRes)





```

##### 插入

插入文档的操作非常接近 ORM，就一个地方不同，MongoDB 没有自增主键的说法。

`InsertedID` 是指，MongoDB 为每一个文档都设置的 _id 字段的值，是一个字符串。

```go
//插入数据
insertRes, err := col.InsertOne(ctx, Article{
   Id:       1,
   Title:    "我的标题",
   Content:  "我的内容",
   AuthorId: 123,
})
assert.NoError(t, err)
oid := insertRes.InsertedID
t.Log("插入ID", oid)
```

##### 删除

```go
//删除
filter := bson.M{
   "id": 1,
}
delRes, err := col.DeleteMany(ctx, filter)
assert.NoError(t, err)
t.Log("deleted", delRes.DeletedCount)
```

##### 更新

```go
//更新
filter := bson.M{
   "id": 1,
}
sets := bson.D{bson.E{Key: "$set",
   Value: bson.E{Key: "title", Value: "新的标题"}}}
updateRes, err := col.UpdateMany(ctx, filter, sets)
assert.NoError(t, err)
t.Log("affected", updateRes.ModifiedCount)
```

##### 查询

```go
//查询数据
//filter := bson.D{bson.E{"id", 1}}
filter := bson.M{
   "id": 1,
}
var art Article
err = col.FindOne(ctx, filter).Decode(&art)
if err == mongo.ErrNoDocuments {
   t.Log("没找到数据")
}
assert.NoError(t, err)
t.Log(art)
```

###### or查询

```go
//or查询
or := bson.A{bson.D{bson.E{Key: "id", Value: 1}},
   bson.D{bson.E{Key: "id", Value: 2}}}
res, err := col.Find(ctx, bson.D{bson.E{Key: "$or", Value: or}})
assert.NoError(t, err)
var arts []Article
//读取所有数据
err = res.All(ctx, &arts)
assert.NoError(t, err)
t.Log(arts)
```

###### and查询

```go
//and查询
and := bson.A{bson.D{bson.E{Key: "id", Value: 1}},
   bson.D{bson.E{Key: "author_id", Value: 123}}}
res, err = col.Find(ctx, bson.D{bson.E{Key: "$and", Value: and}})
assert.NoError(t, err)
var arts2 []Article
//读取所有数据
err = res.All(ctx, &arts2)
assert.NoError(t, err)
t.Log(arts2)
```

###### in查询

```go
//in查询
res, err = col.Find(ctx, bson.D{bson.E{Key: "id",
   Value: bson.D{bson.E{Key: "$in", Value: []int{1, 2, 3, 4}}}}})
assert.NoError(t, err)
var arts3 []Article
//读取所有数据
err = res.All(ctx, &arts3)
assert.NoError(t, err)
t.Log(arts3)
```



###### 查询特定字段

```go
//查询特定字段
res, err = col.Find(ctx, bson.D{bson.E{Key: "id",
   Value: bson.D{bson.E{Key: "$in", Value: []int{1, 2, 3, 4}}}}},
   //只查询id
   options.Find().SetProjection(bson.D{{"id", 1}}),
)
assert.NoError(t, err)
var arts4 []Article
//读取所有数据
err = res.All(ctx, &arts4)
assert.NoError(t, err)
t.Log(arts4)
```



#### 使用 MongoDB 数据库

##### 插入

> 主键问题，已有的 DAO 实现里面是直接依赖于 gorm.DB 的自增主键。而现在，MongoDB 没有自增主键。那么，能不能用 MongoDB 的 _id 字段呢？并不能直接使用，**因为 _id 不是一个数字，是一个 12 字节的字节切片。**

你能够考虑的方案有很多：

• 整个 ID 都修改为 string 类型。而后把 GORM 自增的主键转化为 string。
• 使用 ID 生成策略，为 MongoDB 实现生成 int64 的 ID，比如说雪花算法。
• 定义一个新的接口，MongoArticleDAO。这样做就相当于放弃了 DAO 这个一致性的数据存储的抽象。
• 使用 GUID。

这里我们使用雪花算法生成Id

```go
// Insert 制作库保存帖子
func (m *MongoDBArticleDAO) Insert(ctx context.Context, art Article) (int64, error) {
	now := time.Now().UnixMilli()
	art.Ctime = now
	art.Utime = now
	//使用雪花算法生成主键，解决主键问题
	art.Id = m.node.Generate().Int64()
	_, err := m.col.InsertOne(ctx, &art)
	return art.Id, err
}
```

##### 更新

```go
func (m *MongoDBArticleDAO) Update(ctx context.Context, art Article) error {
	now := time.Now().UnixMilli()
	filter := bson.D{bson.E{"id", art.Id},
		bson.E{"author_id", art.AuthorId}}
	set := bson.D{bson.E{"$set", bson.M{
		"title":   art.Title,
		"content": art.Content,
		"status":  art.Status,
		"utime":   now,
	}}}
	res, err := m.col.UpdateOne(ctx, filter, set)
	if err != nil {
		return err
	}
	if res.ModifiedCount == 0 {
		// 创作者不对，说明有人在瞎搞
		return errors.New("ID 不对或者创作者不对")
	}
	return nil
}
```

##### 发布

更新线上库，依旧是upsert语义

```go
func (m *MongoDBArticleDAO) Sync(ctx context.Context, art Article) (int64, error) {
	var (
		id  = art.Id
		err error
	)
	if id > 0 {
		err = m.Update(ctx, art)

	} else {
		id, err = m.Insert(ctx, art)
	}
	if err != nil {
		return 0, err
	}
	//更新线上库
	art.Id = id
	now := time.Now().UnixMilli()
	art.Utime = now
	// liveCol 是 INSERT or Update 语义
	filter := bson.M{"id": art.Id}
	updateV1 := bson.M{
		"$set": PublishArticle(art),
		//如果不存在就插入ctime
		"$setOnInsert": bson.M{"ctime": now},
	}
	_, err = m.liveCol.UpdateOne(ctx, filter, updateV1,
		options.Update().SetUpsert(true))
	return id, err
}
```

#### 使用 OSS 来存储数据

##### OSS入门

有一种比较新奇的做法，是利用 OSS 来存储线上数据，而后直接通过 CDN 加速访问。

也就是说：
• 制作库：关系型数据库或者 NoSQL。
• 线上库： OSS。

**OSS 是指对象存储，Object Storage Service。**早期伴随云计算而兴起的一种东西。目前 OSS 被大量用于存储文件、流媒体、图片等。并且一般 OSS 的服务商会额外提供一些冷数据存储之类的服务，降低存储的成本。

###### **OSS两个核心概念**

• Bucket：所谓的桶，是一种逻辑上的分组关系。比如说某个业务用一个桶，另外一个业务用另外一个桶。
• Object：对象，也就是你存储的东西。

而后，在 Bucket 或者 Object 上，都可以做权限控制（ACL）。

ACL：Access Control List，非常常见的权限控制方案，就好比，有一张表，上面写满了你能否操作某个资源，以及你可以怎么操作这些资源。

###### OSS 与 CDN

OSS 常常和 CDN 结合在一起使用，即在 OSS 上存储的数据，可以通过 OSS 来访问。**CDN，Content Delivery Network，中文全称内****容分发网络。**通过分布在全球各地的服务器集群来缓存和分发原始服务器的内容，从而提高网站或移动应用的访问速度和稳定性。一种很常见的性能和可用性优化手段，就是在公司内部应用 OSS 和 CDN 来存储网站的静态资源。此时，当 OSS 被用作 CDN 的一个回源站点。

###### OSS 与 S3

很多云厂商都有 OSS 服务，或者类似 OSS 的服务（可能不叫 OSS，但是本质上都是一回事）。但这些云厂商的 API 之间都有差异。按道理来说，如果我们使用 OSS，肯定是希望可以在这些云厂商之间自由切换。

好在，现在绝大部分 OSS 服务都兼容了 `S3 API`。S3 API是一个 RESTful 风格的 API，比较优雅的 RESTful API。S3 是指 Amazon 推出的 Simple Storage Service。PS：类比短信服务，短信就是没有这种 API。

##### s3 api案例

首先通过config结构体初始化一个session，然后通过这个session获取一个client

通过PutObjectWithContext存储数据

通过GetObjectWithContext读取数据

```go
func TestS3(t *testing.T) {
	// 腾讯云中对标 s3 和 OSS 的产品叫做 COS
	cosId, ok := os.LookupEnv("COS_APP_ID")
	if !ok {
		panic("没有找到环境变量 COS_APP_ID ")
	}
	cosKey, ok := os.LookupEnv("COS_APP_SECRET")
	if !ok {
		panic("没有找到环境变量 COS_APP_SECRET")
	}
	sess, err := session.NewSession(&aws.Config{
		Credentials: credentials.NewStaticCredentials(cosId, cosKey, ""),
		Region:      ekit.ToPtr[string]("ap-nanjing"),
		Endpoint:    ekit.ToPtr[string]("https://cos.ap-nanjing.myqcloud.com"),
		// 强制使用 /bucket/key 的形态
		S3ForcePathStyle: ekit.ToPtr[bool](true),
	})
	assert.NoError(t, err)
	client := s3.New(sess)
	ctx, cancel := context.WithTimeout(context.Background(), time.Second*10)
	defer cancel()
    //存储数据
	_, err = client.PutObjectWithContext(ctx, &s3.PutObjectInput{
		Bucket:      ekit.ToPtr[string]("webook-1314583317"),
		Key:         ekit.ToPtr[string]("12634"),
		Body:        bytes.NewReader([]byte("测试内容 abc")),
		ContentType: ekit.ToPtr[string]("text/plain;charset=utf-8"),
	})
	assert.NoError(t, err)
    //读取数据
	res, err := client.GetObjectWithContext(ctx, &s3.GetObjectInput{
		Bucket: ekit.ToPtr[string]("webook-1314583317"),
		Key:    ekit.ToPtr[string]("测试文件"),
	})
	assert.NoError(t, err)
	data, err := io.ReadAll(res.Body)
	assert.NoError(t, err)
	t.Log(string(data))
}
```

##### 设计与实现

###### 设计决策

现在，我们的目标是当创作者发布作品的时候，将内容传输到 OSS 上。很显然，最小化改动，就应该在 DAO 层面上，提供一个结合了 S3 API 的实现。这里我就结合 GORM 和 S3 来提供一个实现。

进一步考虑到读者可能希望看到某个创作者的文章列表，**所以在 OSS 里面，我们只存储 Content，也就是内容，其它依旧存储到数据库里面，以参与复杂查询。**

###### sync发表

首先操作制作库与线上库，然后再将content存到OSS中

```go
func (a *ArticleS3DAO) Sync(ctx context.Context, art Article) (int64, error) {
   var id = art.Id
   err := a.db.WithContext(ctx).Transaction(func(tx *gorm.DB) error {
      var (
         err error
      )
      dao := NewGROMArticleDAO(tx)
      if id > 0 {
         err = dao.Update(ctx, art)
      } else {
         id, err = dao.Insert(ctx, art)
      }
      if err != nil {
         return err
      }
      art.Id = id
      now := time.Now().UnixMilli()
      pubArt := PublishedArticleV2{
         Id:       art.Id,
         Title:    art.Title,
         AuthorId: art.AuthorId,
         Ctime:    now,
         Utime:    now,
         Status:   art.Status,
      }
      pubArt.Ctime = now
      pubArt.Utime = now
      err = tx.Clauses(clause.OnConflict{
         Columns: []clause.Column{{Name: "id"}},
         DoUpdates: clause.Assignments(map[string]interface{}{
            "title":  pubArt.Title,
            "utime":  now,
            "status": pubArt.Status,
         }),
      }).Create(&pubArt).Error
      return err
   })
   if err != nil {
      return 0, err
   }
   _, err = a.oss.PutObjectWithContext(ctx, &s3.PutObjectInput{
      Bucket:      ekit.ToPtr[string]("webook-1314583317"),
      Key:         ekit.ToPtr[string](strconv.FormatInt(art.Id, 10)),
      Body:        bytes.NewReader([]byte(art.Content)),
      ContentType: ekit.ToPtr[string]("text/plain;charset=utf-8"),
   })
   return id, err
}
```

###### 设置状态不可见

```go
func (a *ArticleS3DAO) SyncStatus(ctx context.Context, uid int64, id int64, status uint8) error {
   now := time.Now().UnixMilli()
   err := a.db.WithContext(ctx).Transaction(func(tx *gorm.DB) error {
      res := tx.Model(&Article{}).
         Where("id = ? and author_id = ?", uid, id).
         Updates(map[string]any{
            "utime":  now,
            "status": status,
         })
      if res.Error != nil {
         return res.Error
      }
      if res.RowsAffected != 1 {
         return errors.New("ID 不对或者创作者不对")
      }
      return tx.Model(&PublishedArticleV2{}).
         Where("id = ?", uid).
         Updates(map[string]any{
            "utime":  now,
            "status": status,
         }).Error
   })
   if err != nil {
      return err
   }
   const statusPrivate = 3
   //如果帖子状态为不可见，就要将content从OSS中删除
   if status == statusPrivate {
      _, err = a.oss.DeleteObjectWithContext(ctx, &s3.DeleteObjectInput{
         Bucket: ekit.ToPtr[string]("webook-1314583317"),
         Key:    ekit.ToPtr[string](strconv.FormatInt(id, 10)),
      })
   }
   return err
}
```

#### 帖子查询接口与缓存

##### 功能介绍

到目前我们已经开始开发了增删改的接口，但是还差查询接口。查询接口包括：

**作者的查询接口**

- 列表接口：在自己的创作中心能够看到自己发表的所有的文章，这是一个分页接口。

- 详情接口：当作者在选中了某篇文章的时候，能看到文章的全部内容。

**读者的查询接口**

- 搜索接口：这个需要我们接入搜索模块之后再设计。
- 推荐接口：当读者打开首页的时候，系统可以针对用户的喜好来推荐一些文章，这个也需要我们将来再设计。
- 阅读文章接口：当选中某篇文章的时候，可以看到文章的全部内容。
- 目前我们先实现创作者的列表接口和详情接口，以及读者的阅读文章接口。

##### 作者查询自己的文章列表

###### 缓存方案

大部分情况下，没什么人会缓存分页的结果，因为如果数据的筛选条件、排序条件、分页的偏移量和数据量中的任何一个发生了变化，缓存就很难使用了。

但是有一些缓存方案是可以考虑使用的。也就是只缓存第一页。在大多数的使用场景中，用户都是只看列表页的第一页，很少会往后翻页。所以，**缓存住第一页之后就可以了。**

###### repo层实现

只缓存第一页，这里只有在偏移量是 0 并且每一页大小是 100 的时候才会使用缓存。

**注意不需要把整个 Content 缓存下来，因为在列表页的时候只展示了摘要。**

```go
func (c *articleRepository) GetByAuthor(ctx context.Context, uid int64, offset int, limit int) ([]domain.Article, error) {
	// 首先第一步，判定要不要查询缓存
	// 事实上， limit <= 100 都可以查询缓存
	if offset == 0 && limit == 100 {
		//if offset == 0 && limit <= 100 {
		res, err := c.cache.GetFirstPage(ctx, uid)
		if err == nil {
			return res, err
		} else {
			// 要考虑记录日志
			// 缓存未命中，你是可以忽略的
		}
	}
	//查询数据库
	arts, err := c.dao.GetByAuthor(ctx, uid, offset, limit)
	if err != nil {
		return nil, err
	}
	res := slice.Map[articleDao.Article, domain.Article](arts, func(idx int, src articleDao.Article) domain.Article {
		return c.toDomain(src)
	})

	//回写缓存
	go func() {
		ctx, cancel := context.WithTimeout(context.Background(), time.Second)
		defer cancel()
		if offset == 0 && limit == 100 {
			// 缓存回写失败，不一定是大问题，但有可能是大问题
			err = c.cache.SetFirstPage(ctx, uid, res)
			if err != nil {
				// 记录日志
				// 我需要监控这里
			}
		}
	}()
	//缓存预加载，缓存第一条数据
	go func() {
		ctx, cancel := context.WithTimeout(context.Background(), time.Second)
		defer cancel()
		c.preCache(ctx, res)
	}()
	return res, nil
}
```

###### 更新/新增操作清理缓存

为了保证缓存与数据库的一致性，写入操作的时候，要记得清除相关的缓存。也就是**在新写文章、编辑文章、发表的时候都要清除相关的缓存。**

```go
//清空缓存
	err := c.cache.DelFirstPage(ctx, art.Author.Id)
	if err != nil {
		c.l.Error("删除缓存失败", logger.Int64("authorId", art.Author.Id))
		return err
	}
```

##### 作者查看某个文章详情

###### 缓存方案

从实践上来说，针对创作者设计的缓存性价比其实都很低，因为只有这个创作者用得上。不

过这个场景适合用来演示一种新奇的缓存方案：**业务相关的缓存预加载**。简单来说，就是认为在**拿到列表之后，创作者有更大的概率会访问第一条数据。因此我们查询所有列表后，预缓存第一条数据**

```go
//缓存预加载，缓存第一条数据
go func() {
   ctx, cancel := context.WithTimeout(context.Background(), time.Second)
   defer cancel()
   c.preCache(ctx, res)
}()
```

```go
func (c *articleRepository) preCache(ctx context.Context, arts []domain.Article) {
   if len(arts) > 0 {
      err := c.cache.Set(ctx, arts[0])
      if err != nil {
         c.l.Error("提前预加载失败", logger.Error(err))
      }
   }
}
```

###### repo层实现

```go
func (c *articleRepository) GetById(ctx context.Context, id int64) (domain.Article, error) {
   //查询缓存
   res, err := c.cache.Get(ctx, id)
   if err == nil {
      return res, nil
   }
   //查询数据库
   art, err := c.dao.GetById(ctx, id)
   if err != nil {
      return domain.Article{}, err
   }
   res = c.toDomain(art)
   //异步回写缓存
   go func() {
      er := c.cache.Set(ctx, res)
      if er != nil {
         // 记录日志
      }
   }()
   return res, nil
}
```

##### 读者查看某个文章详情

###### 缓存方案

这个接口有一个奇妙的缓存方案：**在 Publish 接口调用的时候，直接设置好缓存。**

因为我们可以预期，当一个新帖子发布之后，立刻就会被人访问，所以我们可以考虑直接在发布的过程中，就把缓存弄好。

在sync中

```go
err = c.cache.SetPub(ctx, art)
if err != nil {
   c.l.Error("缓存线上库数据失败", logger.Int64("authorId", art.Author.Id))
   return 0, err
}
```

```go
func (a *ArticleRedisCache) SetPub(ctx context.Context, art domain.Article) error {
   val, err := json.Marshal(art)
   if err != nil {
      return err
   }
   return a.client.Set(ctx, a.pubKey(art.Id), val, time.Minute*10).Err()
}
```

###### repo层实现

这里我们为articleRepository封装了userDao，通过userDao查询作者的姓名。返回到前端。

```go
func (c *articleRepository) GetPubById(ctx context.Context, id int64) (domain.Article, error) {
   //读取缓存
   res, err := c.cache.GetPub(ctx, id)
   if err == nil {
      return res, err
   }
   //读取线上库数据，如果Content放到了OSS中，就要让前端去读取Content
   art, err := c.dao.GetPubById(ctx, id)
   if err != nil {
      return domain.Article{}, err
   }
   // 我现在要去查询对应的 User 信息，拿到创作者信息
   res = c.toDomain(articleDao.Article(art))
   author, err := c.userDao.FindById(ctx, art.AuthorId)
   if err != nil {
      c.l.Error("获取用户信息失败", logger.Error(err))
      return domain.Article{}, err
   }
   res.Author.Name = author.Nickname
   //回写缓存
   go func() {
      ctx, cancel := context.WithTimeout(context.Background(), time.Second)
      defer cancel()
      er := c.cache.SetPub(ctx, res)
      if er != nil {
         // 记录日志
      }
   }()
   return res, nil
}
```

## 阅读点赞收藏

### 需求分析

阅读、点赞和收藏是内容生产平台里面非常常见的功能。但是从形态上来说，这三者之间也有点区别。

• 阅读数是每一次打开就加 1，你关掉也不会 -1。

• 点赞和收藏都有取消功能。也就是说，当你点赞之后，你可以取消点赞；你收藏之后，也可以移除收藏夹。

• 收藏还会有一个收藏夹的功能，也就是方便你分门别类。

### 设计与实现

#### 新建服务

创建一个新的服务interactiveService用于实现这三个功能，在articleHandle中集成这个service并使用

#### 阅读计数

##### repo层

在调用 pubDetail 浏览文章详情的时候，调用IncrReadCnt增加一下该帖子阅读数量。

使用redis缓存次数

```go
func (c *CachedInteractiveRepository) IncrReadCnt(ctx context.Context, biz string, bizId int64) error {
   //优先存数据库
   err := c.dao.IncrReadCnt(ctx, biz, bizId)
   if err != nil {
      return err
   }
   // 你要更新缓存了
   // 部分失败问题 —— 数据不一致
   return c.cache.IncrReadCntIfPresent(ctx, biz, bizId)
}
```

##### 数据库操作

注意是upsert操作，可能要创建记录

表设计如下：使用biz:bizId 来唯一标识某个特定业务。使用readCnt、likeCnt、collectCnt记录阅读、点赞、收藏数量

```go
type Interactive struct {
   Id int64 `gorm:"primaryKey,autoIncrement"`
   //业务标识符，创建联合索引
   // <bizid, biz>
   BizId int64 `gorm:"uniqueIndex:biz_type_id"`
   // WHERE biz = ?
   //默认是BLOB/TEXT类型
   Biz string `gorm:"type:varchar(128);uniqueIndex:biz_type_id"`

   ReadCnt    int64
   LikeCnt    int64
   CollectCnt int64
   Utime      int64
   Ctime      int64
}
```

```go
// IncrReadCnt upsert语义
func (dao *GORMInteractiveDAO) IncrReadCnt(ctx context.Context, biz string, bizId int64) error {
   //直接update read_cnt = read_cnt+1
   //不能先查询read_cnt，再update。有并发问题
   now := time.Now().UnixMilli()
   return dao.db.WithContext(ctx).Clauses(clause.OnConflict{
      DoUpdates: clause.Assignments(map[string]interface{}{
         "read_cnt": gorm.Expr("`read_cnt` + 1"),
         "utime":    now,
      }),
   }).Create(&Interactive{
      Biz:     biz,
      BizId:   bizId,
      ReadCnt: 1,
      Ctime:   now,
      Utime:   now,
   }).Error
}
```

##### 缓存实现

**因为是典型的查询更新操作，先查询是否存在，再更新。所以使用lua脚本，解决并发问题**

redis使用hash，存readCnt、lickCnt、collectCnt

```go
func (i *InteractiveRedisCache) IncrReadCntIfPresent(ctx context.Context,
   biz string, bizId int64) error {
   key := i.key(biz, bizId)
   // 不是特别需要处理 res
   //res, err := i.client.Eval(ctx, luaIncrCnt, []string{key}, fieldReadCnt, 1).Int()
   return i.client.Eval(ctx, luaIncrCnt, []string{key}, fieldReadCnt, 1).Err()
}
```

#### 点赞设计

##### 功能分析

点赞事实上分成两个部分：

• 保留某个用户是否点赞过，并且用户可以取消点赞。

• 统计点赞数量。同阅读计数

##### web和service

web前端传入**文章id和一个bool变量**表示点赞后取消点赞，分别调用service层的两个不同方法**Like和CancelLike**

```go
func (i *interactiveService) Like(c context.Context, biz string, id int64, uid int64) error {
   return i.repo.IncrLike(c, biz, id, uid)
}

func (i *interactiveService) CancelLike(c context.Context, biz string, id int64, uid int64) error {
   return i.repo.DecrLike(c, biz, id, uid)
}
```

##### repo层

首先在数据库记录点赞记录（用户为哪个业务的对象点赞）或者删除点赞记录，然后将点赞数量加1或减1

```go
func (c *CachedInteractiveRepository) IncrLike(ctx context.Context, biz string, id int64, uid int64) error {
   //记录点赞记录，并在数据库中贴子点赞数量加1
   err := c.dao.InsertLikeInfo(ctx, biz, id, uid)
   if err != nil {
      return err
   }
   //点赞记录不用缓存
   return c.cache.IncrLikeCntIfPresent(ctx, biz, id)
}

func (c *CachedInteractiveRepository) DecrLike(ctx context.Context, biz string, id int64, uid int64) error {
   err := c.dao.DeleteLikeInfo(ctx, biz, id, uid)
   if err != nil {
      return err
   }
   return c.cache.DecrLikeCntIfPresent(ctx, biz, id)
}
```

##### 数据库操作

插入点赞记录和更新点赞数量实现如下，注意点赞记录删除采用软删除，将status更新为0表示为删除

```go
// InsertLikeInfo 同时记录点赞记录和更新点赞数
func (dao *GORMInteractiveDAO) InsertLikeInfo(ctx context.Context,
   biz string, id int64, uid int64) error {
   now := time.Now().UnixMilli()
   return dao.db.WithContext(ctx).Transaction(func(tx *gorm.DB) error {
      //记录点赞记录
      //因为可能数据库中已经存在了该数据，但是被软删除，所以是upsert操作
      err := tx.Clauses(clause.OnConflict{
         DoUpdates: clause.Assignments(map[string]interface{}{
            "utime":  now,
            "status": 1,
         }),
      }).Create(&UserLikeBiz{
         Uid:    uid,
         Biz:    biz,
         BizId:  id,
         Status: 1,
         Utime:  now,
         Ctime:  now,
      }).Error
      if err != nil {
         return err
      }
      //更新点赞数，upsert
      return tx.WithContext(ctx).Clauses(clause.OnConflict{
         DoUpdates: clause.Assignments(map[string]interface{}{
            "like_cnt": gorm.Expr("`like_cnt` + 1"),
            "utime":    now,
         }),
      }).Create(&Interactive{
         Biz:     biz,
         BizId:   id,
         LikeCnt: 1,
         Ctime:   now,
         Utime:   now,
      }).Error
   })
}
```

##### 缓存实现

同阅读计数将点赞数量缓存即可

#### 收藏功能

##### web和service

collect收藏接口接收了一个收藏对象id和一个收藏夹的cid，cid默认为0表示默认收藏夹。调用Collect方法添加收藏记录和更新收藏数量

##### repo层

添加用户的收藏记录到usercollection表中，并更新该文章的收藏数量。然后缓存收藏数量

```go
func (c *CachedInteractiveRepository) AddCollectionItem(ctx context.Context,
   biz string, id int64, cid int64, uid int64) error {
    //添加收藏夹
   err := c.dao.InsertCollectionBiz(ctx, dao.UserCollectionBiz{
      Biz:   biz,
      BizId: id,
      Cid:   cid,
      Uid:   uid,
   })
   if err != nil {
      return err
   }
   //用户收藏夹不用缓存
   return c.cache.IncrCollectCntIfPresent(ctx, biz, id)
}
```

##### 数据库操作

更新收藏数量时同样是upsert操作

```go
func (dao *GORMInteractiveDAO) InsertCollectionBiz(ctx context.Context,
   cb UserCollectionBiz) error {
   now := time.Now().UnixMilli()
   cb.Ctime = now
   cb.Utime = now
   return dao.db.WithContext(ctx).Transaction(func(tx *gorm.DB) error {
      //记录用户收藏到指定的收藏夹
      err := tx.Create(&cb).Error
      if err != nil {
         return err
      }
      //收藏数量加1
      return tx.WithContext(ctx).Clauses(clause.OnConflict{
         DoUpdates: clause.Assignments(map[string]interface{}{
            "collect_cnt": gorm.Expr("`collect_cnt` + 1"),
            "utime":       now,
         }),
      }).Create(&Interactive{
         Biz:        cb.Biz,
         BizId:      cb.BizId,
         CollectCnt: 1,
         Ctime:      now,
         Utime:      now,
      }).Error
   })
}
```

#### 查询接口

##### 需求分析

我们前面实现了阅读点赞收藏的计数，前端需要在展示文章内容的时候，同步展示这篇文章

有多少人阅读、点赞和收藏，那么就需要考虑在原本的查询接口里面返回相应的内容。

##### web和service



在web层查询帖子详情中，不仅要查询帖子数据，还要去查询获取帖子的阅读点赞收藏数量

servi

ce层使用了 errgroup 来并发查询用户是否收藏点赞了这篇文章，以便在前端显示

```go
//获取这篇文章的所有计数
uc := ctx.MustGet("claims").(*jwt2.UserClaims)
intr, err := h.intrSvc.Get(ctx, h.biz, id, uc.Uid)
if err != nil {
   //这个地方可以容忍错误
   /*ctx.JSON(http.StatusOK, Result{
      Code: 5,
      Msg:  "系统错误",
   })*/
   h.l.Error("获取文章信息失败", logger.Error(err))
   //return
}
```

```go
// Get 获取文章统计数据
func (i *interactiveService) Get(ctx context.Context, biz string, id int64, uid int64) (domain.Interactive, error) {
   intr, err := i.repo.Get(ctx, biz, id)
   if err != nil {
      return domain.Interactive{}, err
   }
   //查看我是否点赞或收藏这篇文章
   //使用官方封装好的group处理并发产生的err
   var eg errgroup.Group
   eg.Go(func() error {
      var er error
      intr.Liked, er = i.repo.Liked(ctx, biz, id, uid)
      return er
   })

   eg.Go(func() error {
      var er error
      intr.Collected, er = i.repo.Collected(ctx, biz, id, uid)
      return er
   })
   return intr, eg.Wait()
}
```

##### repo层

同样使用了缓存

```go
func (c *CachedInteractiveRepository) Get(ctx context.Context, biz string, id int64) (domain.Interactive, error) {
   //先查询缓存
   intr, err := c.cache.Get(ctx, biz, id)
   if err == nil {
      return intr, nil
   }
   //查询数据库
   ie, err := c.dao.Get(ctx, biz, id)
   if err != nil {
      return domain.Interactive{}, err
   }
   //回写缓存
   if err == nil {
      res := c.toDomain(ie)
      err = c.cache.Set(ctx, biz, id, res)
      if err != nil {
         c.l.Error("回写缓存失败",
            logger.String("biz", biz),
            logger.Int64("bizId", id),
            logger.Error(err))
      }
      return res, nil
   }
   return intr, err
}
```

##### 缓存一致性问题

在查询接口的实现中，我缓存了阅读、点赞和收藏的总数，但是没有缓存个人是否点赞、收藏的数据。

• 缓存中有数据：可以想到，所有的 goroutine 在执行lua 脚本的时候，因为缓存中有数据，所以都能正确+1。基本上不存在什么缓存一致性的问题。

• 缓存中没有数据：这就是问题的关键，当缓存中没有数据的时候，查询接口会去加载数据并且**尝试回写缓存，并且这个时候同样可能会更新数据库。那么就可能数据不一致了**

<img src="assets/image-20240921170304308.png" alt="image-20240921170304308" style="zoom:33%;" />

**如何解决？**

答案是：不需要解决。

从业务上来说，没有必要要求阅读数、点赞数或者收藏数量一定是严格准确的，因为即便不准确，也不会对用户产生什么不好的影响。更进一步说，只有高并发的文章才会有并发问题。而高并发的文章，阅读数、点赞数或者收藏数本身就很多，你少一点点无所谓。而低并发的文章，极小概率会遇上这种不一致的场景。

### kafka改造

#### kafka入门

##### 消息队列

**Kafka 是消息队列的一种实现**，类似的还有**RocketMQ** 和 **RabbitMQ**。Kafka 不论是在实践中还是在面试中，都是热点。

消息队列的基本形态，就是有 N 个生产者， N 个消费者。在这种形态下，**生产者和消费者就解耦**了。

##### 基本概念

**producer**：生产者，生产者发送消息到指定的topic下，消息再根据分配规则append到某个分区的末尾。

**consumer**：消费者，消费者从topic中消费数据。

**broker**：kafka 集群包含一个或多个服务器，每个服务器节点称为一个broker。

**topic**：每条发布到kafka集群的消息都有一个类别，这个类别称为topic，其实就是将消息按照topic来分类，topic就是逻辑上的分类，同一个topic的数据既可以在同一个broker上也可以在不同的broker结点上。

**partition**：分区，每个topic被物理划分为一个或多个分区，每个分区在物理上对应一个文件夹，该文件夹里面存储了这个分区的所有消息和索引文件。

**offset**：partition中的每条消息都被标记了一个序号，这个序号表示消息在partition中的偏移量，称offset，每一条消息在partition都有唯一的offset，消息者通过指定offset来指定要消费的消息

**消费者组**：每个消费者都属于一个特定的消费者组

<img src="assets/image-20240923164600309.png" alt="image-20240923164600309" style="zoom: 33%;" />

##### 分区策略

分区策略就是决定生产者将消息发送到哪个分区的算法。Kafka 为我们提供了默认的分区策略，同时它也支持自定义分区策略。kafka允许为每条消息设置一个key，一旦消息被定义了 Key，那么就可以保证同一个 Key 的所有消息都进入到相同的分区。

除了根据key选择分区，还有

• 轮询：一个分区发送一次，挨个轮流。

• 随机：随机挑选一个。

##### 分区消息有序性

同一分区中的数据是有序的，但topic下的多个分区之间在消费数据时不能保证有序性，

**因此，如果要做到全局有序，topic就只能有一个分区。**

<img src="assets/image-20240923165130091.png" alt="image-20240923165130091" style="zoom: 25%;" />

##### 消费者组

topic的一条消息只能被消费者组内的一个consumer消费，但其他消费者组的消费这可同时消费这一分区消息。**也就是一个分区不能给同一个组内的多个消费者使用**

这也是kafka用来实现一个topic消息的广播和单播的手段，**如果需要实现广播，一个consumer group内只放一个消费者即可，要实现单播，将所有的消费者放到同一个consumer group即可。**

#### kafka api使用

##### Docker 启动 Kafka

正常来说，可以直接使用 Docker 来启动 Kafka。Kafka 早期的版本需要一个额外的 ZooKeeper，但是高版本已经不需要了。

```yaml
kafka:
  image: 'm.daocloud.io/docker.io/bitnami/kafka:latest'
  ports:
    - '9092:9092'
    - '9094:9094'
  environment:
    - KAFKA_CFG_NODE_ID=0
    #      - 允许自动创建 topic，线上不要开启
    - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
    - KAFKA_CFG_PROCESS_ROLES=controller,broker
    - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092,CONTROLLER://:9093,EXTERNAL://0.0.0.0:9094
    - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
    - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
    - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
    - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
```

##### Kafka 的 Shell 工具

正常为了调试 Kafka，或者说为了在写代码的时候验证消息有没有发出去、topic 有没有建立起来，都是可以考虑使用 Kakfa 的 Shell 工具。

在启动了 Docker 之后，运行命令，其中ID 替换为 Docker Compose 启动之后对应的 Kafka 的 ID。 

```bash
dockerexec -it $id /bin/sh。
docker exec -it c8d536f693cc3d6df8ec71ec6c6807504599360347917e54ec72d477f05eccf8 /bin/sh
```


而后切换目录到 /opt/bitnami/kafka/bin 下，就可以了。

**Kafka Shell 工具常用命令**

查看kafka shell文档即可

##### Sarama 安装

在 Go 里面有三个比较有名气的 **Go 客户端。**

• **Sarama**：用户数量最多，但是有一些 BUG（个人认为是因为用户多 BUG 才暴露得比较清晰，我没怎么遇到过），早期这个项目是在 Shopify 下面，现在挪到了 IBM 下。

• **segmentio/kafka-go**：没啥大的缺点。

• **confluent-kafka-go**：需要启用 cgo，跨平台问题比较多，交叉编译也不支持。这里我们选用 Sarama，因为用户多，你出去工作接触到它的概率最大

###### Sarama shell命令工具

在 Sarama 里面提供了一些简单的命令行工具，可以看做是 Shell 脚本提供的功能一个子集。Consumer 和 producer 中的用得比较多。

安装使用sarama sell工具命令：

```bash
go install github.com/IBM/sarama/tools/kafka-console-consumer@latest
go install github.com/IBM/sarama/tools/kafka-console-producer@latest
```

启动一个消费者

```bash
kafka-console-consumer -topic=test_topic -brokers=localhost:9094
```



##### 生产者发送消息

导包

```go
"github.com/IBM/sarama"
```

发送一个消息很简单，只需要初始化好producer，然后直接发送消息就可以。可以使用 Sarama 的 kafka-console-consumer启动一个消费者消费来查看是否发送成功。

###### 简单使用

```go
func TestSyncProducer(t *testing.T) {
   cfg := sarama.NewConfig()
   cfg.Producer.Return.Successes = true
   //指定分区策略，默认是hash
   cfg.Producer.Partitioner = sarama.NewHashPartitioner
   producer, err := sarama.NewSyncProducer(addr, cfg)
   assert.NoError(t, err)
   for i := 0; i < 100; i++ {
      _, _, err = producer.SendMessage(&sarama.ProducerMessage{
         Topic: "test_topic",
         //消息数据本体
         Value: sarama.StringEncoder("这是一条消息"),
         //会在生产者和消费者之间传递的
         Headers: []sarama.RecordHeader{
            {
               Key:   []byte("key1"),
               Value: []byte("value1"),
            },
         },
         //只作用于发送过程
         Metadata: "这是 metadata",
      })
   }
}
```

###### **指定分区**

在 Sarama 里面，可以通过指定 config 中的 Partitioner 来指定最终的目标分区。下列出了常见的：
• Random：随机挑一个。
• RoundRobin：轮询。
• Hash：根据 key 的哈希值来筛选一个。
• Manual：根据 Message 中的 partition 字段来选择。
• ConsistentCRC：一致性哈希，用的是 CRC16 算法。
• Custom：实际上不 Custom，而是自定义一部分Hash 的参数，本质上是一个 Hash 的实现。

```go
cfg.Producer.Partitioner = sarama.NewHashPartitioner
```

###### **异步发送**

Sarama 有一个异步发送的 producer，它的用法稍微复杂一点。

• 把 Return.Success 和 Errors 都设置为true，这是为了后面能够拿到发送结果。

• 初始化异步 producer。

• 从 producer 里面拿到 Input 的 channel，并且发送一条消息。

• 利用 select case，同时监听 Success 和 Error 两个channel，来获得发送成功与否的信息。

```go
func TestAsyncProducer(t *testing.T) {
   cfg := sarama.NewConfig()
   cfg.Producer.Return.Successes = true
   cfg.Producer.Return.Errors = true
   producer, err := sarama.NewAsyncProducer(addr, cfg)
   assert.NoError(t, err)
   msgs := producer.Input()
   msgs <- &sarama.ProducerMessage{
      Topic: "test_topic",
      Value: sarama.StringEncoder("这是一条消息"),
      // 会在生产者和消费者之间传递的
      Headers: []sarama.RecordHeader{
         {
            Key:   []byte("key1"),
            Value: []byte("value1"),
         },
      },
      Metadata: "这是 metadata",
   }

   select {
   case msg := <-producer.Successes():
      t.Log("发送成功", string(msg.Value.(sarama.StringEncoder)))
   case err := <-producer.Errors():
      t.Log("发送失败", err.Err, err.Msg)
   }
}
```

###### 指定 acks

在 Kafka 里面，生产者在发送数据的时候，有一个很关键的参数，就是 acks。有三个取值：

• 0：客户端发一次，**不需要服务端的确认。**

• 1：客户端发送，并且**需要服务端写入到主分区。**

• -1：客户端发送，并且**需要服务端同步到所有的ISR(从分区)上。**

从上到下，性能变差，但是数据可靠性上升。需要性能，选 0，需要消息不丢失，选 -1。

##### 消费者获取消息

###### 初步实现

Sarama 的消费者设计不是很直观，稍微有点复杂。

• 首先要初始化一个 ConsumerGroup。
• 调用 ConsumerGroup 上的 Consume 方法。
• 为 Consume 方法传入一个 ConsumerGroupHandler的辅助方法。

必须要自己实现一个consumeHandle

```go
func TestConsumer(t *testing.T) {
   cfg := sarama.NewConfig()
   cg, err := sarama.NewConsumerGroup(addr, "demo", cfg)
   assert.NoError(t, err)
   ctx, cancel := context.WithTimeout(context.Background(), time.Minute*10)
   defer cancel()
   start := time.Now()
   //阻塞调用
   err = cg.Consume(ctx, []string{"test_topic"}, ConsumerHandler{})
   assert.NoError(t, err)
   t.Log(time.Since(start).String())
}
```

```go
type ConsumerHandler struct {
}

func (c ConsumerHandler) Setup(session sarama.ConsumerGroupSession) error {
   log.Println("这是 Setup")
   //partitions := session.Claims()["test_topic"]
   //for _, part := range partitions {
   // session.ResetOffset("test_topic",
   //    part, sarama.OffsetOldest, "")
   //}
   return nil
}

func (c ConsumerHandler) Cleanup(session sarama.ConsumerGroupSession) error {
   log.Println("这是 Cleanup")
   return nil
}

func (c ConsumerHandler) ConsumeClaim(session sarama.ConsumerGroupSession,
	claim sarama.ConsumerGroupClaim) error {
	msgs := claim.Messages()
	for msg := range msgs {
		log.Println(string(msg.Value))
		session.MarkMessage(msg, "")
	}
	return nil
}
```

###### 指定偏移量消费

在部分场景下，我们会希望消费历史消息，或者从某个消息开始消费，那么可以考虑在 Setup 里面设置偏移量。关键调用是 ResetOffset 。不过一般我都是建议走离线渠道，操作 Kafka 集群去重置对应的偏移量。核

心在于，你并不是每次重新部署，重新启动都是要重置这个偏移量的。

```go
func (c ConsumerHandler) Setup(session sarama.ConsumerGroupSession) error {
   log.Println("这是 Setup")
   partitions := session.Claims()["test_topic"]
   for _, part := range partitions {
      session.ResetOffset("test_topic",
         part, sarama.OffsetOldest, "")
   }
   return nil
}
```

###### 异步消费，批量提交

**通过gorouinte进行消息的处理，实现异步消费**

正常来说，为了在异步消费失败之后还能继续重试，可以考虑**异步消费一批，提交一批。**

ctx.Done 分支用来控制凑够一批的超时机制，防止生产者的速率很低，一直凑不够一批。另外一个分支就是读取消息，并且提交到 errgroup 里面执行。

Sleep 是模拟长时间业务执行。

```go
func (c ConsumerHandler) ConsumeClaim(
   //代表的是和kafka的会话（从建立连接到连接彻底断掉的那一段时间）
   session sarama.ConsumerGroupSession,
   claim sarama.ConsumerGroupClaim) error {

   msgCh := claim.Messages()
   const batchSize = 10
   for {
      log.Println("一个批次开始")
      batch := make([]*sarama.ConsumerMessage, 0, batchSize)
      //凑够一批的时间不能超过1s
      ctx, cancel := context.WithTimeout(context.Background(), time.Second)
      var done = false
      var eg errgroup.Group
      for i := 0; i < batchSize && !done; i++ {
         select {
         case <-ctx.Done():
            // 超时了
            done = true
         case msg, ok := <-msgCh:
            //消费者被关闭了
            if !ok {
               cancel()
               return nil
            }
            batch = append(batch, msg)
            eg.Go(func() error {
               // 并发处理
               log.Println(string(msg.Value))
               return nil
            })
         }
      }
      cancel()
      err := eg.Wait()
      if err != nil {
         log.Println(err)
         continue
      }
      // 凑够了一批，然后你就处理
      // log.Println(batch)

      for _, msg := range batch {
         session.MarkMessage(msg, "")
      }
   }
}
```

#### 代码引入kafka

###### 原本代码的问题与解决

我们的实现都是严重耦合的。尤其是在统计阅读数的时候，是直接在 HTTP 的 GET请求里面调用了一个IncrReadCnt 方法。这会导致严重的性能问题，会给数据库带来庞大的压力。因为你可以想到，读文章这种是基本操作！

原本的接口里面，全部都是数据库的读操作，或者直接命中缓存。但是在这里就是额外增加了一个数据库的写操作。

**引入 Kafka 来解耦**

在统计阅读计数这里，我们引入 Kafka 来解耦合。当用户读取了某篇文章的时候，我们就直接发送一个消息，标记某个用户读了某篇文章。

这一步，我们在 **ArticleService 里面发出去一个消息，代表我这个用户读了我这篇文章。**

而后不同的感兴趣的业务方去订阅这个消息。

###### 领域事件定义

领域事件（Domain Event）。也就是在这个场景之下，我们需要一个读了文章的事件。也可以说是生产者发送的消息

```go
// ReadEvent 某个用户读了某篇文章
type ReadEvent struct {
   Uid int64
   Aid int64
}
```

###### 消息发送

消息发送到读文章的topic，read_article

发送了用户id和文章id，表示用户已读了这篇文章

```go
type Producer interface {
   ProduceReadEvent(ctx context.Context, evt ReadEvent) error
}

type KafkaProducer struct {
   producer sarama.SyncProducer
}

func NewKafkaProducer(producer sarama.SyncProducer) Producer {
   return &KafkaProducer{producer: producer}
}

// ProduceReadEvent 发送用户已读文章消息
func (k *KafkaProducer) ProduceReadEvent(ctx context.Context, evt ReadEvent) error {
   data, err := json.Marshal(evt)
   if err != nil {
      return err
   }
   _, _, err = k.producer.SendMessage(&sarama.ProducerMessage{
      Topic: "article_read",
      Value: sarama.ByteEncoder(data),
   })
   return err
}
```

###### 消费者处理消息

创建了一个名为interactive的消费者组

从article_read的topic的默认分区中读取数据，Handler中传入了自定义的函数Consume用于处理消息，即调用了repo的IncrReadCnt方法

```go
type InteractiveReadEventConsumer struct {
   repo   repository.InteractiveRepository
   client sarama.Client
   l      logger.LoggerV1
}

func NewInteractiveReadEventConsumer(repo repository.InteractiveRepository,
   client sarama.Client, l logger.LoggerV1) *InteractiveReadEventConsumer {
   return &InteractiveReadEventConsumer{repo: repo, client: client, l: l}
}

func (i *InteractiveReadEventConsumer) Start() error {
   cg, err := sarama.NewConsumerGroupFromClient("interactive", i.client)
   if err != nil {
      return err
   }
   go func() {
      er := cg.Consume(context.Background(), []string{"article_read"},
         samarax.NewBatchHandler[ReadEvent](i.l, i.Consume))
      if er != nil {
         i.l.Error("退出消费", logger.Error(er))
      }
   }()
   return err
}

// Consume 消费处理函数
func (i *InteractiveReadEventConsumer) Consume(msg *sarama.ConsumerMessage,
	event ReadEvent) error {
	start := time.Now()
	ctx, cancel := context.WithTimeout(context.Background(), time.Second)
	defer cancel()
	err := i.repo.IncrReadCnt(ctx, "article", event.Aid)
	t := time.Since(start).String()
	fmt.Println(t)
	return err
}
```

###### 批量处理消息提高性能

在类似于阅读数的这种场景中，非常适合用批量处理接口来加快消费者速率。在这个解决思路里面，我们假定发送者那一边，依旧是一次点击发送一条消息，但是消费者这一边就改进为批量接口。

repo层新增BatchIncrReadCnt函数批量增加阅读次数

```go
func (c *CachedInteractiveRepository) BatchIncrReadCnt(ctx context.Context, biz []string, bizId []int64) error {
   err := c.dao.BatchIncrReadCnt(ctx, biz, bizId)
   if err != nil {
      return err
   }
   go func() {
      for i := 0; i < len(biz); i++ {
         er := c.cache.IncrReadCntIfPresent(ctx, biz[i], bizId[i])
         if er != nil {
            // 记录日志
         }
      }
   }()
   return nil
}
```

dao层使用了for循环插入多条数据

为什么更快？，**因为一次事务就批量处理多个请求，减少了磁盘IO**

对于大多数数据库来说，插入100 条数据，但是每条数据都提交一次，与插入100 条数据，但是一次提交，后者的性能能快上一个数量级。

```go
func (dao *GORMInteractiveDAO) BatchIncrReadCnt(ctx context.Context, bizs []string, bizIds []int64) error {
   return dao.db.WithContext(ctx).Transaction(func(tx *gorm.DB) error {
      txDAO := NewGORMInteractiveDAO(tx)
      for i := 0; i < len(bizs); i++ {
         err := txDAO.IncrReadCnt(ctx, bizs[i], bizIds[i])
         if err != nil {
            return err
         }
      }
      return nil
   })
}
```

###### 组装消费者

消费者会使用新的goroutine运行

```go
type App struct {
   server    *gin.Engine
   consumers []events.Consumer
}
```

```go
func main() {
   initViper()
   app := InitApp()
   //使用消费者
   for _, c := range app.consumers {
      err := c.StartV1()
      if err != nil {
         panic(err)
      }
   }

   app.server.GET("/hello", func(ctx *gin.Context) {
      ctx.String(http.StatusOK, "hello")
   })
   app.server.Run(":8080")
}
```

## 可观测性

### 基本概念

可观测性（Observability）是指在软件系统中，通过度量、监控和分析系统的各个组件行为，以便于了解系统的状态、性能和问题的能力。它可以帮助开发人员快速定位和解决系统中的问题，提高系统的稳定性和可靠性。可观测性被更频繁地用于提高分布式 IT 系统性能。具体来说，分成三个部分：

• **日志（Logging）**：记录系统的状态和行为，用于了解系统的历史记录和实时状态变化。

• **度量（Metrics）**：量化系统的性能和行为，例如 CPU负载、内存使用量等，帮助我们理解和监控系统的整体性能。

• **追踪（Tracing）**：追踪系统的请求和响应，帮助开发人员诊断和调试问题，了解系统中的路径和延迟。

### prometheus

#### 入门

##### Prometheus 基本架构

Prometheus 自身分为客户端和服务端。服务端也就是我们采集到数据存储的地方，客户端就是我们用 Prometheus 的地方。

你可以注意到，**Prometheus 采用的是服务端主动轮询的方案，也就是拉模型。**

而 Prometheus 的数据，可以被其余的框架访问，可以在 UI 访问。比如最典型的就是使用 Grafana 来
查看 Prometheus 的数据。

##### Prometheus 安装

这里也同样使用 Docker 来安装 Prometheus。注意要暴露端口。

这里指定了配置文件

```yaml
  prometheus:
    image: m.daocloud.io/docker.io/prom/prometheus:latest
    volumes:
#      - 注意是yml不是yaml
      - ./prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - 9090:9090
```

##### Prometheus 配置文件

在前面我们知道了，Prometheus 采用的是主动拉数据的模型。所以很自然地就要告诉 Prometheus 的服务器，拉谁、怎么拉。

这部分就是通过配置文件来设置的。配置文件有非常多的配置项，但是都不需要你提前学习，你只需要学会右图中最小化的配置。在实践中要注意设置 scrape_interval，高并发的应用不宜太过频繁。在 webook 里面，启动了一个额外的端口来暴露自己采集的数据。

prometheus.yaml如下，服务端访问192.168.83.1:8081获取数据

```yaml
scrape_configs:
  - job_name: "webook"
    scrape_interval: 5s
    scrape_timeout: 3s
    static_configs:
#      - 这个是访问webook采集数据的端口
      - targets: ["192.168.83.1:8081"]
```

注册了prometheus路由，浏览器通过这个路由访问可以查询到向prometheus客户端写入的数据，然后服务端也可以通过这个路由主动获取到这些数据

```go
// 初始化prometheus
func initPrometheus() {
   go func() {
      // 专门给 prometheus 用的端口
      http.Handle("/metrics", promhttp.Handler())
      http.ListenAndServe("192.168.83.1:8081", nil)
   }()
}
```

##### 查看 Prometheus 采集的数据

浏览器访问192.168.83.1:8081获取客户端数据

docker部署服务端后，可以访问 localhost:9090 查看是否获取了客户端数据

#### api使用

#### Prometheus 埋点技巧

##### 利用中间件来统计 HTTP 请求

统计 HTTP 请求，最关键的是统计响应时间。正常来说，在响应时间上，使用 Summary 比较多。但显然也可以利用 middleware 来统计。右图就是一个统计 HTTP 响应时间的例子。

```go
// BuildResponseTime 统计响应时间
func (b *Builder) BuildResponseTime() gin.HandlerFunc {
   // pattern 是指命中的路由
   labels := []string{"method", "pattern", "status"}
   vector := prometheus.NewSummaryVec(prometheus.SummaryOpts{
      Namespace: b.Namespace,
      Subsystem: b.Subsystem,
      Help:      b.Help,
      // Namespace 和 Subsystem 和 Name 都不能有 _ 以外的其它符号
      Name: b.Name + "_resp_time",
      ConstLabels: map[string]string{
         "instance_id": b.InstanceId,
      },
      Objectives: map[float64]float64{
         0.5:   0.01,
         0.75:  0.01,
         0.9:   0.01,
         0.99:  0.001,
         0.999: 0.0001,
      },
   }, labels)
   prometheus.MustRegister(vector)
   return func(ctx *gin.Context) {
      start := time.Now()
      defer func() {
         // 准备上报 prometheus
         duration := time.Since(start).Milliseconds()
         method := ctx.Request.Method
         pattern := ctx.FullPath()
         status := ctx.Writer.Status()
         vector.WithLabelValues(method, pattern, strconv.Itoa(status)).
            Observe(float64(duration))
      }()
      ctx.Next()
   }
}
```

### OpenTelemetry

#### OpenTelemetry 简介

OpenTelemetry（OpenTelemetry Protocol）是一套由 CNCF 主导的云原生可观测性的标准协议，简称 OTLP。

它旨在创建和管理遥测数据，例如 Trace、Metrics 和 Logs。该项目提供了一个与供应商无关的实现，可以将其配置为将遥测数据发送到您选择的后端。它支持各种流行的开源项目，包括Jaeger 和 Prometheus。正常来说，建议优先使用OpenTelemetry 的 API。

#### OpenTelemetry API 入门

OpenTelemetry 的 API 初始化的时候有几个步骤：

• 设置 Propagator：这个适用于将 context.Context中的 otel 的数据在客户端和服务端之间传递。

• 设置 TracerProvider：链路追踪用的。这些 Provider 都是要考虑关闭的，在实践中不要忘记了。

##### 设置Propagator

```go
func newPropagator() propagation.TextMapPropagator {
   return propagation.NewCompositeTextMapPropagator(
      propagation.TraceContext{},
      propagation.Baggage{},
   )
}

prop := newPropagator()
// 在客户端和服务端之间传递 tracing 的相关信息
otel.SetTextMapPropagator(prop)
```

##### 设置TracerProvider

我们使用 Zipkin 的 Exporter。也就是用 Zipkin的 Exporter 来实现一个 TracerProvider。

你可以认为 Zipkin 适配了 otel 的 API，所以在 API层面上用的是 otel，但是实际上我们的数据是上传到了 Zipkin 上。

```go
func newTraceProvider(res *resource.Resource) (*trace.TracerProvider, error) {
	exporter, err := zipkin.New(
		"http://localhost:9411/api/v2/spans")
	if err != nil {
		return nil, err
	}

	traceProvider := trace.NewTracerProvider(
		trace.WithBatcher(exporter,
			// Default is 5s. Set to 1s for demonstrative purposes.
			trace.WithBatchTimeout(time.Second)),
		trace.WithResource(res),
	)
	return traceProvider, nil
}

// 初始化 trace provider
// 这个 provider 就是用来在打点的时候构建 trace 的
tp, err := newTraceProvider(res)
defer tp.Shutdown(context.Background())
otel.SetTracerProvider(tp)
```

##### 创建 Span

整体来说 OpenTelemetry API 使用起来还是非常简单的。注意点：

• 当调用 Start 的时候，**如果传进去的 ctx 里面有 Span 的信息，就会用 ctx 里面的Span 作为父 Span。**

• 每一个 Span 都要记得调用 End 方法。原则是谁创建的，谁来调用 End 方法。

```go
server := gin.Default()
server.GET("/test", func(ginCtx *gin.Context) {
   // 名字唯一
   tracer := otel.Tracer("geektime/opentelemetry")
   var ctx context.Context = ginCtx

   ctx, span := tracer.Start(ctx, "top-span")
   defer span.End()
   time.Sleep(time.Second)
   span.AddEvent("发生了某事")

   //传入ctx
   ctx, subSpan := tracer.Start(ctx, "sub-span")
   defer subSpan.End()
   subSpan.SetAttributes(attribute.String("attr1", "value1"))
   time.Sleep(time.Millisecond * 300)

   ginCtx.String(http.StatusOK, "测试 span")
})
server.Run(":8082")
```

#### 业务中使用

##### 初始化OpenTelemetry

docker部署zpkin服务器，采集了数据后，就可以通过http://localhost:9411/zipkin访问了

```yaml
zipkin:
  #    用的是不支持 Kafka 之类的简化版本
  image: m.daocloud.io/docker.io/openzipkin/zipkin-slim:2.24
  ports:
    - '9411:9411'
```

```go
// InitOTEL 返回一个关闭函数，并且让调用者关闭的时候来决定这个 ctx
func InitOTEL() func(ctx context.Context) {
   res, err := newResource("webook", "v0.0.1")
   if err != nil {
      panic(err)
   }
   prop := newPropagator()
   // 在客户端和服务端之间传递 tracing 的相关信息
   otel.SetTextMapPropagator(prop)

   // 初始化 trace provider
   // 这个 provider 就是用来在打点的时候构建 trace 的
   tp, err := newTraceProvider(res)
   if err != nil {
      panic(err)
   }
   otel.SetTracerProvider(tp)
   return func(ctx context.Context) {
      _ = tp.Shutdown(ctx)
   }
}

func newResource(serviceName, serviceVersion string) (*resource.Resource, error) {
   return resource.Merge(resource.Default(),
      resource.NewWithAttributes(semconv.SchemaURL,
         semconv.ServiceName(serviceName),
         semconv.ServiceVersion(serviceVersion),
      ))
}

func newTraceProvider(res *resource.Resource) (*trace.TracerProvider, error) {
   exporter, err := zipkin.New(
      "http://localhost:9411/api/v2/spans")
   if err != nil {
      return nil, err
   }

   traceProvider := trace.NewTracerProvider(
      trace.WithBatcher(exporter,
         // Default is 5s. Set to 1s for demonstrative purposes.
         trace.WithBatchTimeout(time.Second)),
      trace.WithResource(res),
   )
   return traceProvider, nil
}

func newPropagator() propagation.TextMapPropagator {
   return propagation.NewCompositeTextMapPropagator(
      propagation.TraceContext{},
      propagation.Baggage{},
   )
}
```

##### gin使用opentelemetry

OpenTelemetry 里面提供了一个 Gin 的接入的middleware，我们可以直接用。

```go
//使用OpenTelemetry自带的中间件
otelgin.Middleware("webook"),
```

#####  GORM 中接入 OpenTelemetry

同样地，我们可以考虑在 GORM 中接入OpenTelemetry。GORM 提供了一个实现，我们可以直接使用。

**`注意：web层和dao层使用的要是同一个ctx，web层gin.context转换成context.Context`**

```go
//使用OpenTelemetry统计gorm
err = db.Use(tracing.NewPlugin(tracing.WithoutMetrics(),
   tracing.WithDBName("webook")))
if err != nil {
   panic(err)
}
//不要记录metrics
tracing.WithoutMetrics()
//不要记录参数
tracing.WithoutQueryVariables()
```

##### 运行效果

通过wrk发起用户注册请求

```bash
wrk -t1 -d10m -c10 -s ./scripts/wrk/signup.lua http://192.168.83.1:8080/users/signup
```

在浏览器访问zipkin http://localhost:9411/zipkin

![image-20240925151011826](assets/image-20240925151011826.png)

### 监控与告警

集成 Grafana

大部分公司内部都是使用 Grafana 来做仪表盘、查看数据、配置告警和监控，我们也使用Grafana。

## 榜单模型与分布式任务调度

### 需求分析

我们现在有一个业务需求：展示一个热点榜单，展示五十条。从非功能性上来说，热榜功能通常是作为首页的一部分，或者至少是一个高频访问的页面，因此性能和可用性都要非常高。
问题关键点：
• 什么样的才算是热点？
• 如何计算热点？
• 热点必然带来高并发，那么怎么保证性能？
• 如果热点功能崩溃了，怎么样降低对整个系统的影响？

#### 热点计算

热点数据都有一些基本规律。
• 综合考虑了用户的各种行为：例如观看数量、点赞、收藏等。
• 综合考虑时间的衰减特性：包括内容本身的发布时间，用户点赞、收藏的时间。
• 权重因子：这一类可以认为是网站有意识地控制某些内容是否是热点，它可能有好几个参数，也可能是只有
一个综合的参数。

这里我们使用一个简单的方案计算一个帖子的热度，p为点赞数，T为距离发表已过去的时间

**Score = (P - 1) / (T + 2)^G**

#### 使用异步定时计算

如果采用实时计算热榜的话：

• 要**扫描全表**，找出所有帖子的点赞数和发表时间。
• 计算每一个帖子的 score 并且全局排序。
• 找出排名前 50 的。

这样性能非常低下，解决方案是：**异步定时计算。**

**考虑到热榜数据不一定要求非常准确，我们可以每隔一段时间才计算一次热榜，相比频繁的查询，性能无疑提高但降低了实时性。间隔越短，实时性就越好。**

在这个基础上，要进一步考虑：

• **怎么设计缓存**，保证有极好的查询性能。

• **怎么保证可用性**，保证在任何情况下都能拿到热榜数据。

### 设计与实现

#### 热榜计算算法

算法核心是依赖于点赞数，因此我们需要找到每一篇文章的点赞数，而后计算对应的 score。排序出热度前100的帖子。

- 分批次查询，每次只查询一批数据
- 使用小根堆实现的优先队列维护热度前100的数据

每次查询一批贴子，通过贴子id查询对应点赞数据，计算出score，如果队列未满则入队，如果队列已满则考虑score在不在前100名，在就替换score最小的数据。最后得出结果，将结果放入缓存。

```go
func (b *BatchRankingService) topN(ctx context.Context) ([]domain.Article, error) {
   //只取7天内的数据
   now := time.Now()
   //先拿一批数据
   offset := 0
   type Score struct {
      art   domain.Article
      score float64
   }
   topN := queue.NewConcurrentPriorityQueue[Score](b.n,
      func(src Score, dst Score) int {
         if src.score > dst.score {
            return 1
         } else if src.score == dst.score {
            return 0
         } else {
            return -1
         }
      })
   //先拿一批数据
   for {
      arts, err := b.artSvc.ListPub(ctx, now, offset, b.batchSize)
      if err != nil {
         return nil, err
      }
      //获取ids
      ids := slice.Map[domain.Article, int64](arts,
         func(idx int, src domain.Article) int64 {
            return src.Id
         })
      //找对应的点赞数据
      intrs, err := b.intrSvc.GetByIds(ctx, "article", ids)
      //计算score
      for _, art := range arts {
         intr, ok := intrs[art.Id]
         //没有对应的点赞数据
         if !ok {
            continue
         }
         score := b.scoreFunc(art.Utime, intr.LikeCnt)
         //队列未满直接入队
         err = topN.Enqueue(Score{
            art:   art,
            score: score,
         })
         //队列已满，考虑score在不在前100名，在就替换score最小的数据
         if err == queue.ErrOutOfCapacity {
            val, _ := topN.Dequeue()
            if val.score < score {
               topN.Enqueue(Score{
                  art:   art,
                  score: score,
               })
            }
         }
      }
      //如果没有下一批数据了，或已经到了7天前的数据
      if len(arts) < b.batchSize || now.Sub(arts[len(arts)-1].Utime).Hours() > 7*24 {
         break
      }
      offset = offset + len(arts)
   }
   //最后得出结果
   res := make([]domain.Article, b.n)
   for i := b.n - 1; i >= 0; i-- {
      val, err := topN.Dequeue()
      //取完了不够n个数据
      if err != nil {
         break
      }
      res[i] = val.art
   }
   return res, nil
}
```

#### 缓存初步实现

查询到数据后就要放入缓存

```go
func (b *BatchRankingService) TopN(ctx context.Context) error {
	//先拿一批数据
	arts, err := b.topN(ctx)
	if err != nil {
		return err
	}
	// 最终是要放到缓存里面的
	// 存到缓存里面
	return b.repo.ReplaceTopN(ctx, arts)
}

```

```go
func (r *RankingRedisCache) Set(ctx context.Context, arts []domain.Article) error {
   for i := range arts {
      arts[i].Content = arts[i].Abstract()
   }
   val, err := json.Marshal(arts)
   if err != nil {
      return err
   }
   return r.client.Set(ctx, r.key, val, r.expiration).Err()
}
```

#### 定时任务实现

##### job接口

在这个完成之后，我们就可以利用这个 Service 来组装成一个定时任务了。为了放这个定时任务，**我们引入一个新的模块：Job。**Job 不属于 DDD 中的概念。

从项目结构上来说，**`Job、Web 和 gRPC 都是同一个层级`**的，也就是属于“对外”暴露方式。

因为将来可能不只有计算热榜这个任务，所以我们定义一个job接口以便扩展，在此基础上实现热榜计算任务，**我们只需调用run()方法即可**。

```go
type Job interface {
   Name() string
   Run() error
}
```

```go
type RankingJob struct {
   svc     service.RankingService
   l       logger.LoggerV1
   timeout time.Duration
}

func NewRankingJob(svc service.RankingService, l logger.LoggerV1, timeout time.Duration) *RankingJob {
   return &RankingJob{svc: svc, l: l, timeout: timeout}
}

func (r *RankingJob) Name() string {
   return "ranking"
}

func (r *RankingJob) Run() error {
   ctx, cancel := context.WithTimeout(context.Background(), time.Second*10)
   defer cancel()
   return r.svc.TopN(ctx)
}
```

##### 定时任务封装

###### 定时器

定时器的实现有很多种

time包中的ticker和timer，ticker是周期性计时器会重复执行，而timer是单次定时器只执行一次，两者都可以通过stop结束

```go
// 定时器
func TestTicker(t *testing.T) {
   tm := time.NewTicker(time.Second)
   defer tm.Stop()
   for now := range tm.C {
      t.Log(now.Unix())
   }
}

// 计时器
func TestTimer(t *testing.T) {
   tm := time.NewTimer(time.Second)
   defer tm.Stop()
   go func() {
      for now := range tm.C {
         t.Log(now.Unix())
      }
   }()
}
```

除了直接使用 time.Timer 来控制定时任务，更加常见的做法是利用 cron 表达式以及对应的开源库来控制定时任务。cron 表达式不仅可以控制固定间隔时间执行，还可以控制在特定的时间点执行

使用 **`github.com/robfig/cron/v3`** 这个库。

• 创建的时候要注意设置 WithSeconds。

• **通过 AddJob/AddFunc 来添加任务**，该方法是线程安全的。

• **调用 Start 来开始调度任务**。

• **调用 Stop 之后，只是暂停了调度，还要利用返回的 ctx 来监听任务结束。**

```go
func TestCronExpression(t *testing.T) {
	expr := cron.New(cron.WithSeconds())
	expr.AddJob("@every 1s", myJob{})
	expr.AddFunc("@every 1s", func() {
		t.Log("111")
	})
	expr.Start()
	//模拟运行了10s
	time.Sleep(time.Second * 10)
	//发出停止信号，但不会中断已经调度的任务
	stop := expr.Stop()
	//阻塞等待所有任务完成
	<-stop.Done()
}

type myJob struct {
}

func (m myJob) Run() {
	log.Println("222")
}
```

###### 封装为 cron.Job

我们利用一个 Builder 模式来构造各种 cron.Job。在这个 Job 里面，我加上了 Prometheus 的监控和日志。
正常来说，你需要在 Grafana 里面添加对运行时间、运行失败的监控的和告警。

**这里builder模式为cron要使用的job封装了我们的热榜计算功能以及监控和日志**

```go
type CronJobBuilder struct {
   l      logger.LoggerV1
   vector *prometheus.SummaryVec
}

func NewCronJobBuilder(l logger.LoggerV1, opt prometheus.SummaryOpts) *CronJobBuilder {
   vector := prometheus.NewSummaryVec(opt,
      []string{"job", "success"})
   return &CronJobBuilder{
      l:      l,
      vector: vector}

}

func (b *CronJobBuilder) Build(job Job) cron.Job {
   name := job.Name()
   return cronJobAdapterFunc(func() {
      // 接入 tracing
      start := time.Now()
      b.l.Debug("开始运行",
         logger.String("name", name))
      err := job.Run()
      if err != nil {
         b.l.Error("执行失败",
            logger.Error(err),
            logger.String("name", name))
      }
      b.l.Debug("结束运行",
         logger.String("name", name))
      duration := time.Since(start)
      b.vector.WithLabelValues(name, strconv.FormatBool(err == nil)).
         Observe(float64(duration.Milliseconds()))
   })
}

type cronJobAdapterFunc func()

func (c cronJobAdapterFunc) Run() {
   c()
}
```

###### 任务启动

初始化热榜计算任务，封装成定时任务后在main函数启动即可

```go
func InitRankingJob(svc service.RankingService, l logger.LoggerV1,
   rlockClient *rlock.Client) *job.RankingJob {
   //一次运行不超过30秒
   return job.NewRankingJob(svc, l, time.Second*30, rlockClient)
}

func InitJobs(l logger.LoggerV1, rjob *job.RankingJob) *cron.Cron {
   builder := job.NewCronJobBuilder(l, prometheus.SummaryOpts{
      Namespace: "geekbang_daming",
      Subsystem: "webook",
      Name:      "cron_job",
      Help:      "定时任务执行",
      Objectives: map[float64]float64{
         0.5:   0.01,
         0.75:  0.01,
         0.9:   0.01,
         0.99:  0.001,
         0.999: 0.0001,
      },
   })
   expr := cron.New(cron.WithSeconds())
   //定时执行job，每3分钟执行一次
   _, err := expr.AddJob("0 */3 * * *", builder.Build(rjob))
   if err != nil {
      panic(err)
   }
   return expr
}
```

```go
//启动定时job
app.cron.Start()
defer func() {
   // 等待定时任务退出
   <-app.cron.Stop().Done()
}()
```

#### 查询接口

##### 缓存方案

查询热榜的接口是一个首页高频访问的功能，**因此我们必须使用缓存，在追求极致性能的情况下，我们引入本地缓存。**

**在 Repository 上直接操作 Redis 实现和本地缓存实现。**

再查询榜单时，**先去查询本地缓存，没找到再去查询redis缓存。更新缓存时两者先更新本地缓存，因为本地缓存基本不可能出错。**

**为了进一步提高一点性能可以使用`缓存预加载`，在查询到榜单数据后，可以提前缓存几个帖子详情**

```go
func (repo *CachedRankingRepository) ReplaceTopNV1(ctx context.Context, arts []domain.Article) error {
   _ = repo.localCache.Set(ctx, arts)
   return repo.redisCache.Set(ctx, arts)
}
```

```go
func (repo *CachedRankingRepository) GetTopNV1(ctx context.Context) ([]domain.Article, error) {
	res, err := repo.localCache.Get(ctx)
	if err == nil {
		return res, nil
	}
	res, err = repo.redisCache.Get(ctx)
	//回写本地缓存
	if err == nil {
		repo.localCache.Set(ctx, res)
	} else {
		//使用本地缓存解决redis可用性问题，进行兜底
		return repo.localCache.ForceGet(ctx)
	}
	return res, err
}
```

##### 可用性提升

整个榜单依赖于数据库和 Redis 的可用性。那么问题就在于：万一这两个东西崩溃了呢。首先可以肯定一点的就是：

- 如果 MySQL 都崩溃了，那么肯定没有办法更新榜单了，因为此时你的定时任务必然失败。
- 如果 Redis 崩溃了，后果就是一旦节点本身的本地缓存也失效了，那么查询接口就会失败。

解决方案如下：

- redis故障提升可用性：给本地缓存设置一个兜底。即正常情况下，我们的会从本地缓存里面获取，获取不到就会去Redis 里面获取。**但是我们可以在 Redis 崩溃的时候，再次尝试从本地缓存获取。此时不会检查本地缓存是否已经过期了**
- 当mysql崩溃后，redis缓存也会因为过期消失，我们可以直接设置redis缓存永不过期，**这样即便是数据库有问题，导致定时任务无法运行，但是可以预期的是，Redis 中始终都有缓存的数据。**

```go
func (repo *CachedRankingRepository) GetTopNV1(ctx context.Context) ([]domain.Article, error) {
   res, err := repo.localCache.Get(ctx)
   if err == nil {
      return res, nil
   }
   res, err = repo.redisCache.Get(ctx)
   //回写本地缓存
   if err == nil {
      repo.localCache.Set(ctx, res)
   } else {
      //使用本地缓存解决redis可用性问题，进行兜底
      return repo.localCache.ForceGet(ctx)
   }
   return res, err
}
```

##### 本地缓存扩散

因为榜单计算由一个节点计算，所以本地缓存只会缓存在本地。后续可以实现，本地缓存在多实例下的扩散，提高性能

### 分布式任务调度

分布式锁原理与实现，csdn

## 微服务

### 拆分微服务-模块化

#### 微服务拆分路线

![image-20240930172702902](assets/image-20240930172702902.png)

这里我们先找一个模块直接拆分出去，作为一个独立的微服务提前验证整个微服务拆分流程。

#### 模块化

##### 梳理与分析重构点

在重构的时候，最忌讳的就是没有分析重构点，就直接开始重构，这样你很可能是在重构的过程中，突然发现某个地方漏了，那时候就可能很难补救了。具体来说（包含对应的测试文件）：

• Web 层：受影响的是 article.go。
• Service 层：受影响的是 article.go、interactive.go。
• Domain 层： interactive.go。
• Repository 层：interactive.go。
• Cache 层：interactive.go。
• DAO 层：interactive.go、init.go。
• wire.go
• ioc：db.go，这里主要是初始化表结构的地方需要修改。
• events: article.go。

##### 执行拆分

在确保有足够的测试来验证接下来的重构步骤之后，我们就可以开始执行第一步：**模块化**。

在这一个步骤中，要充分利用 IDE 的重构功能，并且在任何一个关键步骤之后运行测试来验证效果。模块化是指，我们会把模块内的代码都拆分出去放到一个目录里面，这个目录同样会包含 service、domain、repository 等目录。我们先把核心的内容挪出去，也就是我展示的。

interactive

```
├─domain
├─events
├─grpc
├─integration
│  └─startup
├─repository
│  ├─cache
│  │  └─lua
│  └─dao
└─service
```

##### 解决数据库初始化的问题

注意在我们的 dao 里面有一个 init 文件，里面就是初始化数据库的表结构，所以我们需要同样挪出来。

##### 动消费者

在使用了 Kafka 的形态中，增加阅读数是通过订阅Kafka 的消息来实现的，所以对应的代码，我们也要挪动过来。

##### 集成测试

重新对这个服务进行集成测试

#### 模块依赖化

接下来，我们要做的就是**将整个模块的代码挪动到一个新的代码仓库。**

### 拆分微服务-微服务化

#### 技术选型与框架

在真正执行微服务化之前，我们要先确定自己的微服务框架的选型（虽然之前我已经确定了我们要用 gRPC）。目前在 Go 里面，可以选择的微服务框架并不是很多。选型主要考虑以下几个方面：

• **功能性**：这个可以说是最重要的，也可以说是最不重要的。因为在大多数时候，如果你的业务没有任何特别之处，那么你可以认为主流的开源框架都能满足你的需求。

• **成熟度**：这个成熟度主要是包含社区支持度、搜索引擎支持度、招聘人才难度。也就是说，你需要考虑到你遇到问能不能搜索到解决方案，招人容不容易……

• **非功能性**：只有很少的一些应用会很关注非功能性，比如说性能。中小型公司基本上不需要考虑这个因素。

**Go 中的微服务框架**

在 Go 中可供考虑的框架有：

• gRPC：这是一个专注于 RPC 的框架，但是也提供了基本的负载均衡之类的功能。同时生态也最好，有非常多的相关开源软件。

• go-zero：可以认为是一个集成了 Web 开发和微服务开发各种功能的微服务框架，它的底层通信就是利用了 gRPC。插件非常丰富，而且是中国人维护的，所以没有什么语言壁垒。

• Dubbo-go：实现了 Dubbo 协议，所以可以无缝和 Java 的 Dubbo 应用进行通信，功能强大，社区也很活跃，也支持 gRPC 协议。

• go-micro：专注于微服务的简单易用的微服务框架，有自己的协议，也适配了 gRPC 协议。

从功能性上来说，它们没特别大的差距，基本上都能满足实际需要；从成熟度上来说，gRPC最好；从非功能性上来说，毫无疑问 gRPC 性能最好。因此，我们就使用 gRPC。

#### api管理

##### 目录结构

这里我们使用同一个仓库。比如说一个公司（或者一个部门）只有一个 API 仓库，所有的微服务的 API 定义都放在这里。

![image-20240930174359356](assets/image-20240930174359356.png)

##### protobuf定义

定义interactiveService和消息结构体

```protobuf
syntax = "proto3";

package intr.v1;
option go_package = "intr/v1;intrv1";

service InteractiveService {
  rpc IncrReadCnt(IncrReadCntRequest) returns (IncrReadCntResponse);
  rpc Like(LikeRequest) returns(LikeResponse);
  rpc CancelLike(CancelLikeRequest) returns (CancelLikeResponse);
  rpc Collect(CollectRequest) returns(CollectResponse);
  rpc Get(GetRequest) returns (GetResponse);
  rpc GetByIds(GetByIdsRequest) returns(GetByIdsResponse);
}

message GetByIdsRequest {
  string biz = 1;
  repeated int64 ids = 2;
}

message GetByIdsResponse {
  map<int64, Interactive> intrs = 1;
}

message GetResponse {
  Interactive intr = 1;
}

message Interactive {
  string biz = 1;
  int64 biz_id =2;
  int64 read_cnt = 3;
  int64  like_cnt = 4;
  int64 collect_cnt = 5;
  bool  liked = 6;
  bool  collected = 7;
}

message GetRequest {
  string biz = 1;
  int64 biz_id = 2;
  int64  uid = 3;
}

message CollectResponse {

}

message CollectRequest {
  string biz = 1;
  int64 biz_id = 2;
  int64  uid = 3;
  int64 cid = 4;
}

message CancelLikeRequest {
  string biz = 1;
  int64 biz_id = 2;
  int64  uid = 3;
}

message CancelLikeResponse {

}

message LikeRequest {
  string biz = 1;
  int64 biz_id = 2;
  int64  uid = 3;
}

message LikeResponse {

}


message IncrReadCntRequest {
  string biz = 1;
  int64 biz_id = 2;
}

message IncrReadCntResponse {

}
```

##### 使用工具管理protobuf

从https://github.com/bufbuild/buf/releases下载buf工具，

编写**buf.gen.yaml 文件**，里面定义了 buf 怎么帮我们管理和编译 protobuf。

• go_package_prefix: 避免了每次写 go package 都要写老长一段的问题。

• plugins: 这是比较关键的配置，这里指定了两个插件，并且在两个插件里面分别指定了 out 和 opt。

• Go 语言插件

• gRPC 插件

```yaml
version: v1
managed:
  enabled: true
  go_package_prefix:
    default: "geektime/webook/api/proto/gen"
plugins:
  - plugin: buf.build/protocolbuffers/go
    out: api/proto/gen
    opt: paths=source_relative
  - plugin: buf.build/grpc/go
    out: api/proto/gen
    opt:
      - paths=source_relative
```

最终在webook下运行我封装好的 make grpc 就可以，又或者直接执行 

```bash
buf generate api/proto。
```

**生成了interactive相关结构体，和grpc客户端和服务端接口**

#### 实现rpc接口

**在 API 定义出来之后，我们需要做得就是实现这些gRPC 接口。**封装好interactiveService服务, 将其包装实现grpc服务端

```go
// InteractiveServiceServer 这里只是把service包装成一个grpc
// 和grpc有关的操作限定在这里
type InteractiveServiceServer struct {
   intrv1.UnimplementedInteractiveServiceServer
   //封装好业务的核心逻辑
   svc service.InteractiveService
}

func NewInteractiveServiceServer(svc service.InteractiveService) *InteractiveServiceServer {
   return &InteractiveServiceServer{svc: svc}
}

func (i *InteractiveServiceServer) IncrReadCnt(ctx context.Context, request *intrv1.IncrReadCntRequest) (*intrv1.IncrReadCntResponse, error) {
	err := i.svc.IncrReadCnt(ctx, request.Biz, request.BizId)
	return &intrv1.IncrReadCntResponse{}, err
}

func (i InteractiveServiceServer) Like(ctx context.Context, request *intrv1.LikeRequest) (*intrv1.LikeResponse, error) {
	err := i.svc.Like(ctx, request.Biz, request.BizId, request.Uid)
	return &intrv1.LikeResponse{}, err
}

func (i InteractiveServiceServer) CancelLike(ctx context.Context, request *intrv1.CancelLikeRequest) (*intrv1.CancelLikeResponse, error) {
	//可以做参数校验
	//也可以用grpc的插件
	if request.Uid <= 0 {
		return nil, status.Error(codes.InvalidArgument, "uid 错误")
	}
	err := i.svc.CancelLike(ctx, request.Biz, request.BizId, request.Uid)
	return &intrv1.CancelLikeResponse{}, err
}
....
```

##### gRPC 模块

注意，我们的实现都是放到了一个 gRPC 的包里面。从地位上来说 gRPC、 Job 和 Web 都是同一个级别的，是”服务对外表现形式“：

• 对 Job 来说，就是我以定时任务的形式暴露出去。
• 对于 Web 来说，就是我以 HTTP 接口的形式暴露出去。
• 对于 gRPC 来说，就是我以 RCP 接口的形式暴露出去。

<img src="assets/image-20241008142705579.png" alt="image-20241008142705579" style="zoom:33%;" />

#### 集成测试

这里我们将 Interactive 的集成测试分离出来，放到 Interactive 内部的 integration 里面。同时，将原本的集成测试调整为测试 gRPC 的 Server。

```go
svc := startup.InitInteractiveGRPCService()
for _, tc := range testCases {
   s.T().Run(tc.name, func(t *testing.T) {
      tc.before(t)
      resp, err := svc.IncrReadCnt(context.Background(), &intrv1.IncrReadCntRequest{
         Biz:   tc.biz,
         BizId: tc.bizId,
      })
      assert.Equal(t, tc.wantErr, err)
      assert.Equal(t, tc.wantResp, resp)
      tc.after(t)
   })
}
```

#### 微服务启动

创建一个app结构体封装grpc的server和kafka消费者，通过server监听客户端连接

```go
type App struct {
   server    *grpcx.Server
   consumers []saramax.Consumer
}
```

```go
func main() {
   initViper()
   app := InitApp()
   //启动消费者
   for _, c := range app.consumers {
      err := c.StartV1()
      if err != nil {
         panic(err)
      }
   }
   //监听，等待客户端连接
   err := app.server.ListenAndServe()
   if err != nil {
      panic(err)
   }
}
```

#### 客户端实现

##### 替换grpc客户端

替换原本interactiveService的实现，在所有使用 InteractiveService 的地方替换为使用 gRPC 的客户端。

```go
type ArticleHandler struct {
   svc     service.ArticleService
   l       logger.LoggerV1
   intrSvc intrv1.InteractiveServiceClient
   biz     string
}
```

```go
//获取这篇文章的所有计数
resp, err := h.intrSvc.Get(ctx, &intrv1.GetRequest{
   Biz:   h.biz,
   BizId: id,
   Uid:   uc.Uid,
})
intr := resp.Intr
```

```go
type RankingService interface {
   TopN(ctx context.Context) error
   GetTopN(ctx context.Context) ([]domain.Article, error)
}
type BatchRankingService struct {
	repo      repository.RankingRepository
	artSvc    ArticleService
	intrSvc   intrv1.InteractiveServiceClient
	batchSize int
	n         int
	scoreFunc func(t time.Time, likeCnt int64) float64
}
```

##### 本地调用和 gRPC 调用并行方案

在我们的最开始的方案里面，为了保证迁移顺利，并且保证可以回滚，我们有一个阶段是请求可以是本地调用 InteraciveService 的，也可以是真的发起了 gRPC 调用，这个过程我们需要进一步控制住流量，为了达成这个目的，综合使用适配器和装饰器模式：

• 适配 InteraciveService，构成一个本质上是本地调用的 gRPC 客户端。

• 将真实的 gRPC 和本地调用的 gRPC 客户端装饰在一起，并且接入流量控制机制。

###### 本地调用客户端实现

通过适配器模式将service.InteractiveService适配给grpc的客户端接口

```go
// InteractiveServiceAdapter
// 将本地调用适配给InteractiveServiceClient客户端
type InteractiveServiceAdapter struct {
   svc service.InteractiveService
}

func NewInteractiveServiceAdapter(svc service.InteractiveService) *InteractiveServiceAdapter {
   return &InteractiveServiceAdapter{svc: svc}
}

func (i InteractiveServiceAdapter) IncrReadCnt(ctx context.Context, in *intrv1.IncrReadCntRequest, opts ...grpc.CallOption) (*intrv1.IncrReadCntResponse, error) {
   err := i.svc.IncrReadCnt(ctx, in.Biz, in.BizId)
   return &intrv1.IncrReadCntResponse{}, err
}

func (i InteractiveServiceAdapter) Like(ctx context.Context, in *intrv1.LikeRequest, opts ...grpc.CallOption) (*intrv1.LikeResponse, error) {
   err := i.svc.Like(ctx, in.Biz, in.BizId, in.Uid)
   return &intrv1.LikeResponse{}, err
}
```

###### 集成两种客户端与流量调度

流量调度算法很简单：
• 产生一个随机数。
• 如果随机数小于我们的阈值，就使用远程调用。
• 否则，使用本地调用。
在这种机制之下，我们需要考虑监听 Threshold 的变动。

```go
/ GrayScaleInteractiveClient
// 集成grpc和本地的客户端，进行流量控制
type GrayScaleInteractiveClient struct {
   remote intrv1.InteractiveServiceClient
   local  intrv1.InteractiveServiceClient

   //阈值，选择grpc还是本地调用
   threshold *atomicx.Value[int32]
}

func NewGrayScaleInteractiveClient(remote intrv1.InteractiveServiceClient, local intrv1.InteractiveServiceClient) *GrayScaleInteractiveClient {
   return &GrayScaleInteractiveClient{
      remote:    remote,
      threshold: atomicx.NewValue[int32](),
      local:     local,
   }
}

func (i *GrayScaleInteractiveClient) IncrReadCnt(ctx context.Context, in *intrv1.IncrReadCntRequest, opts ...grpc.CallOption) (*intrv1.IncrReadCntResponse, error) {
   return i.selectClient().IncrReadCnt(ctx, in, opts...)
}

func (i *GrayScaleInteractiveClient) Like(ctx context.Context, in *intrv1.LikeRequest, opts ...grpc.CallOption) (*intrv1.LikeResponse, error) {
   return i.selectClient().Like(ctx, in, opts...)
}
。。。
func (i *GrayScaleInteractiveClient) selectClient() intrv1.InteractiveServiceClient {
	// [0, 100) 的随机数
	num := rand.Int31n(100)
	if num < i.threshold.Load() {
		return i.remote
	}
	return i.local
}

func (i *GrayScaleInteractiveClient) UpdateThreshold(val int32) {
	i.threshold.Store(val)
}
```

##### 客户端创建并初始化

使用viper读取客户端连接地址以及流量调度阈值，创建好grpc客户端已经本地调用客户端，初始化好集成客户端。

正如我们之前在配置模块里面讲到的，最好的监听配置变更的位置，就是在初始化的地方，监听配置变更之后，更新创建的对象。在 **IoC 中准备客户端实例的时候，监听了配置变更，并且调用了更新 Threshold 的方法。**

```go
func InitIntrGRPCClient(svc service.InteractiveService) intrv1.InteractiveServiceClient {
   type Config struct {
      Addr      string
      Secure    bool
      threshold int32
   }
   var cfg Config
   err := viper.UnmarshalKey("grpc.client.intr", &cfg)
   if err != nil {
      panic(err)
   }

   var opts []grpc.DialOption
   if cfg.Secure {
      //上面，要去加载你的证书之类的东西
      //启用HTTPS
   } else {
      opts = append(opts, grpc.WithTransportCredentials(insecure.NewCredentials()))
   }

   cc, err := grpc.Dial(cfg.Addr, opts...)
   if err != nil {
      panic(err)
   }
   remote := intrv1.NewInteractiveServiceClient(cc)
   local := client.NewInteractiveServiceAdapter(svc)
   res := client.NewGrayScaleInteractiveClient(remote, local)
   //监听配置变化，设置阈值
   viper.OnConfigChange(func(in fsnotify.Event) {
      var cfg Config
      err = viper.UnmarshalKey("grpc.client.intr", &cfg)
      if err != nil {
         //输出日志
      }
      res.UpdateThreshold(cfg.threshold)
   })
   return res
}
```

### 不停机数据迁移

#### 基本理论

##### 为什么要数据迁移

当下，在经过微服务化之后，我们的代码已经拆出去了，但是还在共享一个数据库（逻辑上和物理上都是一个数据库）。在微服务的理论里面，有一个广泛认同的观点，即**某个微服务的数据也是单独存储的**。因此，正常在微服务化之后，我们还要把所有的数据都分离了，单独存储。

另外一方面，绝大多数人在自己的职业生涯里面，都会遇到数据迁移相关的任务，所以非常适合拿出去面试。

##### 异构数据迁移

**这里说的异构数据迁移，是指源表和目标表的结构不一样。显然，如果数据库都不一样，也算是异构数据迁移。**比如说 MySQL 迁移到 MongoDB，一般数据量大了又懒得分库分表，就可能用 MongoDB。异构数据迁移会更加复杂，因为数据转换 - 完整性校验都很麻烦。

• 数据转换：最麻烦的就是不同数据类型转换可能出现不兼容的问题，如果是数字，还会有精度问题。有长度限制，则有溢出问题。

• 完整性校验：也就是常见的外键约束，数据之间的业务关系等，都很难验证。

##### 停机迁移

**因为数据迁移过程很长，在迁移过程中会有数据增删改，所以要停机迁移。**但如果数据量很多，那么不管是导出，还是导入数据，都会非常慢。这就是意味着你的应用要停很久。

##### 不停机迁移

在停机迁移方案里面，我们知道停机对用户体验的伤害是非常大的，所以基本上在现实中，核心应用都是不停机迁移数据。这意味着：

**• 当你一边迁移数据，一边又有新数据产生，或者老数据被更新了。**

**• 当你迁移数据的时候，你不能对数据库造成太大的压力，否则会影响应用的正常运行。**

#### 不停机迁移阶段和步骤

整个不停机迁移可以分成四个阶段：
• 第一阶段：**业务读写源表**。在这个阶段，你要完成目标表的数据初始化过程。

• 第二阶段：**双写阶段，以源表为准，即读写源表，写目标表**。在这个阶段，数据会被双写到源表和目标表中，并且读是读源表，如果数据不一致，也是以源表的数据为准。

• 第三阶段：**双写阶段，以目标表为准。即读写目标表，写源表**在这个阶段，数据也是保持双写，但是读以目标表为准，并且修复数据的时候，是以目标表为准。

• 第四阶段：**业务读写目标表。**

**具体步骤比较复杂：**

• 创建目标表。

• 用源表的数据初始化目标表。

• 执行一次校验，并且修复数据，此时用源表数据修复目标表数据。

• 业务代码开启双写，此时读源表，并且先写源表，数据以源表为准。

• 开启增量校验和数据修复，保持一段时间。

• 切换双写顺序，此时读目标表，并且先写目标表，数据以目标表为准。

• 继续保持增量校验和数据修复。

• 切换为目标表单写，读写都只操作目标表。

<img src="assets/image-20241009202848467.png" alt="image-20241009202848467" style="zoom:33%;" />

#### 具体实现

##### 数据导出并导入到目标表

导出工具：mysqldump

正常来说，最简单的数据导出工具，就是 mysqldump。它是 MySQL 的命令行工具中的一个，原本是设计了用来做数据备份的。这里我们借助 mysqldump 就可以将数据导出为 SQL，而后导入到目标表中。

**-h: 指定了 host。你不指定的话，它默认是本地安装，于是会尝试找 /tmp/mysql.sock，就会出错。**

**--port：指定了端口。**

最终我还指定了数据库和对应的表结构，而后将导出结果写到了 intr.sql 中。你可以使用 --result-file 来指定文件路径。

```
mysqldump -h 127.0.0.1 --port 13316 -u root -p webook --ssl-mode=DISABLE interactives user_like_bizs user_collection_bizs > intr.sql
```

我们新建一个数据库，这个数据库叫做 webook-intr，而后重新导入数据，也就是要先执行一下:create database if not exists webook_intr;

导入的命令也很简单，**命令行连接上 MySQL 之后，直接执行 source 就可以。**

```
mysql -h 127.0.0.1 --port 13316 -uroot -proot --ssl-mode=DISABLE 
```

<img src="assets/image-20241008162409373.png" alt="image-20241008162409373" style="zoom:33%;" />

##### 校验逻辑

###### 校验方案

为了提高修复的速度，我们使用kafka并发进行修复。也就是通过 Kafka 解耦和削峰，我们可以很容易控制住消费者的消费速率，也就间接控制住了目标表的写入速率。当然，从面试的角度上说，也是 Kafka 方案更加高级一点。

<img src="assets/image-20241009203846556.png" alt="image-20241009203846556" style="zoom:33%;" />

###### 校验思路

校验的基本思路就是：**从源表中取出数据，再根据主键去目标表中找出对应的数据，比较所有的字段是否相等。**

我们借助泛型来写一个通用的查询方法，而后要求实体实现具体的比较方法。

```
validator[[T migrator.Entity]]
validator[article]
validator[user]
```

**反向校验**

如果 srcDB 中没有数据，但 dstDB 中又有数据呢？这种情况是在同步数据到 dstDB 之后，srcDB 中的数据就被删除了。**因此，我们还需要有一个反向的修复过程，也就是根据 base 来查找 from，这个过程只需要找 base 中没有的数据，所以整个逻辑比较简单。**

###### 核心代码

```go
func (v *Validator[T]) validateBaseToTarget(ctx context.Context) error {
   offset := 0
   for {
      //一条一条的获取base数据库的数据
      src, err := v.fromBase(ctx, offset)
      if err == context.DeadlineExceeded || err == context.Canceled {
         return nil
      }
      if err == gorm.ErrRecordNotFound {
         // 你增量校验，要考虑一直运行的
         // 全量校验直接退出
         if v.sleepInterval <= 0 {
            return nil
         }
         //sleep后继续增量校验
         time.Sleep(v.sleepInterval)
         continue
      }
      if err != nil {
         // 查询出错了
         v.l.Error("base -> target 查询 base 失败", logger.Error(err))
         // 在这里，跳过这条数据的错误
         offset++
         continue
      }

      // 根据id获取目标数据库的数据
      var dst T
      err = v.target.WithContext(ctx).
         Where("id = ?", src.ID()).
         First(&dst).Error
      switch err {
      case gorm.ErrRecordNotFound:
         // target 没有
         // 丢一条消息到 Kafka 上
         v.notify(src.ID(), events.InconsistentEventTypeTargetMissing)
      case nil:
         //目标表找到了数据，开始比较
         equal := src.CompareTo(dst)
         //数据不一致
         if !equal {
            // 要丢一条消息到 Kafka 上
            v.notify(src.ID(), events.InconsistentEventTypeNEQ)
         }
      default:
         // 记录日志，然后继续
         // 做好监控
         v.l.Error("base -> target 查询 target 失败",
            logger.Int64("id", src.ID()),
            logger.Error(err))
      }
      offset++
   }
}
```

###### 校验的调度时机（ms）

如果你的全量校验会持续很长时间，那么你就可以考虑挑一个合适的时机来执行全量校验，基本的思路有：
• **在业务低峰期运行。**也就是设置一个定时任务，设置时间到业务低峰期。
• **动态判定当前负载，负载高则挂起，负载低则继续。**

##### 数据修复

###### 基本思路

校验过程中出现了数据不一致的情况就会向kafka发送消息，消费者根据消息类型选择不同的处理方法

• 如果 type 是 target_missing：也就是目标表中缺少了数据，那么就是 insert。

• 如果 type 是 neq：源库与目标库数据不一致那么就是更新，直接用源表中的数据来覆盖掉目标表。

• 如果 type 是 base_missing：注意，这时候代表的是目标表多了数据，源库中的数据已被删除，此时应该把目标表中的数据删掉。

###### 核心代码

```go
// Fix 三种类型修复
func (f *OverrideFixer[T]) Fix(ctx context.Context, id int64) error {
   // 最最粗暴的
   var t T
   err := f.base.WithContext(ctx).Where("id=?", id).First(&t).Error
   switch err {
   //base数据被删了，target就不用修复了直接删除
   case gorm.ErrRecordNotFound:
      return f.target.WithContext(ctx).Model(&t).Delete("id = ?", id).Error
   case nil:
      // upsert
      return f.target.WithContext(ctx).Clauses(clause.OnConflict{
         DoUpdates: clause.AssignmentColumns(f.columns),
      }).Create(&t).Error
   default:
      return err
   }
}
```

##### 增量校验与修复

###### 实现方法

增量校验与修复是指：老数据就不需要校验了，新修改的数据会被再次验证和修复。**`该过程是针对双写过程，防止双写中间出现了问题。`**所以如右图，增量校验在第二阶段和第三阶段都可以运行。

增量修复是根据utime查询数据进行校验的，这样就只校验和修复新数据了。

##### 数据库双写实现

这里我们借助GORM中的ConnPool，借助 ConnPool 的思路很高级，要求对 GORM 的底层框架有很深的理解，并且要求有比较高的代码技巧。

```go
// ConnPool db conns pool interface
type ConnPool interface {
   PrepareContext(ctx context.Context, query string) (*sql.Stmt, error)
   ExecContext(ctx context.Context, query string, args ...interface{}) (sql.Result, error)
   QueryContext(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error)
   QueryRowContext(ctx context.Context, query string, args ...interface{}) *sql.Row
}
```

ConnPool 是 GORM 的底层接口，它是直接和数据库打交道的出口，数据库的增删改最终都是调用的ExecContext方法，查询单行数据调用QueryRowContext，查询多行调用QueryContext，**我们要实现一个数据库对象操作两个数据库，可以使用装饰器装饰ConnPool，在其中集成两个数据库的ConnPool操作即可。**与之类似的，是 GORM 还暴露了事务控制的接口，这样我们还可以同步控制住在事务内部的双写。

#### 总结

![数据迁移](assets/数据迁移.png)

除此之外，还有双写的实现

### 服务注册与发现

#### 理论基础（ms）

##### 服务注册与发现基本模型

在我们接入了微服务架构之后，就面临着一个

问题：在微服务架构里面，我怎么知道哪些机器上面部署了我需要的服务？在已有的代码里面，我们使用的是直接写了localhost:8090 这种形式。显然这种方式在开发环境就不合适。

###### 服务注册与发现：IP 直连

最原始的形态，就是直接使用 IP + 端口来通信。这个过程甚至不能称为服务注册与发现，因为没啥注册，也没啥发现。缺点很明显：**IP 是会变的 + 不好维护。**虽然它很原始，但是实践中偶尔也会使用：

• 在开发环境要联调的时候。例如说你和同事合作完成一个任务，而后你需要调用你同事的接口，这个时候，你直接用同事电脑的 IP 地址调用，同事就可以本地 DEBUG 了。

• 当你 DEBUG 的时候。某些时候一些 BUG 只会在特定的某些实例上出现，这时候你可以直接把请求发到这个实例上。



<img src="assets/image-20241010075452879.png" alt="image-20241010075452879" style="zoom:33%;" />

###### 服务注册与发现：使用域名 + DNS

既然直接使用 IP 不好用，那么另外一个可行的思路就是使用域名 + DNS。**考虑到解析域名的开销，所以一般会在客户端这边缓存解析域名的结果。**

优点：

• 简单好用，不需要额外的组件。

缺点：

• 如果客户端不缓存 DNS 结果，那么每次调用都会多一次调用。

• 如果客户端缓存 DNS 结果，那么就可能**无法及时更行本地可用节点列表。**

<img src="assets/image-20241010075656365.png" alt="image-20241010075656365" style="zoom:33%;" />

###### 服务注册与发现：注册中心机制

既然域名解析的问题是不能及时得到通知，那么能

不能让**域名服务器主动通知一下节点变化**？这也就是注册中心的原理，基本的思路就是：

• 服务端在启动的时候主动在注册中心注册一下。

• 客户端在第一次发起调用之前先查询注册中心，而后缓存住可用节点。

• 注册中心在服务端节点发生变动的时候，主动通知一下客户端。

这种形态没有显著的缺点，硬要说缺点，就是注册中心在大规模集群下容易成为瓶颈。

<img src="assets/image-20241010075648685.png" alt="image-20241010075648685" style="zoom:33%;" />

###### 服务注册与发现：借助网关的服务注册与发现

实际上，这个并不能算是服务注册与发现，只是说大家在实践中可能遇到，所以在这里描述一下这个模型。

如右图，事实上当你的请求经过一个微服务网关的时候，你首先得知道网关在哪里。这个过程一般是通过域名来达成的，例如说 gateway.mycompany.com，或者从一个特定的配置项里面得知所有的网关节点的地址。

而网关要把请求转发到真实服务端上，也是需要知道服务端的地址，因此这也是一个服务注册与发现的过程，同样可以是借助注册中心，或者 DNS。

<img src="assets/image-20241010075738631.png" alt="image-20241010075738631" style="zoom: 33%;" />

##### 注册中心机制

###### 服务启动过程

简单来说，服务端在启动的时候，就需要在注册中心注册自身的信息。

那么问题来了：**服务端怎么知道注册中心在哪里？答案是：一般是通过域名 + 端口或者直接指定 IP + 端**
**口来连上注册中心。**

而且，正常来说如果在启动阶段的服务端 A 无法连上注册中心，那么服务端 A 会直接失败（也就是退出程序）。

<img src="assets/image-20241010075910265.png" alt="image-20241010075910265" style="zoom:33%;" />

###### 服务注册什么数据？

那么问题又来了：服务端注册的时候，究竟注册了什么信息？大致上可以分成两类：

**• 定位信息。通俗来说，就是 IP + 端口。**
• **其它信息。这部分信息一般和微服务框架的具体功能有关系，**比如说在支持分组功能的微服务框架里面，服务端节点就需要在注册信息里面带上自己的分组信息。

其中，最为关键的就是定位信息，因为客户端要利用这个信息来连上服务端 A。

###### 服务端和注册中心保持心跳

在注册成功之后，注册中心会和服务端保持心跳。根据注册中心选型，微服务框架设计，心跳有两种模式：

• **服务端主动。也就是服务端每隔一段时间就朝着注册中心发一个心跳**，注册中心可以应答也可以不应答（正常来说，服务端只要没收到网络错误，就可以认为和注册中心的联系是正常的）。

• **注册中心主动。也就是注册中心会主动朝着所有注册的服务端节点发送心跳。**

如果心跳失败，注册中心会判定服务端节点已经崩溃，那么就会通知客户端不要使用这个节点

![心跳机制](assets/心跳机制.png)

本质上来说，注册中心和服务端的整个机制，可以看做是租约，心跳就是续租。那么在租约里面讨论的心跳有关的问题，在注册机制这里一样要讨论。

• 心跳间隔多长？

• 如何判定节点连不上了？是一次心跳就连不上，还是多次心跳连不上？

正常来说，这些都是跟业务有关的。核心就是：

**• 间隔越短，心跳越频繁，压力越大。**

**• 次数越少，越快发现服务端节点崩溃，但是难以应对偶发性心跳失败。**

**• 次数越多，越慢发现服务端节点崩溃，但是可以避免一些偶发性心跳失败的问题。****

> 心跳机制的高级做法

还有一些更加高级的机制，**有效应对偶发性心跳失败**，但是比较少采用，性价比不高。

• 双向心跳。也就是服务端会主动发心跳，注册中心也会主动发心跳。

• 单向心跳失败之后，另外一边主动发起心跳。例如说在正常情况下，都是服务端主动发心跳给注册中心，但是注册中心在一段时间没有收到服务端的心跳之后，会主动发起心跳。

###### 服务下线

很显然，**如果服务端要关闭了，就需要通知注册中心，而后注册中心需要通知客户端。客户端就会把该节点从可用节点列表里面挪走。**

> 服务优雅下线

但是下线还有一些细节需要思考：服务端 A 能不能在告诉注册中心自己下线了，立刻就退出？

答案是不能。因为这个时候服**务端 A 上可能还有一些请求正在处理，而网络上也还有客户端正在发请求。等服务端已经接收的请求处理完毕，服务端才会结束运行。**

###### 客户端

根据微服务框架的设计**，客户端发起`服务发现`有两个时机：**

• **客户端启动的时候。**这种设计一般是在客户端从配置文件，或者注册接口，拿到了所有的需要使用的微服务。于是可以在初始化的过程中直接执行服务发现，如果服务发现失败则客户端不会启动。

• 客户端启动的时候，并没有去做服务发现。而是**在第一次调用某个服务的时候，执行服务发现。**

同样地，注册中心和客户端之间也要保持心跳，这样可以保证在服务端数据变更的时候，能够连上客户端，并且通知客户端。

#### 在gRPC中使用注册中心

##### gRPC服务注册与发现原理

在默认情况下，gRPC 用的是域名解析策略。也就是我们传入的服务地址是一个域名+端口的形态。显然，如果你直接传入的就是 IP 地址，那么连域名解析这一步都省略了。

gRPC 会定时访问域名服务器，更新本地缓存的可用 IP 地址。

<img src="assets/image-20241010152123430.png" alt="image-20241010152123430" style="zoom:33%;" />

在 gRPC 中使用注册中心也很简单，本质上就是提供一个使用注册中心来实现的 Resolver。Resolver 是 gRPC 中的一个核心抽象，它代表的是对服务的解析，也就是获得服务的定位信息在内的一些元数据。本质上，gRPC 根本不知道你使用的是不是注册中心，它只是提供了在客户端这边的Resolver 接口。

<img src="assets/image-20241010152232976.png" alt="image-20241010152232976" style="zoom: 50%;" />

##### 使用etcd作为注册中心

###### 服务注册

etcd 直接提供了一个 gRPC 的服务注册与发现的Resolver 实现。使用起来也很简单。服务端注册的例子。关键点：

• 创建一个 endpoint.Manager。

• 添加一个 Endpoint。

• 后续如果注册数据有变动，那么就可以调用Update 方法。

• 后续在退出的时候，需要调用一个 Delete 方法，删除这个 Endpoint。

通过这个命令可以查看已注册的服务

```bash
etcdctl --endpoints=localhost:12379 get key
//通过前缀查看所有已注册服务
etcdctl --endpoints=localhost:12379 get service/interactive --prefix
```



```go
// 服务端注册服务
func (s *EtcdTestSuite) TestServer() {
   t := s.T()
   //一个服务一个Manager
   em, err := endpoints.NewManager(s.cli, "service/user")
   require.NoError(t, err)
   ctx, cancel := context.WithTimeout(context.Background(), time.Second)
   defer cancel()
   addr := "127.0.0.1:8090"
   key := "service/user/" + addr
   l, err := net.Listen("tcp", ":8090")
   require.NoError(t, err)
   //key是指这个实例的key
   //如果有instance id，用instance id, 如果没有用本机IP+端口
   err = em.AddEndpoint(ctx, key, endpoints.Endpoint{
      // 定位信息，客户端怎么连你
      Addr: addr,
   })
   require.NoError(t, err)
```

###### **更新元数据**

```go
//注册信息变动要更新
go func() {
   ticker := time.NewTicker(time.Second)
   for now := range ticker.C {
      ctx, cancel := context.WithTimeout(context.Background(), time.Second)
      //AddEndpoint是一个覆盖的语义，有这个key就覆盖
      em.AddEndpoint(ctx, key, endpoints.Endpoint{
         Addr: addr,
         //分组信息，权重信息，机房信息
         //以及动态判定负载，可以把负载信息写在这
         Metadata: now.String(),
      })
}
```

###### 续约机制

基本步骤是：

• 要先创建租约。

• 开启自动续约，也就是调用 KeepAlive 方法。这边一般我是建议处理续约结果，只是简单打个日志就够了。

• 在操作的 Endpoint 的时候（Add、Update）都要带上这个租约的 ID。

• 在退出的时候，我们可以取消 kaCtx，以退出续约的goroutine。

**在默认的情况下，etcd 是每隔 ttl/3 的时间就去续约一次。**

```go
// 租期
ctx, cancel = context.WithTimeout(context.Background(), time.Second)
defer cancel()
//租期长度，单位：秒
var ttl int64 = 10
leaseResp, err := s.cli.Grant(ctx, ttl)
require.NoError(t, err)
//key是指这个实例的key
//如果有instance id，用instance id, 如果没有用本机IP+端口
err = em.AddEndpoint(ctx, key, endpoints.Endpoint{
   // 定位信息，客户端怎么连你
   Addr: addr,
}, etcdv3.WithLease(leaseResp.ID))
require.NoError(t, err)
kaCtx, kaCancel := context.WithCancel(context.Background())
//进行续约
go func() {
   //开启自动续约
   ch, err1 := s.cli.KeepAlive(kaCtx, leaseResp.ID)
   require.NoError(t, err1)
   for kaResp := range ch {
      //大概ttl/3时候续约一次
      t.Log(kaResp.String())
   }
}()
```

###### 客户端发现服务

grpc.Dial不再dial微服务的地址，而是dial注册中心服务对应的key

```go
// 客户端发现并调用服务
func (s *EtcdTestSuite) TestClient() {
   t := s.T()
   etcdResolver, err := resolver.NewBuilder(s.cli)
   require.NoError(s.T(), err)
   //注意target要写正确
   cc, err := grpc.Dial("etcd:///service/user",
      grpc.WithResolvers(etcdResolver),
      grpc.WithTransportCredentials(insecure.NewCredentials()))
   require.NoError(t, err)
   client := NewUserServiceClient(cc)
   ctx, cancel := context.WithTimeout(context.Background(), time.Second)
   defer cancel()
   resp, err := client.GetById(ctx, &GetByIdReq{Id: 123})
   require.NoError(t, err)
   t.Log(resp.User)
}
```

##### 改造interactive微服务

###### 服务注册启动

在启动服务之前，将服务在注册中心进行了注册

```go
func (s *Server) ListenAndServe() error {
   l, err := net.Listen("tcp", ":"+strconv.Itoa(s.Port))
   if err != nil {
      return err
   }
   s.Register()
   //这边类似gin.Run()
   return s.Server.Serve(l)
}

func (s *Server) Register() error {
   cli, err := etcdv3.New(etcdv3.Config{
      Endpoints: s.EtcdAddrs,
   })
   if err != nil {
      return err
   }
   s.client = cli
   em, err := endpoints.NewManager(cli, "service/"+s.Name)
   addr := netx.GetOutboundIP() + ":" + strconv.Itoa(s.Port)
   key := "service/" + s.Name + "/" + addr
   // 租期
   ctx, cancel := context.WithTimeout(context.Background(), time.Second)
   defer cancel()
   //租期长度，单位：秒
   var ttl int64 = 10
   leaseResp, err := cli.Grant(ctx, ttl)
   //key是指这个实例的key
   //如果有instance id，用instance id, 如果没有用本机IP+端口
   err = em.AddEndpoint(ctx, key, endpoints.Endpoint{
      // 定位信息，客户端怎么连你
      Addr: addr,
   }, etcdv3.WithLease(leaseResp.ID))
   kaCtx, kaCancel := context.WithCancel(context.Background())
   s.kaCancel = kaCancel
   //进行续约
   ch, err := cli.KeepAlive(kaCtx, leaseResp.ID)
   if err != nil {
      return err
   }
   go func() {
      for kaResp := range ch {
         //大概ttl/3时候续约一次
         s.L.Debug(kaResp.String())
      }
   }()
   return nil
}
```

###### 客户端初始化

```go
/ InitIntrGRPCClientV1 使用服务发现的客户端
func InitIntrGRPCClientV1(client *etcdv3.Client) intrv1.InteractiveServiceClient {
   type Config struct {
      Addr   string `yaml:"addr"`
      Secure bool   `yaml:"secure"`
   }
   var cfg Config
   err := viper.UnmarshalKey("grpc.client.intr", &cfg)
   if err != nil {
      panic(err)
   }
   resolver, err := resolver2.NewBuilder(client)
   if err != nil {
      panic(err)
   }
   opts := []grpc.DialOption{grpc.WithResolvers(resolver)}
   if !cfg.Secure {
      opts = append(opts, grpc.WithTransportCredentials(insecure.NewCredentials()))
   }

   cc, err := grpc.Dial(cfg.Addr, opts...)
   if err != nil {
      panic(err)
   }
   remote := intrv1.NewInteractiveServiceClient(cc)
   return remote
}
```

##### 不同微服务框架的接入

###### go-zero

```go
// TestGoZeroClient 启动 grpc 客户端
func (s *GoZeroTestSuite) TestGoZeroClient() {
   zClient := zrpc.MustNewClient(zrpc.RpcClientConf{
      Etcd: discov.EtcdConf{
         Hosts: []string{"localhost:12379"},
         Key:   "user",
      },
   })
   client := NewUserServiceClient(zClient.Conn())
   ctx, cancel := context.WithTimeout(context.Background(), time.Second)
   defer cancel()
   resp, err := client.GetById(ctx, &GetByIdReq{
      Id: 123,
   })
   require.NoError(s.T(), err)
   s.T().Log(resp.User)
}

// TestGoZeroServer 启动 grpc 服务端
func (s *GoZeroTestSuite) TestGoZeroServer() {
   // 正常来说，这个都是从配置文件中读取的
   //var c zrpc.RpcServerConf
   // 类似与 main 函数那样，从命令行接收配置文件的路径
   //conf.MustLoad(*configFile, &c)
   c := zrpc.RpcServerConf{
      ListenOn: ":8090",
      Etcd: discov.EtcdConf{
         Hosts: []string{"localhost:12379"},
         Key:   "user",
      },
   }
   // 创建一个服务器，并且注册服务实例
   server := zrpc.MustNewServer(c, func(grpcServer *grpc.Server) {
      RegisterUserServiceServer(grpcServer, &Server{})
   })

   // 这个是往 gRPC 里面增加拦截器（也可以叫做插件）
   // server.AddUnaryInterceptors(interceptor)
   server.Start()
}
```

###### krtos

```go
func (s *KratosTestSuite) SetupSuite() {
   cli, err := etcdv3.New(etcdv3.Config{
      Endpoints: []string{"localhost:12379"},
   })
   require.NoError(s.T(), err)
   s.etcdClient = cli
}

func (s *KratosTestSuite) TestClient() {

   r := etcd.New(s.etcdClient)
   cc, err := grpc.DialInsecure(context.Background(),
      grpc.WithEndpoint("discovery:///user"),
      grpc.WithDiscovery(r),
   )
   require.NoError(s.T(), err)
   defer cc.Close()

   client := NewUserServiceClient(cc)
   ctx, cancel := context.WithTimeout(context.Background(), time.Second)
   defer cancel()
   resp, err := client.GetById(ctx, &GetByIdReq{
      Id: 123,
   })
   require.NoError(s.T(), err)
   s.T().Log(resp.User)
}

// TestServer 启动服务器
func (s *KratosTestSuite) TestServer() {
   grpcSrv := grpc.NewServer(
      grpc.Address(":8090"),
      grpc.Middleware(recovery.Recovery()),
   )
   RegisterUserServiceServer(grpcSrv, &Server{})
   // etcd 注册中心
   r := etcd.New(s.etcdClient)
   app := kratos.New(
      kratos.Name("user"),
      kratos.Server(
         grpcSrv,
      ),
      kratos.Registrar(r),
   )
   app.Run()
}
```

###### 总结

在以 gRPC 为底层通信机制的微服务框架上，基本上服务注册与发现都有一些共同点：

• **必然实现了gRPC 的 Resolver 接口。**

• **必然有一个 register 过程**，也就是在服务端启动的注册数据。

• **必然会有一个续约的过程**。要么是自己写续约，要么是依赖于注册中心客户端提供的续约机制。

• 如果微服务框架本身打算支持很多种不同的服务注册与发现的实现，那么就会有一个类似于 Registry、Discovery 之类的接口层，用于构建自己微服务框架内部的一致性抽象。

因此你在学习不同框架的过程中，沿着这个思路去找对应的源码，就容易理解。

#### 注册中心选型（ms）

##### CAP 理论

分布式 CAP 理论是分布式系统中的一个重要概念，它指出一个分布式系统不可能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition Tolerance）这三个基本要求，最多只能同时满足其中的两项。

• **一致性**：在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性。当一个系统在数据一
致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。强调的是你在任何一个节点上读到
的数据都是一样的。

• **可用性**：可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限
时间内返回结果。强调的是不管系统内部发生了什么，集群都需要能够对外提供正常服务。

• **分区容错性**：分区容错性是指系统在出现网络分区（网络故障导致不同部分之间的通信中断）时仍能正常工
作的能力。强调的是，网络通信被破坏导致系统被分割成多个部分，依旧能够对外提供服务。

##### CP 还是 AP

在分布式环境下，大部分时候 P（分区容忍性） 都是必不可少的。如果有两个节点 A 和 B，两者之间通过网络来通信。那么在网络通信不通畅的情况下，你只能：

• 放弃让 A 和 B 之间同步数据，那么意味着你舍弃了一致性。

• 放弃让 A 和 B 对外服务，也就是你放弃了可用性。

CAP 理论一般在设计中间件的时候需要第一时间确认清楚，而对于用户来说，只要考虑清楚自己的业务是侧重一致性还是侧重可用性，就可以做出决策了。

但是也有例外，比如 etcd 严格来说就是 AC 模型，没有选择P。也就是说，如果网络故障导致集群节点之间不能通信，那么 etcd 集群就不可用。

##### 注册中心 CAP 选哪个？

面试的标准答案的是选 AP。但是我个人看法的是，小规模随便选，大规模选 AP。在服务发现与注册这个模型里面，有一个问题就是：

• 如果你认为拿到错误的数据，也好过拿不到数据，那么你就应该选择 AP。

• 如果你认为我一定要拿到正确的注册信息，那么就应该选 CP。

而在小规模集群的时候，可用性问题并不突出。毕竟这个时候你的负载小、流量小，注册中心想要崩溃
都很难。接下来我们看看不同的中间件用作注册中心会如何。

##### 各种注册中心

#### 服务注册与发现的高可用

###### 服务端崩溃

在注册模型里面，正常来说注册中心能够发现服务端崩溃。

但是这需要时间。也就是说，服务端崩溃到注册中心发现、再到注册中心通知客户端，可能存在秒级的延迟。

这个时候，就要求客户端要能够正常处理这个情况。简单来说，就是**客户端发现服务端连不上之后，就要换一个节点来重试，这就是我们之前多次讨论的failover 策略。**

###### 注册中心崩溃 —— 注册中心高可用

那么首先第一个要确保的就是注册中心不能轻易崩溃。

• **启用注册中心的高可用方案**，例如说部署为一个集群，多活方案。

• **双注册中心方案**：也就是注册的时候同时注册两个注册中心，在一个注册中心崩溃之后可以使用另外一个注册中心。

• **按照业务拆分多个注册中心**：也就是说一个公司内部不要只用一个注册中心，最好是不同的业务使用不同的注册中心，核心业务和非核心业务使用不同的注册中心。

###### 注册中心崩溃怎么办？ —— 客户端怎么办？

注册中心崩溃之后，后果之一就是客户端无法获得服务端变动的信息：例如上线或者下线等。客户端要想容忍这个错误，可以考虑：

• **使用本地缓存的可用节点信息**，缺陷就是可能不准，有可能有些节点已经下线了，有些节点新加进来。

• 客户端使用本地缓存的节点信息之后，如果发起调用的时候发现无法连通，**那么就要把这个节点移出可用节点列表。**

但是，如果是一个新服务，因为客户端本身没有缓存任何的可用节点信息，那么就没有办法发起调用，返回特定的错误，让调用者自己的去决定怎么处理。

###### 注册中心崩溃怎么办？ —— 服务端怎么办？

注册中心崩溃之后，服务端已经没有办法更新注册信息了。这时候：

• **如果是一个新节点，那么注册失败的时候，它应该直接退出服务。**

• 如果是一个老的节点，此时客户端可能还在使用本地缓存的注册数据，所以**老节点应该继续提供服务。**

### 负载均衡算法

#### 基本理论

前面我们已经讨论了服务注册与发现，也就是已经解决了”有哪些可用服务实例”的问题，现在就要讨论下一个问题：这么多可用服务实例，我该把请求发给谁？这一步，我们直觉就想到关键字：**负载均衡。**

**所谓负载均衡目的就是把请求转发给“最适合”处理这个请求的节点**。最适合意味着：

• 如果这个请求需要很多内存，那么将它转发给内存多的节点。

• 如果这个请求是 CPU 密集的，那么将它转发给 CPU 比较空闲的请求。

• ……

显然，在大多数时候，我们会希望将请求转发给“能够最快返回响应”的那个节点。

也就是说，如果我们的目标是挑选出那个最适合的节点，我们就得**设立一个标准。而这个标准，被简化为“`负载`”。**换言之，我们认为负载最轻的就是最适合的。

而考虑到所有的节点都提供同等的服务，那么你**每次挑负载最轻的节点，就造成所有的节点负载是大致均衡的**。

<img src="assets/image-20241010223313192.png" alt="image-20241010223313192" style="zoom:33%;" />

#### 负载均衡算法（ms）

##### 静态均衡算法与动态均衡算法

**静态负载均衡算法的思路是完全不管节点的实际负载，而是依靠统计学来保证负载是均衡的。而动态负载均衡算法就要去计算节点的负载，根据负载选择合适的节点**

也就是说，只要发的请求足够多，这些节点就大概率负载是均衡的。主要有：**轮询、加权轮询、随机、加权随机、哈希、一致性哈希。**

**动态负载均衡算法核心在于：尝试实时计算节点的负载**。而在实践中，我们其实也不知道怎么定量计算一个节点的负载，所以动态负载均衡算法也只能根据各自的理解，选择一些指标。
常见的有：
**• 最少连接数。**
**• 最少活跃数。**
**• 最快响应。**

##### 轮询

轮询算法的效果很不错，而且应该是应用最广泛的负载均衡算法了。轮询的思路非常简单的，也就是一个个节点的轮流过去。

##### 加权轮询

加权轮询是对轮询的一个改进。算法大体上还是轮询，但是不再是一个请求一个节点这样轮询，而是按照权重来轮询。权重多的会多分到几个请求，权重小的就少分几个请求。

> 如何获得权重？

在注册中心模型下，权重基本上是通过注册中心来传递的。也就是每一个**服务端节点在启动的时候会把自己的权重信息也注册进去。客户端在服务发现的过程中，就能获得对应的权重信息。**

##### 平滑的加权轮询算法

加权轮询算法是在加权算法的基础上，引入了一个 currentWeight（当前权重）的概念。每次选中一个节点之后，就降低 currentWeight。

于是一个节点就有两个权重了：初始权重和当前权重。整个算法的步骤是：
• 计算所有的初始权重的和，也就是总权重。
• 每一个节点，都让自己的当前权重加上初始权重。
• 选出当前权重最大的节点，发送请求。
• 将选中的节点的当前权重减去总权重。

后面我们讲到动态调整权重的时候，实际上调整的是初始权重，也叫做有效权重。

##### 随机

随机算法比轮询还要简单，就是随便挑一个。从实现的角度来说，就是生成一个随机数，利用随机数除以总节点数量，得到的余数就是节点的下标。随机算法有两个假设：

• 所有服务器的处理能力是一样的。
• 所有请求所需的资源也是一样的。

并且，随机算法还认为在请求数量足够多的情况下，节点负载是均衡的。当然，如果你特别倒霉，也是会遇到负载不均衡的问题。

##### 加权随机

加权随机类似于加权轮询，给每一个节点一个权重。如右图，就是加权随机的基本思路。
• 计算一个总权重 T。
• 生成一个随机数 r。
• r 落到总权重的哪个区间，就选择哪个节点

<img src="assets/image-20241010224021565.png" alt="image-20241010224021565" style="zoom:33%;" />

##### 哈希

哈希算法核心就是根据业务特征来计算一个哈希值，而后除以节点总数的余数，就是目标节点。你可以将哈希和随机结合在一起看，那么随机算法就是用随机数，而哈希算法就是用业务特征来计算哈希值。

所谓的业务特征，可以是：
• 业务 ID
• 外键
• 唯一索引
• ……
跟你的业务是密切相关的。

<img src="assets/image-20241010224034448.png" alt="image-20241010224034448" style="zoom:33%;" />

##### 加权哈希算法

加权哈希算法也类似于加权随机算法。用哈希值替代了随机数，就变成了加权哈希算法。

##### 一致性哈希

一致性哈希算法是哈希算法的改进，引入了一个类似环。同样是计算哈希值，但是哈希值和节点的关系是：**哈希值落在某一个区间内，会被特定的一个节点处理。因此优势就是当增加节点或者减少节点的时候，只有一部分请求命中的节点会发生变化。**

#### 在gRPC中接入负载均衡

##### gRPC 中默认的负载均衡算法

gRPC **在默认情况下，可以认为是没有负载均衡的，因为它永远只会选择第一个节点。**这个第一个节点是你在 Resolver 里面解析出来的第一个节点。当然大多数时候，第一个节点是什么你一点都不关心，也没啥影响。

我们在服务端启动了两个节点，分别监听 8090和 8091 端口。但是运行 TestClientPickFirst 这个测试，得到的结果就是只有 8090 被使用了。

##### gRPC 中使用 round_robin 算法

gRPC 提供了一些负载均衡算法的实现，可以直接使用。比如说轮询算法，大多数时候你直接使用这个算法就足够了。

下面是指定使用 round_robin 算法的例子。要注意，这里是一个 gRPC 的垃圾设计，即传入JSON 串。这是很容易写错的地方。round_robin 没有任何配置，所以后面就是一个{}。

```go
//负载均衡器配置
svcCfg := `
{
   "loadBalancingConfig": [
      {
         "round_robin": {}
      }
   ]
}`
cc, err := grpc.Dial("etcd:///service/user",
    grpc.WithResolvers(etcdResolver),
    //在这里使用负载均衡器
    //使用轮询
    grpc.WithDefaultServiceConfig(svcCfg),
    grpc.WithTransportCredentials(insecure.NewCredentials()))
```

这个算法自己实现也很简单，面试可能要写

```go
type Balancer struct {
	nodes []*Node
	idx   int
}

// 随机
func (b *Balancer) random() *Node {
	r := rand.Int31()
	return b.nodes[int(r)%len(b.nodes)]
}
```

##### gRPC 中使用加权负载均衡算法

gRPC 本身就提供了加权负载均衡算法实现。使用起来有两个地方需要修改：

• 匿名引入基于权重的负载均衡算法，也就是初始化好这个算法。

```go
_ "google.golang.org/grpc/balancer/weightedroundrobin"
```

• 在客户端启动的时候指定使用加权轮询算法。前面使用 round_robin 不用匿名引入，是因为
gRPC 本身引入了。

```go
//负载均衡器配置
svcCfg := `
{
   "loadBalancingConfig": [
      {
         "weighted_round_robin": {}
      }
   ]
}`
cc, err := grpc.Dial("etcd:///service/user",
    grpc.WithResolvers(etcdResolver),
    //在这里使用负载均衡器
    //使用轮询
    grpc.WithDefaultServiceConfig(svcCfg),
    grpc.WithTransportCredentials(insecure.NewCredentials()))
```

gRPC 中的加权负载均衡算法并不是我们之前讨论的服务端配置权重的加权算法。gRPC 是自己主动计算每个节点的权重的。它会根据每个节点的 QPS、错误比率来计算一个权重。

简单来说， **QPS 越高，权重越大；错误率越高，权重越低**。而后，利用 EDF 算法（Earliest Deadline First）从这些节点里面挑选出来一个。

##### gRPC 中接入自己的负载均衡算法

###### 方法

有些时候你需要根据业务特征来定制一些负载均衡算法，那么就需要了解怎么接入自定义的算法。在 gPRC 里面，实现一个自定义算法，需要实现两个关键接口：

• PickerBuilder：它在 balancer.base 子包里面。

• Picker：在 balancer 子包下面。

```go
// PickerBuilder creates balancer.Picker.
type PickerBuilder interface {
	// Build returns a picker that will be used by gRPC to pick a SubConn.
	Build(info PickerBuildInfo) balancer.Picker
}
type Picker interface {
	Pick(info PickInfo) (PickResult, error)
}
```

###### 自定义基于权重的负载均衡算法

现在我们尝试实现一个基于权重的负载均衡算法。这个算法的要点：

**• 要在客户端这边拿到服务端节点的权重。**
**• 利用权重实现一个平滑的基于权重的负载均衡算法。**

之前我们提到过，**要在服务端到客户端之间传递元数据，那么就要利用到注册时候的 metadata 字段**。大部分候，每一个节点的权重信息都是从配置文件或者环境变量里面读取到的。

<img src="assets/image-20241010230502996.png" alt="image-20241010230502996" style="zoom:33%;" />

```go
const Name = "custom_weighted_round_robin"

type PickerBuilder struct {
}

func newBuilder() balancer.Builder {
   return base.NewBalancerBuilder(Name, &PickerBuilder{}, base.Config{HealthCheck: true})
}

func init() {
   balancer.Register(newBuilder())
}

// Build 拿到连接信息，构建picker
func (p *PickerBuilder) Build(info base.PickerBuildInfo) balancer.Picker {
   conns := make([]*weightConn, 0, len(info.ReadySCs))
   for sc, sci := range info.ReadySCs {
      //获取服务的元数据
      md, _ := sci.Address.Metadata.(map[string]any)
      weightVal, _ := md["weight"]
      weight, _ := weightVal.(float64)
      //if weight == 0 {
      //
      //}
      conns = append(conns, &weightConn{
         SubConn:       sc,
         weight:        int(weight),
         currentWeight: int(weight),
      })
   }

   return &Picker{
      conns: conns,
   }
}

type Picker struct {
   conns []*weightConn
   lock  sync.Mutex
}

// Pick 实现基于权重的负载均衡算法
func (p *Picker) Pick(info balancer.PickInfo) (balancer.PickResult, error) {
   p.lock.Lock()
   defer p.lock.Unlock()
   if len(p.conns) == 0 {
      return balancer.PickResult{}, balancer.ErrNoSubConnAvailable
   }
   // 总权重
   var total int
   var maxCC *weightConn
   for _, c := range p.conns {
      total += c.weight
      c.currentWeight = c.currentWeight + c.weight
      if maxCC == nil || maxCC.currentWeight < c.currentWeight {
         maxCC = c
      }
   }

   maxCC.currentWeight = maxCC.currentWeight - total

   return balancer.PickResult{
      SubConn: maxCC.SubConn,
      //回调方法
      Done: func(info balancer.DoneInfo) {
         // 要在这里进一步调整weight/currentWeight
         // failover 要在这里做文章
         // 根据调用结果的具体错误信息进行容错
         // 1. 如果要是触发了限流了，
         // 1.1 你可以考虑直接挪走这个节点，后面再挪回来
         // 1.2 你可以考虑直接将 weight/currentWeight 调整到极低
         // 2. 触发了熔断呢？
         // 3. 降级呢？
      },
   }, nil
}

type weightConn struct {
   balancer.SubConn
   weight        int
   currentWeight int

   // 可以用来标记不可用
   available bool
}
```

###### 不同框架接入负载均衡的方法

#### 在 gRPC 中实现 Failover 机制

##### failover实现思路

failover 讨论的就是：如果我下游的某一个节点已经崩溃了，或者说我连不上了，怎么解决。可能的原因有：
• 下游服务确实崩溃了。
• 和下游节点的网络出现了问题。
• 下游服务下线了，但是客户端还没得到注册中心的通知。

在 gRPC 中实现 failover 的基本思路简单来说就是：重试 + 负载均衡。

• 重试：当调用不通的时候，gRPC 会重试，再次经过负载均衡。
• 负载均衡：要确保在某个节点不可用之后，负载均衡的策略不会再次挑选中它。

##### retry policy 详解

下面的json字符串就是最简单利用重试 + 轮询实现的一个 failover 策略。同样在配置负载均衡器的地方配置即可

```json
{
  "loadBalancingConfig": [{"round_robin": {}}],
  "methodConfig":  [
    {
      "name": [{"service":  "UserService"}],
      "retryPolicy": {
        "maxAttempts": 4,
        "initialBackoff": "0.01s",
        "maxBackoff": "0.1s",
        "backoffMultiplier": 2.0,
        "retryableStatusCodes": ["UNAVAILABLE"]
      }
    }
  ]
}
```

- **name：指定哪些方法。**这里我们指定的是UserService 上所有方法。这个 service 对应的值，要去 proto 编译后的文件里面找。
- **retryPolicy：重试的策略。**
  - maxAttempts：最大重试次数。
  - initBackoff：重试间隔的初始值。
  - maxBackoff：重试间隔的最大值。
  - backoffMultiplier：重试间隔的增长系数。

手写 JSON 很容易出错，千万仔细检查。

### 服务治理

#### 理论基础

##### 引入

服务治理是一个非常宽泛的问题，涵盖了：

• 熔断、限流和降级
• 超时控制
• 隔离
• 分组和路由
• 优雅退出

**其中熔断、限流和降级是你们听说过的最多的服务治理措施，也是实践中和面试中的高频面试点。**应当说，微服务架构里面要想把亮点面出来，就必须要有一整套的，从前端到后端的服务治理方案。

整体来说，你可以认为，服务治理讨论的就是：

• 怎么保证系统不会出现故障？或者说，尽量让系统少出现故障。
• 万一系统出现故障了：
• 怎么尽快发现出现故障了？
• 怎么处理出现的故障？
• 怎么从故障中恢复过来？或者说，怎么退出故障处理机制，恢复正常的业务处理流程？

这也就是整个故障处理的理论：
**• 故障检测**
**• 故障处理**
**• 故障恢复**

##### 故障检测

故障检测，实际上你在前面的课程里面已经陆续接触过了。总体来说故障检测的思路就是两条：

• **静态检测**：也就是压根不管服务的真实状态，而是**根据压测结果、研发人员的历史经验**，设定一些阈值，触发了阈值就认为服务出现了（或者快要出现）问题。

• **动态检测**：也就是我们讨论过的可以**根据服务所在节点的硬件信息，服务本身的信息（如响应时间、错误率）等来判断。**

##### 故障处理

故障处理实际上能玩的花样也不多。主要的方式有：

• **同步转异步：也就是把请求记录下来，后续再处理。**

• **执行特殊代码**：对于熔断、限流来说，就是返回error；对于降级来说，**可能是返回默认值，或者执行一个快路径。**

• **请求转发**：也就是当前节点不能处理该请求，就会把请求转发到别的节点。这个过程如果通过客户端来配
合，实现更加简单。

所以你可以看到，所谓的熔断、限流、降级本质上其实都是一回事。

##### 故障恢复

故障恢复的要点在于：

该如何确定我的服务是否已经恢复正常了？
• 固定时间等待：例如说等一分钟，就认为已经恢复了。
• 实时计算：也就是根据故障检测的算法，实时计算服务端节点的状态。
• 试探法：尝试处理请求，而后根据处理结果，来确定系统是否已经恢复。

避免抖动：也就是退出故障处理流程的时候，不要立刻引起系统再次触发故障。基本思路就是：结合试探法来逐步放开流量，也可以叫做灰度。

故障恢复如果处理不好，很容易导致出现系统一会好，一会崩溃的情况。

> 故障恢复：几乎通用的策略

大部分时候，故障恢复都可以采用试探 + 逐步放开流量的方式来进行。这里以系统发生故障之后所有请求都返回默认值为例。
• 首先，触发故障修复（返回了默认值）之后，间隔一段时间就试探性地处理一个请求。
• 如果该请求被正常处理了，那么就加大流量；如果没有被正常处理，那么就继续返回默认值。
• 在逐步加大流量的过程中，如果要是请求又没有被正常处理，那么就减少流量，或者再次进入到返回默认值的状态。
• 在不断加大流量之后，直到 100% 的流量都被正常处理了。

**这种政策， 你可以参考之前我们随机数 + 阈值的流量控制方案。你只需要在这里根据处理结果实时调整阈值就**
**可以。**

##### 微服务流量放大与雪崩

这算是一个额外的知识点。也就是在微服务架构里面，有一个很重要的特性，就是一个单一请求在微服务处理过程中，会产生非常多的服务调用。

如图，当后端收到一个请求之后，在整个过程中总共发起了四个微服务查询。

因此你可以看到，一旦请求本身流量增长了一倍，那么整个系统的实际负载增长，不止一倍。所以我们在做服务治理的时候，也要考虑这个问题。例如说通过限流、熔断等措施防止服务雪崩，或者说风险扩大。

<img src="assets/image-20241011133454040.png" alt="image-20241011133454040" style="zoom:33%;" />

#### 具体措施

##### 熔断

熔断是经常使用的一种服务治理手段。**`它是一种保护机制，用于防止微服务架构中的级联故障。`**

所谓**`级联故障`**，**就是因为一个节点出错之后，导致别的节点跟着出错。本质上是因为微服务架构流量放大引起的。**

例图中，当流量突然增大的时候，节点 A 直接熔断之后，就可以避免 B，C 和 D 崩溃，也进一步避免了 E 崩溃。限流和降级也能打成类似的效果。

<img src="assets/image-20241011133643108.png" alt="image-20241011133643108" style="zoom:33%;" />

熔断的要点在于：
• 如何判定服务是否需要熔断了？这也就是我们之前说的故障检测，也就是可以考虑**静态检测和动态检测**；
• 熔断之后怎么办？大多数时候都是**返回特定错误**；
• 怎么从熔断之中恢复过来？也一样是**试探请求 + 逐步放开流量**。

熔断还有一种说法，是说熔断处于三种状态：
• 开放状态：也就是所有的请求都返回特定错误；
• 闭合状态：也就是所有的请求都被正常处理；
• 半开放状态：一部分请求被正常处理，一部分请求直接返回错误。而本质上，半开放状态就是处于恢复过程中的状态。

##### 降级

降级本质上和熔断差不多。如果说熔断是直接拒绝全部请求，那么降级就是尽可能返回一个响应。这个响应可以直接是一个提前配置好的默认响应，也可以是走某些特定的快路径。

如右图经典的缓存降策略示例，在大多数的业务场景中，有些时候会有快慢两条路径：
**• 快路径耗费资源少，计算快；**
**• 慢路径耗费资源多，计算慢。**

**`正常都是先执行快路径，再执行慢路径。在降级之后，可以只执行快路径，而不执行慢路径。`**

<img src="assets/image-20241011133925237.png" alt="image-20241011133925237" style="zoom:33%;" />

这种策略的优势在于：

• **应用本身的负载会快速降低。**因为只查询 Redis，所以会很快，因此请求被快速处理完毕，就能腾出资源来；
• **能撑住极高并发**。整个瓶颈变成了 Redis。注意，即便缓存命中率极高，但是少数的数据库查询也会极大拖累并发；
• **能保住数据库**。例如说 Redis 本身有问题，或者玩过有问题，在这种策略之下，数据库不会被请求压垮。

> 跨服务降级。

例如说在用户服务上，有增删改和查询服务。那么在资源不足的时候，就可以考虑将增删改停掉，全力支持查询服务。

类似地可以做得更加高级：
• 在集群层面上，如果服务器资源不足，那么就将边缘业务停掉，腾出服务器资源来给核心服务；
• 如果是读服务和写服务分组部署的模式，那么也可以将写服务停掉，调用资源支持读服务；
• 同一个节点上部署的不同服务，可以按照重要性从不重要到重要，逐步停掉服务，直到腾出来足够的资源。

##### 限流

限流是指，当判定系统已经无法处理更多的请求的时候，就会执行限流策略。

目前大多数时候，我们讨论的限流都是静态限流。也就是提前设置好一个限流阈值。如果超过这个阈值，那么就开启限流，正常来说就是拒绝这个请求，即返回特定的错误。

###### 被限流的请求怎么办？

很少有人会仔细讨论限流中被限流的请求该怎么办。实际上也可以考虑：
• 同步转异步：也就是被限流的请求临时保存下来，后续再处理。
• 执行特殊代码：例如直接返回默认值；
• 转发请求：例如说通知客户端换一个节点重试；
从这里你就能看出来，限流、熔断和降级之间的界限不是非常分明。

###### 针对什么限流？

在实践中，你可以考虑：
• 针对单机限流，或者针对集群限流；
• 针对整个应用限流，也可以针对应用提供的某个服务限流。例如说可以整个用户服务限流，也可以是用户服务内部的某个接口（例如说更新信息）限流；
• 针对业务对象限流：这个就可以完全根据你的业务特征来设置，非常灵活。
• 针对用户限流：例如说 VIP 用户不限流，但是普通用户限流
• 针对 IP 限流：我们之前在 Web 登录阶段采用过

##### 各种限流算法

限流里面，讨论比较多的就是限流算法。限流算法具体来说，有：
`• 计数器`
`• 固定窗口`
`• 滑动窗口`
`• 令牌桶`
`• 漏桶`

这些算法用起来的效果都差不多，实践中可以随便选择一个算法实现。

###### 计数器算法

计数器很简单：**收到请求的时候，计数器 +1；返回响应的时候，计数器-1。通过当前阈值判断要不要限流。**你可以确定，这个算法就是保持在任何时刻，整个系统里面只有固定数量个请求正在被处理。该算法效果强力，并且不会比后面的算法差。

###### 固定窗口

固定窗口的原理很简单：将时间切成一个个窗口，确保每个窗口内的请求数量没有超过阈值。

###### 滑动窗口

滑动窗口算法原理也很简单。你可以认为自始至终只有一个窗口，这个窗口就是从当前时间往前回溯窗
口大小的一段时间，在这个窗口内，只能处理固定数量的请求。

###### 令牌桶

令牌桶算法要点：
• 有一个人按一定的速率发令牌
• 令牌会被放到一个桶里
• 每一个请求从桶里面拿一个令牌
• 拿到令牌的请求就会被处理
• 没有拿到令牌的请求就会：
• 直接被拒绝
• 阻塞直到拿到令牌或者超时

###### 漏桶

漏桶算法要点：
• 有一个人按一定的速率发令牌
• 每一个请求拿一个令牌
• 拿到令牌的请求就会被处理
• 没有拿到令牌的请求就会：
• 直接被拒绝
• 阻塞直到拿到令牌或者超时

> 令牌桶与漏桶的对比

令牌桶
• 允许积压。也就是说可以攒一些令牌，那么突发流量来的时候，有更多的请求可以拿到令牌；

漏桶：
• 没有积压，绝对均匀。漏桶可以做到，不管什么时候，放过去请求的速率都是均匀的；

##### 总结：熔断、限流、降级用哪个？

• 如果你希望即便出现了各种故障，但是**服务也要尽可能保持可用，那么应该使用降级；**
• 如果你希望服务**尽快从故障中恢复过来，那么应该选用熔断；**
• 如果你希望**至少有一部分请求能够被正确处理，那么应该选用限流；**

如果从我个人的使用频率上来说，是限流>降级>熔断。熔断本身的破坏性也很强，所以我比较少用。

#### 在gRPC中接入服务治理

##### 通过Interceptor接入

拦截器，**`类似web中间件`，可以在其中处理客户端请求，接入熔断、降级、限流等操作**

具体来说，gRPC 的 Interceptor 分成：
• 客户端 Interceptor
• 服务端 Interceptor

大部分情况下，我们熔断、限流和降级等都是在服务端处理的， 所以我们会用到的服务端的Interceptor。

用法并不是很难。在服务端中，处理普通的 RPC 调用的拦截器叫做UnaryServerInterceptor。它本质上是一个函数类型，接收参数：

• ctx：常见的参数，一般来说和链路有关的数据可以从这里拿出来。
• req：客户端发过来的请求，具体类型和调用的服务有关。
• info：有两个字段，其中 Server 代表的是你的服务实现，FullMethod 是调用的目标服务。
• handler：处理请求的处理器。

```go
func(ctx context.Context, 
     req any, 
     info *grpc.UnaryServerInfo,
     handler grpc.UnaryHandler) (resp any, err error)
```

##### 限流

引入已经实现的限流器，通过构造器模式构造限流处理函数即可。

```go
// InterceptorBuilder grpc限流
type InterceptorBuilder struct {
   limiter ratelimit.Limiter
   key     string
}

// NewInterceptorBuilder key 1. limiter:interactive-service => 整个点赞的应用限流
func NewInterceptorBuilder(limiter ratelimit.Limiter, key string) *InterceptorBuilder {
   return &InterceptorBuilder{limiter: limiter, key: key}
}

func (b *InterceptorBuilder) BuildServerUnaryInterceptor() grpc.UnaryServerInterceptor {
   return func(ctx context.Context,
      req any, info *grpc.UnaryServerInfo,
      handler grpc.UnaryHandler) (resp any, err error) {
      //调用限流
      limited, err := b.limiter.Limit(ctx, b.key)
      if err != nil {
         // 你有保守的做法，也有激进的做法
         // 这个是保守的做法
         return nil, status.Errorf(codes.ResourceExhausted, "限流")
         //这是激进的策略
         //return handler(ctx, req)
      }
      if limited {
         return nil, status.Errorf(codes.ResourceExhausted, "限流")
      }
      return handler(ctx, req)
   }
}
```

###### 限流阈值该如何确定？—— 压测

标准答案是根据限流来确定。面试也是最好回答这个答案。

##### 熔断

```go
/ InterceptorBuilder 拦截器中使用熔断
type InterceptorBuilder struct {
   breaker circuitbreaker.CircuitBreaker
}

func (b *InterceptorBuilder) BuildServerUnaryInterceptor() grpc.UnaryServerInterceptor {
   return func(ctx context.Context, req any,
      info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp any, err error) {
      err = b.breaker.Allow()
      if err == nil {
         resp, err = handler(ctx, req)
         if err == nil {
            b.breaker.MarkSuccess()
         } else {
            // 更加仔细检测，只有真实代表服务端出现故障的，才 mark failed
            b.breaker.MarkFailed()
         }
         return
      } else {
         //触发熔断
         b.breaker.MarkFailed()
         return nil, status.Errorf(codes.Unavailable, "熔断")
      }
   }
}
```

##### 降级

降级是比较难做到非常通用的。因为具体来说，一个服务能不能降级，要怎么降级，降级到什么程度，都是和具业务强相关的。但是我们可以借助限流器、熔断器来判定要不要降级。而后业务根据这个标记位，来判定要不要执行一些特殊的逻辑。

#### grpc可观察性

使用方法与gin中间件类似

日志

trace

Prometheus

## 支付系统

![image-20241011141940871](assets/image-20241011141940871.png)

## 评论系统

## 用户关系

## ElasticSearch搜索服务



## bloghub

### 用户功能

#### 用户注册 sigup

通过邮箱注册，使用bcrypt进行密码加密，插入用户数据

#### 用户登录 loginJWT

通过邮箱登录，邮箱有唯一键索引，直接插入，看是否冲突

#### 使用长短token

怎么退出登录？

因为服务器无法修改jwt的无效，所以服务器只能额外存储一些用户信息判断判断是否退出登录

```go
err := h.cmd.Set(ctx, fmt.Sprintf("users:ssid:%s", claims.Ssid), "", h.rcExpiration).Err()
```

#### 用户查询和修改个人信息 Profile/Edit

获取参数，进行基本的修改和查询操作

profile查询时先查询redis缓存，再查询数据库，为了保证数据一致性edit修改用户数据后，删除缓存。



### 原理

#### 什么是跨域问题？跨域问题怎么解决的？

#### jwt原理？为什么使用jwt？为什么使用长短token?

jwt它由头部、负载、签名组成，通过对头部和负载生成签名来验证jwt的完整性，确保jwt不会被篡改。

但使用jwt安全性风险很大，因为秘钥可能泄露，泄露后攻击者可以自己生成jwt。

jwt不依赖第三方存储，比如session在分布式环境下，可能要通过redis来存储，每次请求都要查询redis影响性能

**长短token**

登录成功后给用户返回两个token，过期时间一个长一个短，使用短的jwt**一定程度上提高了安全性，防止token泄露**。让前端长token刷新短token

#### cookie和session和jwt对比



### 问题

前端多次点击注册的时候，后端可能插入了多条数据，遇到了并发问题，因为后端先查询数据是否存在，再去插入数据，可能多次请求同时查询到了数据不存在，然后都执行了插入。给账号添加一个唯一索引，然后也没有必要去查询，直接插入，然后通过唯一键冲突判断用户是否存在。
